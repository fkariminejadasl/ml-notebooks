{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Deep Learning Tutorials and Notebooks","text":"<p>This repository is for my students.</p>"},{"location":"#tutorials","title":"Tutorials","text":""},{"location":"#deep-learning-general","title":"Deep Learning General","text":"<ul> <li>Access Snellius GPUs</li> <li>Deep Learning Project Setup</li> <li>Pytorch Lightning</li> <li>Practical Information about Data</li> <li>Shared Jupyter Notebook in SRC (SURF Research Cloud)</li> <li>Gradio App in SRC (SURF Research Cloud)</li> <li>Hugging Face Guide</li> <li>Set Up a Custom Python Function within PostgreSQL</li> </ul>"},{"location":"#training-and-inference","title":"Training and Inference","text":"<ul> <li>Training and Inference with Limited Resources</li> <li>Improve Deep Learning Training Results</li> <li>DDP(Distributed Data Parallel) in PyTorch</li> </ul>"},{"location":"#courses-and-literature","title":"Courses and Literature","text":"<ul> <li>Courses in Deep Learning and Computer Vision</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Localization, Mapping, 3D Reconstruction</li> <li>Time Series</li> </ul>"},{"location":"#blogpost","title":"Blogpost","text":"<ul> <li>Example Tasks for Large Language, Multimodal, and Vision Models</li> <li>Companies</li> <li>Handling Variable Sequence Lengths in Transformer Models</li> <li>Installing Deep Learning Tools for Point Cloud Segmentation</li> <li>Enhancing AI Capabilities: Post-Training, Reasoning, and Agent, Models</li> <li>Generative AI</li> <li>Time Series Augmentation</li> <li>Metrics for Comparing Distributions</li> <li>Simulation: Generation / Editing</li> </ul>"},{"location":"#general","title":"General","text":"<ul> <li>Services</li> <li>Git Basics: A Simple Manual</li> <li>Python Basics</li> <li>Linux Basics</li> <li>Docker Basics</li> <li>Tmux Basics</li> <li>Making a Reveal.js presentation from Markdown (with Python + Pandoc)</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Scheduler</li> </ul>"},{"location":"#miscellaneous","title":"Miscellaneous:","text":"<ul> <li>GEE</li> </ul>"},{"location":"#notebook-examples","title":"\ud83d\ude80 Notebook Examples","text":"notebook open in colab Colab basics Training basics Classification Example Object detection with yolov8 and roboflow"},{"location":"AIEnhancement/","title":"AI Enhancement","text":""},{"location":"AIEnhancement/#enhancing-ai-capabilities-post-training-reasoning-and-agent","title":"Enhancing AI Capabilities: Post-Training, Reasoning, and Agent","text":"<p>This document outlines how post-training techniques, reasoning, and agent development interrelate. Each topic contributes to a common goal: enhancing the capabilities of foundation models to perform complex tasks, reason through problems step-by-step, and interact with real-world environments effectively.</p>"},{"location":"AIEnhancement/#post-training-foundation-models","title":"Post-training Foundation Models","text":"<p>Post-training refers to the process of further refining a pre-trained foundation model. The goal is to improve the model\u2019s performance, safety, and alignment with human expectations by applying reinforcement learning\u2013inspired methods after the initial training phase. This refinement process allows models to better adapt to specific tasks or user requirements without altering the foundational knowledge acquired during pre-training.</p> <p>The typical methods are:</p> <ul> <li>SFT (Supervised Fine-Tuning)</li> <li>RLHF (Reinforcement Learning from Human Feedback)</li> <li>RM (Reward Model) rule-based RM or model-based RM</li> <li>DPO (Direct Preference Optimization)</li> <li>PPO (Proximal Policy Optimization)</li> <li>GRPO (Group Relative Policy Optimization), arXiv:2402.03300 from DeepSeekMath</li> <li>Sequence Policy Optimization (GSPO), arXiv:2507.18071 from Alibaba Qwen announced Group </li> <li>RLVR (Reinforcement Learning with Verifiable Reward)</li> </ul>"},{"location":"AIEnhancement/#examples","title":"Examples","text":"<ul> <li>DeepSeek v3 used SFT, (rule-based, model-based) RM, GRPO</li> <li>T\u00fclu 3 used RLVR</li> </ul>"},{"location":"AIEnhancement/#reasoning","title":"Reasoning","text":"<p>Reasoning refers to a model's ability to generate or simulate step-by-step thought processes. The goal is to break down complex problems into smaller, more manageable steps, making the decision-making process more transparent and interpretable. Enhanced reasoning capabilities, often achieved through effective reinforcement learning during post-training, enable models not only to arrive at answers but also to provide insights into how those answers were derived. Examples of such reasoning techniques include chain-of-thought prompting. Notable models demonstrating these capabilities include OpenAI's o1 and o3, DeepSeek-R1, Qwen QwQ, and Google Gemini 2.0 Flash Thinking.</p>"},{"location":"AIEnhancement/#agents","title":"Agents","text":"<p>An agent is an AI-powered program that acts as a bridge between a language model and the external world. Instead of just generating text, it interprets the model's outputs to execute real-world tasks\u2014like retrieving information, interacting with software tools, or controlling hardware. Essentially, agents give language models the ability to \"do\" things, making them active participants in various workflows.</p> <p>Agents transform passive language model outputs into actionable commands that can manipulate external systems, thereby expanding the practical applications of AI beyond simple text generation.</p>"},{"location":"AIEnhancement/#examples-of-agents","title":"Examples of Agents","text":"<p>Agents can integrate multiple tools or services simultaneously to handle complex tasks.</p> <ul> <li> <p>Search Agent: An agent that receives a query from a language model and automatically uses a search API (like Google or Bing) to retrieve relevant information. It then processes and presents the search results, allowing the AI to provide up-to-date and accurate responses.</p> </li> <li> <p>Data Analysis Agent: An agent that integrates with data analysis libraries or environments (such as Python\u2019s Pandas and Matplotlib). When the language model generates a request for data analysis or visualization, the agent executes the necessary code, performs the analysis, and returns the output to the user.</p> </li> <li> <p>Task Automation Agent: Consider a virtual assistant that schedules meetings. Here, the language model interprets user requests (e.g., \"Schedule a meeting with John tomorrow at 3 PM\") and the agent interacts with calendar APIs to set up the meeting, send invites, and confirm availability.</p> </li> <li> <p>Chatbot with External API Calls: A conversational agent that not only chats with users but also interacts with external services, such as weather or news APIs. If a user asks, \u201cWhat\u2019s the weather like in New York?\u201d, the agent processes the request, calls a weather API, and then integrates the retrieved data into its response.</p> </li> </ul>"},{"location":"AIEnhancement/#libraries-and-codes","title":"Libraries and Codes","text":"<p>Post-Training Libraries:</p> <ul> <li>TRL from Hugging Face: A cutting-edge library designed for post-training foundation models using advanced techniques like Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO).</li> </ul> <p>Reasoning:</p> <ul> <li>Training a small math reasoner with RL: Qwen 0.5b on GRPO from @LiorOnAI</li> <li>TinyZero is a reproduction of DeepSeek R1 Zero built upon veRL.</li> <li>Fully open reproduction of DeepSeek-R1 by Hugging Face</li> </ul> <p>Agents:</p> <ul> <li>Open-source DeepResearch by Hugging Face \u2013 Freeing our search agents</li> </ul>"},{"location":"AIEnhancement/#courses","title":"Courses","text":"<ul> <li>deeplearning.ai short courses: post training of llms</li> <li>smol-course on Agents and post-trainig.</li> </ul>"},{"location":"AIEnhancement/#fully-open-models","title":"(Fully) Open Models","text":"<p>Fully open models release not only the model and weights but also data strategies and implementation details. Examples of fully open models include NVIDIA Eagle 2, Cambrian-1 and the LLaVA family (LLaVA-OneVision, LLAVA-v1), DeepSeek-R1.</p>"},{"location":"AIEnhancement/#model-and-architecture-enhancement","title":"Model and Architecture Enhancement","text":"<p>Titan (Memorize at test time), Jamba (Mamba, MoE, Attention), Mamba (SSM: state space model), Transformer (Attention, MLP), MoE (Mixture of experts) instead of MLP. DeepSeek AI's MLA (Multi-Head Latent Attention) instead of MQA (Multi-Quary Attention), GQA (Group-Quary Attention), MHA (Multi-Head Attention).</p> <p>Improvements: Rotary embeddings, RMSNorm, QK-Norm, and ReLU\u00b2, softcap logits, Muon optimizer</p> <p>Example improvement is training nanoGPT from 45 minutes to 3 minutes using 8 x H100 GPUs. From the github:</p> <pre><code>This improvement in training performance was brought about by the following techniques:\n\n- Modernized architecture: Rotary embeddings, QK-Norm, and ReLU\u00b2\n- Muon optimizer [writeup] [repo]\n- Untie head from embedding, use FP8 matmul for head, and softcap logits (latter following Gemma 2)\n- Projection and classification layers initialized to zero (muP-like)\n- Skip connections from embedding to every block as well as between blocks in U-net pattern\n- Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)\n- FlexAttention with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup\n</code></pre>"},{"location":"GEE/","title":"Google Earth Engine","text":"<p>Google Earth Engine (GEE) is a cloud based platform for working with large collections of geospatial data. It provides access to a vast catalog of satellite imagery and environmental datasets along with server side tools for processing them at global scale. Researchers, students, and analysts use it for tasks such as land cover mapping, vegetation monitoring, environmental change analysis, disaster assessment, and many other forms of earth observation. Because the computation happens on Google's servers, users can run complex spatial operations on huge datasets without needing powerful local hardware.</p>"},{"location":"GEE/#basic-tutorial","title":"Basic tutorial","text":"<p>Search for \"Google Earth Engine\". Go to the site and create a project under academic use.</p> <pre><code>pip install earthengine-api\npip install geemap\n# pip install notebook ipywidgets\n# pip install jupyterlab\n</code></pre> <pre><code>import ee\nimport geemap\n\nee.Authenticate()  # run once\nee.Initialize(project=\"my-project-project_id\")  # use your project ID from the GEE site\n\nMap = geemap.Map(center=[52.37, 4.89], zoom=10)\n\nimage = (\n    ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n    .filterBounds(ee.Geometry.Point([4.895168, 52.370216]))\n    .filterDate(\"2020-06-01\", \"2020-06-10\")\n    .sort(\"CLOUDY_PIXEL_PERCENTAGE\")\n    .first()\n)\n\nndvi = image.normalizedDifference([\"B8\", \"B4\"]).rename(\"NDVI\")\n\n# Add layers\nvis = {\"min\": 0, \"max\": 1, \"palette\": [\"blue\", \"white\", \"green\"]}\nMap.addLayer(ndvi, vis, \"NDVI\")\nMap.addLayerControl()\n\n# In a Jupyter notebook\nMap\n\n# Export as HTML for static viewing\n# Map.to_html(\"ndvi_map.html\")\n</code></pre>"},{"location":"companies/","title":"Companies","text":"<p>The methodology employed categorizes companies based on their primary focus or offerings, detailing each company's capabilities alongside its name.</p> <p>Key to Keywords: - Pre: Pretraining capabilities or services - FT: Finetuning services - Custom: Support for building custom models - API: Offers an API to integrate ML functionality - Dev: Development tools or environments - Dep: Deployment tools or platforms - Model: Develops or provides their own ML models - GPU: GPU-based compute resource provider - Storage: Data storage provider - Data: Data platform or data indexing provider</p>"},{"location":"companies/#highlights","title":"Highlights","text":"<p>Models are accessed via their respective companies, such as OpenAI (ChatGPT), DeepSeek, Anthropic (Claude), and Google (Gemini). <code>Hugging Face</code>, <code>vLLM</code>, <code>LM Studio</code>, <code>llama.cpp</code>, <code>Ollama</code>, <code>Jan.ai</code> are designed to let you run LLMs locally. <code>Together AI</code> provides a platform for running models that might not fit on your local machine. There are other serving locations, such as the Hugging Face Inference Playground, and <code>Hyperbolic.xyz</code>, which also offers base models. </p> <p>Manus AI (Monica.im) autonomously executes complex, goal-driven workflows as an AI agent, Devin AI serves as an AI-powered software engineer for coding and build automation, and Cursor functions as an intelligent IDE assistant that augments human developers across the software development lifecycle.</p> <p>For better prompting, it\u2019s recommended to add a <code>llms.txt</code> file at your site\u2019s root (analogous to <code>robots.txt</code>) to provide large language models with structured, machine-readable guidance. Additionally, Gitingest converts any Git repository into a single, prompt-friendly text digest\u2014complete with directory structure and file content\u2014making it ideal for feeding codebases into LLMs.</p> <p>The Model Context Protocol (MCP), introduced by Anthropic, is an open standard for connecting LLMs to external data sources and tools\u2014much like a USB-C port for AI applications (e.g., see How to Build an MCP Server with Gradio).</p> <p>DeepWiki, from Cognition Labs, offers AI-powered, interactive documentation for any GitHub repository. Just replace <code>github.com</code> with <code>deepwiki.com</code> in your repo URL to generate searchable, context-rich docs instantly.</p>"},{"location":"companies/#model-creators-foundation-model-providers","title":"Model Creators / Foundation Model Providers","text":"<ul> <li>OpenAI: Model, Pre, FT, API  </li> <li>Anthropic: Model, Pre, FT  </li> <li>Google DeepMind, Google: Model, Pre, FT, API  </li> <li>xAI: Model, Pre, FT, API  </li> <li>Meta AI: Model, Pre, FT, API  </li> <li>Mistral AI: Model, Pre, FT, API  </li> <li>Nous Research: Model, Pre, FT  </li> <li>Cohere: Model, Pre, FT, API  </li> <li>Allen Institute for AI (Ai2): Model, Pre, FT  </li> <li>EleutherAI: Model (Open-Source Models)  </li> <li>AssemblyAI: Model, API  </li> <li>Deepgram: Model, API  </li> <li>ElevenLabs: Model, API  </li> <li>Perplexity AI: Model, API</li> <li>Stability AI: Model, Pre, FT  </li> <li>Ideogram: Model</li> <li>Midjourney: Model</li> <li>Black Forest Labs: Model</li> <li>Pika Labs: Model</li> <li>Luma Labs: Model</li> <li>Minimax: Model</li> <li>Tencent AI Lab: Model</li> <li>ByteDance (owns TikTok): Model</li> <li>Inflection AI: Model</li> <li>Character.AI: Model</li> </ul>"},{"location":"companies/#apiinference-providers","title":"API/Inference Providers","text":"<ul> <li>Replicate: API, GPU, Custom</li> <li>Together AI: API, FT, Custom, GPU. Also finetune by WebUI.</li> <li>OpenRouter: API</li> <li>Groq.AI: API  </li> <li>Novita.ai: API, GPU </li> <li>Lepton.ai: API, GPU</li> <li>Hyperbolic.xyz: API, GPU  </li> <li>Fireworks AI: API, GPU  </li> <li>Baseten: API  </li> <li>Deepinfra: API, GPU</li> <li>Octo AI: API</li> </ul>"},{"location":"companies/#development-tools-code-generation-and-productivity","title":"Development Tools, Code Generation, and Productivity","text":"<ul> <li>Codeium: Dev (It developed both the Codeium AI code acceleration platform and the Windsurf IDE.)</li> <li>Replit AI: Dev</li> <li>Bolt from StackBlitz: Dev (Bolt platform)</li> <li>Anthropic Computer Use</li> <li>Amazon CodeWhisperer: Dev</li> <li>GitHub Copilot: Dev  </li> <li>Cursor: Dev</li> <li>Cognition Labs: Dev  </li> </ul> <p>Platforms like Bolt, Replit Agent, Vercel V0 use agentic workflows to improve code quality. They also help deploy generated applications. </p> <p>Other tools related:</p> <ul> <li>Llama Stack developed by Meta is a collection of APIs that standardize the building blocks necessary for developing generative AI applications.</li> <li>Adept's ACT-1: Adept AI has developed ACT-1, an AI model designed to interact with software applications through natural language commands.  It can perform tasks such as navigating web pages, clicking buttons, and entering data, effectively acting as a digital assistant to automate workflows.</li> </ul>"},{"location":"companies/#deployment-tools-and-platforms","title":"Deployment Tools and Platforms","text":"<ul> <li>Vercel: Dev, Dep (V0 by Vercel generates UI from text)</li> <li>Open WebUI: Dep (Manage and deploy AI models locally. UI for interacting with various LLMs. Integrate with LLM runners like Ollama and OpenAI-compatible APIs)</li> </ul>"},{"location":"companies/#libraries-and-frameworks","title":"Libraries and Frameworks","text":"<ul> <li>Ollama, vLLM: an open\u2011source, high\u2011throughput inference engine designed to run LLMs locally. It can be used with Chatbox.ai app desktop, mobile and web-based app.</li> <li>unsloth (Daniel Han), torchtune, Oxolotl: Fast Training (Pre/FT related).  They enhance the speed and efficiency of LLM fine-tuning.</li> <li>LiteLLM: LiteLLM is an open-source Python library designed to streamline interactions with a wide range of LLMs by providing a unified interface.</li> <li>ai-gradio: A Python package that makes it easy for developers to create machine learning apps powered by various AI providers. Built on top of Gradio, it provides a unified interface for multiple AI models and services.</li> <li>DSPy: Dev (Provides a programming model for developing and optimizing language model pipelines)  </li> <li>Torchrun: Dev  </li> <li>Cog (by Replicate): Packaging Custom ML Models for Deployment (Custom, Dep)  </li> <li>ComfyUI: Dev (GUI for Workflow)  </li> <li>Tinygrad: Dev  </li> <li>Agentic AI systems: LangChain, CrewAI, LlamaIndex, Haystack, Devin (Cognition)</li> <li>OpenHands: Dev (providing frameworks for building AI-driven applications with agentic capabilities)  </li> </ul>"},{"location":"companies/#data-platforms-and-providers","title":"Data Platforms and Providers","text":"<ul> <li>Encord: Data  </li> <li>LlamaIndex: Dev, Data Indexing  </li> <li>LAION: Data Provider, Open-Source Datasets</li> </ul>"},{"location":"companies/#end-to-end-cloud-based-ml-platforms","title":"End-to-End Cloud-Based ML Platforms","text":"<ul> <li>Google Vertex AI, AI Studio, GenAI SDK: API, Pre, FT, Custom, Dev, Dep, Model</li> <li>Amazon Bedrock: API, Pre, FT, Custom, Dev, Dep, Model  </li> <li>Microsoft Azure Machine Learning: API, Pre, FT, Custom, Dev, Dep, Model  </li> <li>Databricks: API, Dev, Dep, Model  </li> <li>IBM Watson Studio: Dev, Dep, Model  </li> <li>DataRobot: Dev, Dep, Model  </li> <li>H2O.ai: Dev, Dep, Model  </li> <li>Domino Data Lab: Dev, Dep, Model  </li> <li>Algorithmia: Dev, Dep, Model</li> </ul>"},{"location":"courses/","title":"Courses","text":""},{"location":"courses/#deep-learning-in-uva","title":"Deep learning in UvA","text":"<p>For self study, the video recordings and lecture notes are provided in the Uva Deep Learning 1 Course. </p> <p>Deep Learning 1, Deep Learning 2, Computer Vision 1, Computer Vision 2 (mainly 3D computer vision or a.k.a multiview geometry) are in the DataNose, under <code>program list \\ Master Artificial Intelligent</code>.</p>"},{"location":"courses/#online-courses","title":"Online Courses","text":""},{"location":"courses/#computer-vision-deep-learning-for-computer-vision","title":"Computer Vision | Deep Learning for Computer Vision","text":"<ul> <li>Stanford CS231n: Deep Learning for Computer Vision: Videos newer, Videos winter 2016 and lecture notes. Deep learning based 2D computer vision. There is also MIT 6.S191: Introduction to Deep Learning and Stanford CS 25 transformers united.</li> <li>Photogrammetry I &amp; II by Cyrill Stachniss: Contains both tranditional and deep-learning based computer vision in 2D and 3D.</li> <li>Computer Vision by Andreas Geiger: Contains both tranditional and deep-learning based 2D and 3D computer vision.</li> <li>EfficientML.ai Lecture, Fall 2023, MIT 6.5940</li> </ul>"},{"location":"courses/#traditional-computer-vision","title":"Traditional Computer Vision","text":"<ul> <li>Photogrammetric Computer Vision by Cyrill Stachniss</li> <li>Multi View Geometry by Daniel Cremers: For in depth understdaning of multi view geometry, SfM (Structure from Motion), SLAM (Simultaneous Localization and Mapping). It is not a easy course. </li> </ul>"},{"location":"ddp/","title":"DDP (Distributed Data Parallel) in PyTorch","text":"<p>This manual is from the PyTorch DDP tutorial. The code can be found here. This manual summarizes the changes needed when transitioning from a single GPU to multiple GPUs on a single node, both with and without torchrun, as well as multiple GPUs across multiple nodes. You can compare the code yourself using the <code>diff</code> command:</p> <pre><code>diff --color -U 0 multigpu.py multigpu_torchrun.py\n</code></pre>"},{"location":"ddp/#single-node-multiple-gpu","title":"Single node multiple GPU","text":""},{"location":"ddp/#imports","title":"Imports","text":"<pre><code>import torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n</code></pre>"},{"location":"ddp/#script-function","title":"Script function","text":"<pre><code>def main(rank, world_size, other_args):\n    ddp_setup(rank, world_size)\n    # define dataset, model, optimizer, trainer\n    destroy_process_group()\n\n\nworld_size = torch.cuda.device_count()\nmp.spawn(main, args=(world_size, other_args,), nprocs=world_size)\n</code></pre>"},{"location":"ddp/#ddp-setup","title":"DDP setup","text":"<pre><code>os.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12355\"\ninit_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\ntorch.cuda.set_device(rank)\n</code></pre>"},{"location":"ddp/#model","title":"model","text":"<pre><code>- self.model = model.to(gpu_id)\n+ self.model = DDP(model, device_ids=[gpu_id])\n</code></pre>"},{"location":"ddp/#data","title":"data","text":"<pre><code>train_data = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n-   shuffle=True,\n+   shuffle=False,\n+   sampler=DistributedSampler(train_dataset),\n)\n</code></pre>"},{"location":"ddp/#shuffling-across-multiple-epochs","title":"Shuffling across multiple epochs","text":"<p>Calling the set_epoch() method on the DistributedSampler at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch.</p> <pre><code>for epoch in epochs:\n    train_data.sampler.set_epoch(epoch)\n    for source, targets in train_data:\n</code></pre>"},{"location":"ddp/#save-checkpoints","title":"Save checkpoints","text":"<p>We only need to save model checkpoints from one process.</p> <pre><code>- ckp = model.state_dict()\n+ ckp = model.module.state_dict()\n- if epoch % save_every == 0:\n+ if gpu_id == 0 and epoch % save_every == 0:\n</code></pre>"},{"location":"ddp/#slurm-job","title":"Slurm job","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --gpus=2\n#SBATCH --cpus-per-task=18\n#SBATCH --partition=gpu\n#SBATCH -o test_multigpu_%j.out\npython python_script.py arguments\n# python multigpu.py 50 10\n</code></pre>"},{"location":"ddp/#single-node-multiple-gpu-with-torchrun","title":"Single node multiple GPU with torchrun","text":""},{"location":"ddp/#script-function_1","title":"Script function","text":"<pre><code>-  world_size = torch.cuda.device_count()\n-  mp.spawn(main, args=(world_size, other_args,), nprocs=world_size)\n+  main(other_args)\n</code></pre>"},{"location":"ddp/#ddp-setup_1","title":"DDP setup","text":"<p>torchrun provided environment variables <code>os.environ[\"LOCAL_RANK\"]</code> for the GPU id:</p> <pre><code>gpu_id = int(os.environ[\"LOCAL_RANK\"])\n</code></pre> <pre><code>- os.environ[\"MASTER_ADDR\"] = \"localhost\"\n- os.environ[\"MASTER_PORT\"] = \"12355\"\n- init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n+ init_process_group(backend=\"nccl\")\n+ torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n</code></pre>"},{"location":"ddp/#slurm-job_1","title":"Slurm job","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --gpus=2\n#SBATCH --cpus-per-task=18\n#SBATCH --partition=gpu\n#SBATCH --time=00:05:00\n#SBATCH -o test_multigpu_torchrun_%j.out    \ntorchrun --nnodes=1 --nproc_per_node=2 python_script.py arguments\n# torchrun --nnodes=1 --nproc_per_node=2 multigpu_torchrun.py 50 10\n</code></pre>"},{"location":"ddp/#multi-node-multi-gpu","title":"Multi node multi GPU","text":"<p>The only diffeerence with previous one:</p> <pre><code>+ local_rank = int(os.environ[\"LOCAL_RANK\"])\n+ global_rank = int(os.environ[\"RANK\"])\n+ model = model.to(local_rank)\n\n- local_rank = int(os.environ[\"LOCAL_RANK\"]) # local_rank == gpu_id\n- model = model.to(local_rank)\n</code></pre>"},{"location":"ddp/#slurm-job_2","title":"Slurm job","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus-per-node=4\n#SBATCH --cpus-per-task=18\n#SBATCH --partition=gpu\n#SBATCH --time=00:05:00\n#SBATCH -o test_multinode_%j.out  \ntorchrun --nnodes=2 --nproc_per_node=4 python_script.py arguments\n# torchrun --nnodes=2 --nproc_per_node=1 multigpu_torchrun.py 50 10\n</code></pre>"},{"location":"ddp/#references","title":"References","text":"<ul> <li>https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff</li> <li>https://medium.com/pytorch/pytorch-data-parallel-best-practices-on-google-cloud-6c8da2be180d</li> <li>https://medium.com/pytorch/pytorch-sessions-at-nvidia-gtc-march-20-2023-b86210711c9b</li> </ul>"},{"location":"deep_learning_project_setup/","title":"Deep Learning Project Setup","text":""},{"location":"deep_learning_project_setup/#deep-learning-project-setup","title":"Deep Learning Project Setup","text":"<p>To establish a foundational deep learning project that includes data loading, model definition, training with validation, and hyperparameter tuning, consider the following structured approach. This setup is modular, facilitating scalability and maintenance.</p>"},{"location":"deep_learning_project_setup/#folder-structure","title":"Folder Structure","text":"<pre><code>project/\n\u2502\n\u251c\u2500\u2500 data/               # Data loading scripts\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 dataset.py      # Custom dataset script\n\u2502\n\u251c\u2500\u2500 models/             # Model architectures\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 model.py        # Model definition\n\u2502\n\u251c\u2500\u2500 configs/            # Configuration files\n\u2502   \u2514\u2500\u2500 config.yaml     # Hyperparameter configurations\n\u2502\n\u251c\u2500\u2500 scripts/            # Training and evaluation scripts\n\u2502   \u2514\u2500\u2500 train.py        # Training script\n\u2502\n\u251c\u2500\u2500 utils/              # Utility functions\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 utils.py        # Helper functions (e.g., logging, metrics)\n\u2502\n\u251c\u2500\u2500 main.py             # Entry point of the project\n\u2514\u2500\u2500 requirements.txt    # List of required packages\n</code></pre>"},{"location":"deep_learning_project_setup/#dataset-loader","title":"Dataset Loader","text":"<pre><code># data/dataset.py\nimport torch\nfrom torch.utils.data import Dataset\n\nclass SimpleDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return {\n            \"data\": torch.tensor(self.data[idx], dtype=torch.float32),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n</code></pre>"},{"location":"deep_learning_project_setup/#model-definition","title":"Model Definition","text":"<pre><code># models/model.py\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(SimpleModel, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n</code></pre>"},{"location":"deep_learning_project_setup/#training-script-with-validation","title":"Training Script with Validation","text":"<pre><code># scripts/train.py\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom data.dataset import SimpleDataset\nfrom models.model import SimpleModel\nfrom sklearn.metrics import accuracy_score\n\ndef train_model(config, dataset):\n    # Determine sizes for training and validation sets\n    total_size = len(dataset)\n    val_size = int(total_size * config['val_split'])\n    train_size = total_size - val_size\n\n    # Split the dataset\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n\n    # Model, Loss, Optimizer\n    model = SimpleModel(input_size=dataset[0]['data'].shape[0], num_classes=len(set(dataset.labels)))\n    criterion = CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=config['learning_rate'])\n\n    # TensorBoard Summary Writer\n    writer = SummaryWriter(log_dir=config['log_dir'])\n\n    # Training Loop\n    for epoch in range(config['epochs']):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader):\n            data, labels = batch['data'], batch['label']\n\n            # Forward pass\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{config['epochs']}], Training Loss: {avg_train_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        all_preds = []\n        all_labels = []\n        with torch.no_grad():\n            for batch in val_loader:\n                data, labels = batch['data'], batch['label']\n                outputs = model(data)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                preds = torch.argmax(outputs, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_accuracy = accuracy_score(all_labels, all_preds)\n        print(f\"Epoch [{epoch+1}/{config['epochs']}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n        # Log metrics to TensorBoard\n        writer.add_scalar('Training Loss', avg_train_loss, epoch + 1)\n        writer.add_scalar('Validation Loss', avg_val_loss, epoch + 1)\n        writer.add_scalar('Validation Accuracy', val_accuracy, epoch + 1)\n\n    writer.close()\n</code></pre>"},{"location":"deep_learning_project_setup/#main-entry","title":"Main Entry","text":"<pre><code># main.py\nimport yaml\nimport numpy as np\nfrom data.dataset import SimpleDataset\nfrom scripts.train import train_model\n\nif __name__ == \"__main__\":\n    # Load configurations\n    with open(\"configs/config.yaml\", \"r\") as file:\n        config = yaml.safe_load(file)\n\n    # Dummy data (replace with real data loading)\n    X = np.random.rand(1000, 10)\n    y = np.random.randint(0, 2, 1000)\n\n    # Create dataset\n    dataset = SimpleDataset(X, y)\n\n    # Train the model\n    train_model(config, dataset)\n</code></pre>"},{"location":"deep_learning_project_setup/#configuration","title":"Configuration","text":"<p>Ensure your configuration file includes the <code>log_dir</code> for TensorBoard logs and a <code>val_split</code> parameter to define the proportion of data used for validation.</p> <pre><code># configs/config.yaml\nbatch_size: 16\nlearning_rate: 0.001\nepochs: 10\nlog_dir: 'runs/experiment_1'\nval_split: 0.2  # 20% of data used for validation\n</code></pre>"},{"location":"deep_learning_project_setup/#running-the-code","title":"Running the Code","text":"<ol> <li>Install Dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Run the Training:</li> </ol> <pre><code>python main.py\n</code></pre> <ol> <li>Launch TensorBoard:</li> </ol> <pre><code>tensorboard --port 4004 --logdir=runs\n</code></pre> <p>Open the provided URL in your browser to access the TensorBoard dashboard.</p> <p>By integrating PyTorch's <code>random_split</code> function, you can effectively partition your dataset into training and validation sets, facilitating model evaluation without the need for external libraries. </p>"},{"location":"deep_learning_project_setup/#simplified-hyperparameter-tuning","title":"Simplified Hyperparameter Tuning","text":"<p>For a more concise approach to hyperparameter tuning, consider using a configuration file to define multiple sets of hyperparameters and iterate over them. Here's an example:</p>"},{"location":"deep_learning_project_setup/#define-hyperparameter-sets","title":"Define Hyperparameter Sets:","text":"<p>Create a YAML file listing different hyperparameter configurations:</p> <pre><code># configs/hyperparams.yaml\nexperiments:\n    - batch_size: 16\n    learning_rate: 0.001\n    epochs: 5\n    log_dir: 'runs/exp1'\n    - batch_size: 32\n    learning_rate: 0.01\n    epochs: 10\n    log_dir: 'runs/exp2'\n</code></pre>"},{"location":"deep_learning_project_setup/#modify-the-main-script","title":"Modify the Main Script:","text":"<p>Update your <code>main.py</code> to load and iterate over these configurations:</p> <pre><code># main.py\nimport yaml\nfrom scripts.train import train_model\n\nif __name__ == \"__main__\":\n    # Load hyperparameter configurations\n    with open(\"configs/hyperparams.yaml\", \"r\") as file:\n        experiments = yaml.safe_load(file)['experiments']\n\n    # Iterate over each configuration\n    for idx, config in enumerate(experiments):\n        print(f\"Running experiment {idx + 1}/{len(experiments)} with config: {config}\")\n        train_model(config)\n</code></pre> <p>This approach allows you to manage multiple experiments efficiently, with each configuration's results logged separately for easy comparison.</p>"},{"location":"deep_learning_project_setup/#references","title":"References:","text":"<ul> <li>How to use TensorBoard with PyTorch</li> <li>Visualizing Models, Data, and Training with TensorBoard </li> </ul>"},{"location":"distribution_metrics/","title":"Metrics for Comparing Distributions","text":"<p>This document summarizes common metrics used to compare probability distributions. These measures are often applied in statistics, machine learning, and information theory to quantify similarity, dissimilarity, or divergence between distributions.</p>"},{"location":"distribution_metrics/#1-kullbackleibler-kl-divergence","title":"1. Kullback\u2013Leibler (KL) Divergence","text":"<ul> <li>Type: Divergence (not symmetric, not a true distance).</li> <li>Definition: <code>D_KL(P\u2016Q) = \u03a3 P(x) log(P(x) / Q(x))</code></li> <li>Interpretation: Measures how much information is lost when distribution Q is used to approximate P.</li> <li>Notes: Asymmetric; can be infinite if <code>Q(x) = 0</code> where <code>P(x) &gt; 0</code>.</li> </ul>"},{"location":"distribution_metrics/#2-jensenshannon-divergence-jsd","title":"2. Jensen\u2013Shannon Divergence (JSD)","text":"<ul> <li>Type: Symmetrized and smoothed version of KL divergence.</li> <li>Definition: <code>D_JS(P\u2016Q) = \u00bd D_KL(P\u2016M) + \u00bd D_KL(Q\u2016M)</code> where <code>M = \u00bd(P + Q)</code></li> <li>Interpretation: Bounded between 0 and 1 (when using log base 2).   Often used in clustering and GAN training.</li> <li>Notes: Square root of JSD is a proper metric.</li> </ul>"},{"location":"distribution_metrics/#3-cross-entropy","title":"3. Cross-Entropy","text":"<ul> <li>Type: Expectation-based measure.</li> <li>Definition: <code>H(P, Q) = - \u03a3 P(x) log Q(x)</code></li> <li>Interpretation: Expected number of bits to encode samples from P when using a code optimized for Q.</li> <li>Notes: Common in machine learning as a loss function (e.g., classification tasks).</li> </ul>"},{"location":"distribution_metrics/#4-bhattacharyya-coefficient-distance","title":"4. Bhattacharyya Coefficient &amp; Distance","text":"<ul> <li>Coefficient: <code>BC(P, Q) = \u03a3 sqrt(P(x) * Q(x))</code>   (measures overlap between distributions; ranges from 0 to 1).</li> <li>Distance: <code>D_B(P, Q) = -ln(BC(P, Q))</code></li> <li>Interpretation: Higher overlap \u2192 smaller distance.   Used in pattern recognition and Bayesian classification.</li> </ul>"},{"location":"distribution_metrics/#5-earth-movers-distance-emd-wasserstein-distance","title":"5. Earth Mover\u2019s Distance (EMD) / Wasserstein Distance","text":"<ul> <li>Type: True metric (for certain conditions).</li> <li>Definition: Informally, the minimal \"cost\" of transforming one distribution into another,   where cost is the amount of probability mass moved times the distance it is moved.</li> <li>Interpretation: Reflects differences in support and geometry.   Often used in computer vision and GANs (Wasserstein GAN).</li> <li>Notes: Computationally more expensive than KL or JSD.</li> </ul>"},{"location":"distribution_metrics/#6-hellinger-distance","title":"6. Hellinger Distance","text":"<ul> <li>Type: True metric.</li> <li>Definition: <code>H(P, Q) = (1/\u221a2) * sqrt( \u03a3 ( sqrt(P(x)) - sqrt(Q(x)) )\u00b2 )</code></li> <li>Interpretation: Related to Bhattacharyya coefficient.   Ranges between 0 (identical) and 1 (maximally different).</li> <li>Notes: Symmetric and bounded.</li> </ul>"},{"location":"distribution_metrics/#7-total-variation-tv-distance","title":"7. Total Variation (TV) Distance","text":"<ul> <li>Type: Metric.</li> <li>Definition: <code>D_TV(P, Q) = \u00bd \u03a3 |P(x) - Q(x)|</code></li> <li>Interpretation: Maximum difference in probabilities assigned by P and Q over all events.   Represents the largest discrepancy in outcome probabilities.</li> <li>Notes: Bounded between 0 and 1.</li> </ul>"},{"location":"distribution_metrics/#8-maximum-mean-discrepancy-mmd","title":"8. Maximum Mean Discrepancy (MMD)","text":"<ul> <li>Type: Kernel-based metric (generalization of Total Variation).</li> <li>Definition: <code>MMD(P, Q; k) = || E_P[k(x,\u00b7)] - E_Q[k(x,\u00b7)] ||_H</code>   where <code>k</code> is a kernel (e.g., Gaussian RBF), and <code>H</code> is the corresponding reproducing kernel Hilbert space (RKHS).</li> <li>Interpretation: Measures how distinguishable distributions P and Q are when mapped into a feature space defined by the kernel.   With a universal kernel, MMD = 0 if and only if P = Q.</li> <li>Relation to TV: If the kernel is chosen as a Dirac delta, MMD reduces to the Total Variation distance.</li> <li>Notes: Commonly used in two-sample tests, domain adaptation, and Generative Models (e.g., MMD-GAN).</li> </ul> Metric Symmetric Bounded True Metric Notes KL Divergence \u2717 \u2717 \u2717 Asymmetric, infinite possible Jensen\u2013Shannon Divergence \u2713 \u2713 \u221a(JSD) only Smoothed KL, used in ML Cross-Entropy \u2717 \u2717 \u2717 Common ML loss Bhattacharyya Distance \u2713 \u2713 \u2717 Based on overlap Earth Mover\u2019s (Wasserstein) \u2713 \u2713 \u2713 Captures geometry Hellinger Distance \u2713 \u2713 \u2713 Related to Bhattacharyya Total Variation \u2713 \u2713 \u2713 Intuitive probability gap Maximum Mean Discrepancy \u2713 Depends \u2713 Kernelized extension of TV"},{"location":"distribution_metrics/#further-reading-and-notes","title":"Further Reading and Notes","text":"<ul> <li>In Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning, the choice of distribution metric significantly impacts the quality of learned representations. </li> </ul>"},{"location":"docker/","title":"DOCKER","text":""},{"location":"docker/#installation-for-ubuntu","title":"Installation for Ubuntu","text":"<p>More information can be found on the \"engine install ubuntu\".</p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install the latest version\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>More information can be found on the linux postinstall.</p> <pre><code># Create the `docker` group. The Docker group already exists, so there is no need to run this command.\nsudo groupadd docker\n\n# Add your user to the docker group.\nsudo usermod -aG docker $USER\n\n# Activate the changes to groups. \n# I think it is better to restart the system than to run this command. I encountered a weird issue when I installed Docker Compose.\nnewgrp docker\n\n# Verify docker\ndocker run hello-world\n</code></pre>"},{"location":"docker/#uninstall-docker-engine","title":"Uninstall Docker Engine","text":"<p>More information can be found on the official website.</p> <pre><code>sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras\n\nsudo rm -rf /var/lib/docker\nsudo rm -rf /var/lib/containerd\n\nsudo rm /etc/apt/sources.list.d/docker.list\nsudo rm /etc/apt/keyrings/docker.asc\n</code></pre>"},{"location":"docker/#basic-commands","title":"Basic Commands","text":"<p>Here's a guide to essential Docker commands, followed by a cheat sheet for quick reference.</p>"},{"location":"docker/#listing-docker-images-and-containers","title":"Listing Docker Images and Containers","text":""},{"location":"docker/#list-images","title":"List Images:","text":"<p>To view all Docker images on your system:</p> <pre><code>docker images\n</code></pre> <p>This displays a table of images with details like repository, tag, and image ID.</p>"},{"location":"docker/#list-running-containers","title":"List Running Containers:","text":"<p>To see all currently running containers:</p> <pre><code>docker ps\n</code></pre> <p>This shows a list of active containers with their IDs, names, and statuses.</p>"},{"location":"docker/#list-all-containers","title":"List All Containers","text":"<p>To list all containers, including those that are stopped:</p> <pre><code>docker ps -a\n</code></pre> <p>This provides a comprehensive list of all containers, regardless of their state.</p> <p>To display the full command without truncation:</p> <pre><code>docker ps -a --no-trunc\n</code></pre>"},{"location":"docker/#managing-docker-images","title":"Managing Docker Images","text":""},{"location":"docker/#pull-an-image","title":"Pull an Image","text":"<p>To download an image from Docker Hub:</p> <pre><code>docker pull &lt;image_name&gt;\n</code></pre> <p>Replace <code>&lt;image_name&gt;</code> with the desired image, e.g., <code>ubuntu</code>.</p>"},{"location":"docker/#remove-an-image","title":"Remove an Image","text":"<p>To delete a specific image:</p> <pre><code>docker rmi &lt;image_name_or_id&gt;\n</code></pre> <p>Use the image name or ID from the <code>docker images</code> list.</p>"},{"location":"docker/#building-docker-images","title":"Building Docker Images","text":""},{"location":"docker/#build-an-image-from-a-dockerfile","title":"Build an Image from a Dockerfile","text":"<p>To create a Docker image from a Dockerfile:</p> <pre><code>docker build -t &lt;image_name&gt; &lt;path_to_dockerfile_directory&gt;\n</code></pre> <p>Replace <code>&lt;image_name&gt;</code> with your desired image name and <code>&lt;path_to_dockerfile_directory&gt;</code> with the path to the directory containing your Dockerfile. The <code>-t</code> flag tags the image with a name.</p> <p>Example:</p> <pre><code>docker build -t myapp:latest .\n</code></pre> <p>This builds an image named <code>myapp</code> with the tag <code>latest</code> from the Dockerfile in the current directory.</p>"},{"location":"docker/#running-and-managing-containers","title":"Running and Managing Containers","text":""},{"location":"docker/#run-a-container","title":"Run a Container","text":"<p>To create and start a new container:</p> <pre><code>docker run [OPTIONS] &lt;image_name&gt;\n</code></pre> <p>Common options include: - <code>-d</code>: Run the container in detached mode (in the background). - <code>-it</code>: Run the container in interactive mode with a terminal. - <code>--name &lt;container_name&gt;</code>: Assign a name to the container. - <code>-p &lt;host_port&gt;:&lt;container_port&gt;</code>: Map host port to container port. - <code>-v &lt;host_directory&gt;:&lt;container_directory&gt;</code>: Mount a host directory as a volume in the container.</p> <p>Example:</p> <pre><code>docker run -it bird-behavior bash\n</code></pre> <p>Example:</p> <p>This runs a container named <code>bird-behavior</code> and opens an interactive bash shell inside the <code>bird-behavior</code> container.</p> <pre><code>docker run -d -p 8080:80 -v /host/data:/container/data --name mynginx nginx\n</code></pre> <p>This runs an Nginx container named <code>mynginx</code> in detached mode, mapping port 8080 on the host to port 80 in the container, and mounts the host directory <code>/host/data</code> to <code>/container/data</code> in the container.</p> <p>Resource constraints: For more details check here.</p> <pre><code>docker run --cpus=\"4\" --memory=\"8g\" ...\n</code></pre>"},{"location":"docker/#execute-commands-in-a-running-container","title":"Execute Commands in a Running Container","text":"<p>To run a command inside a running container:</p> <pre><code>docker exec [OPTIONS] &lt;container_name_or_id&gt; &lt;command&gt;\n ```\n\nCommon options:\n- `-it`: Run in interactive mode with a terminal.\n\nExample:\n\n```bash\ndocker exec -it mynginx /bin/bash\n</code></pre> <p>This opens an interactive bash shell inside the <code>mynginx</code> container.</p>"},{"location":"docker/#stop-a-running-container","title":"Stop a Running Container","text":"<p>To stop a container:</p> <pre><code>docker stop &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#start-a-stopped-container","title":"Start a Stopped Container","text":"<p>To start a container that has been stopped:</p> <pre><code>docker start &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#remove-a-container","title":"Remove a Container","text":"<p>To delete a container:</p> <pre><code>docker rm &lt;container_name_or_id&gt;\n</code></pre> <p>Note: Ensure the container is stopped before removing it.</p>"},{"location":"docker/#viewing-logs-and-inspecting-containers","title":"Viewing Logs and Inspecting Containers","text":""},{"location":"docker/#view-container-logs","title":"View Container Logs","text":"<p>To see the logs of a container:</p> <pre><code>docker logs &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#inspect-container-details","title":"Inspect Container Details","text":"<p>To get detailed information about a container:</p> <pre><code>docker inspect &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is a tool that simplifies running applications with multiple containers. By defining services, networks, and volumes in a single YAML file, you can start and manage all components of your application with one command. </p> <p>You can find an example of a Docker Compose file here. The description is provided below.</p>"},{"location":"docker/#creating-a-docker-composeyml-file","title":"Creating a <code>docker-compose.yml</code> File","text":"<p>Create a <code>docker-compose.yml</code> file with the following content:</p> <pre><code>version: '3'\nservices:\n  jekyll:\n    image: jekyll/jekyll:latest\n    command: jekyll serve --watch --incremental\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - .:/srv/jekyll\n</code></pre>"},{"location":"docker/#running-the-docker-compose-file","title":"Running the Docker Compose File","text":"<p>To start the services defined in your <code>docker-compose.yml</code> file, run:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"docker/#cheat-sheet","title":"Cheat Sheet","text":"Command Description <code>docker images</code> List all Docker images <code>docker ps</code> List running containers <code>docker ps -a</code> List all containers (running and stopped) <code>docker pull &lt;image_name&gt;</code> Pull an image from Docker Hub <code>docker rmi &lt;image_name_or_id&gt;</code> Remove a Docker image <code>docker build -t &lt;image_name&gt; &lt;path&gt;</code> Build an image from a Dockerfile <code>docker run [OPTIONS] &lt;image_name&gt;</code> Run a new container <code>docker exec [OPTIONS] &lt;container&gt; &lt;command&gt;</code> Execute a command in a running container <code>docker stop &lt;container_name_or_id&gt;</code> Stop a running container <code>docker start &lt;container_name_or_id&gt;</code> Start a stopped container <code>docker rm &lt;container_name_or_id&gt;</code> Remove a container <code>docker logs &lt;container_name_or_id&gt;</code> View logs of a container <code>docker inspect &lt;container_name_or_id&gt;</code> Inspect detailed information of a container"},{"location":"docker/#installing-the-nvidia-container-toolkit","title":"Installing the NVIDIA Container Toolkit","text":"<p>Follow these instructions from NVIDIA Container Toolkit Installation Guide to enable GPU support in Docker containers:</p> <p>First, back up your NVIDIA packages in case anything breaks during installation:</p> <pre><code>dpkg --get-selections | grep -i nvidia &gt; ~/nvidia-packages-backup.txt\n</code></pre> <p>Then proceed with the installation steps below.</p> <pre><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt update\nsudo apt install -y nvidia-container-toolkit\n</code></pre>"},{"location":"docker/#configuring-docker-with-nvidia-runtime","title":"Configuring Docker with NVIDIA Runtime","text":"<ol> <li>After installing the NVIDIA Container Toolkit, configure Docker to use the NVIDIA runtime:</li> </ol> <pre><code>sudo nvidia-ctk runtime configure --runtime=docker\n</code></pre> <p>This command updates <code>/etc/docker/daemon.json</code> to enable NVIDIA GPU support in containers. The file will contain:</p> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"args\": [],\n            \"path\": \"nvidia-container-runtime\"\n        }\n    }\n}\n</code></pre> <ol> <li>Restart the Docker daemon:</li> </ol> <pre><code>sudo systemctl restart docker\n</code></pre> <ol> <li>Verify the installation:</li> </ol> <pre><code>docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-sm\n</code></pre>"},{"location":"docker/#rollback-installation","title":"Rollback Installation","text":"<pre><code># Rollback Installation\n# Remove the Nvidia GPG Key\nsudo rm -f /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n# Remove the Nvidia Container Source List\nsudo rm -f /etc/apt/sources.list.d/nvidia-container-toolkit.list\n# Refresh the Package List\nsudo apt update\nsudo apt remove --purge nvidia-container-toolkit\nsudo apt autoremove\n\n# Reinstall Your Nvidia Drivers\n# If your GPU stops working, reinstall the driver listed in your backup (e.g., nvidia-driver-535):\nsudo apt install --reinstall nvidia-driver-535  # adjust version to match your backup\n\n# If you had issues with missing dependencies after reinstalling the driver, running the full \n# reinstall command ensures everything is restored.\ncat ~/nvidia-packages-backup.txt | awk '{print $1}' | xargs sudo apt install --reinstall -y\n\n# Reboot and Check\nsudo reboot\nnvidia-smi\n</code></pre>"},{"location":"docker/#apptainer","title":"Apptainer","text":"<p>In Snellius, there is Apptainer instead of Docker.</p> <p>Apptainer (formerly Singularity) is a containerization tool designed for high-performance computing (HPC), scientific workloads, and secure application deployment. Unlike Docker, Apptainer focuses on security, reproducibility, and portability, allowing users to run containers without requiring root privileges. It uses single-file SIF (Singularity Image Format) images, making it ideal for environments like HPC clusters and supercomputers.</p> <p>The equivalent commands for the Docker commands you mentioned are:</p> Command Description <code>apptainer cache list</code> <code>docker images</code> <code>apptainer instance list</code> <code>docker ps -a</code> <code>apptainer cache clean</code> <code>docker rmi imageid</code>. See example below. <code>docker rm containerid</code> <code>apptainer instance stop &lt;instance_name&gt;</code> <p>These directories store the downloaded images.</p> <pre><code>ls ~/.apptainer/cache/library\nls ~/.apptainer/cache/oci-tmp\n</code></pre> <p>E.g. If you pulled docker using apptainer pull, it should have been saved as a <code>.sif</code> file in your working directory </p> <pre><code>apptainer pull docker://godlovedc/lolcow \napptainer inspect lolcow_latest.sif\n# Since Apptainer doesn\u2019t use a centralized image store like Docker, you typically just remove the .sif file:\nrm lolcow_latest.sif\n# If you want to clean the cache (which includes OCI blobs and temporary files), use:\napptainer cache clean\n</code></pre>"},{"location":"generative/","title":"Generative AI","text":""},{"location":"generative/#generative-models","title":"Generative Models","text":""},{"location":"generative/#diffusion-model-related","title":"Diffusion model related","text":"<ul> <li>Diffusion model: DDPM, Ho 2020;Sohl-Dickstein 2015; Score-based, Song 2019, Song 2021; Tuturial AssemblyAI; Li'Ling</li> <li>Flow matching /  Lipman, Lipman Guide and Code, Introduction by Cambridge; GMFlow</li> <li>Rectified Flow: Liu; Liu</li> <li>Normalizing Flow</li> </ul>"},{"location":"generative/#to-be-organized","title":"TO be organized","text":""},{"location":"generative/#code","title":"Code:","text":"<ul> <li>Pyramid-Flow</li> <li>Flow Matching</li> <li>Stable Diffusion 3.5</li> </ul>"},{"location":"generative/#new-works","title":"New works","text":"<ul> <li>Kaiming He: Mean Flows (similar to Align Your Flow), Dispersive Loss</li> </ul>"},{"location":"git/","title":"Git","text":""},{"location":"git/#git-basics-a-simple-manual","title":"Git Basics: A Simple Manual","text":"<p>Git is a version control system that helps you manage your code and collaborate with others. This manual will cover some basic Git commands to get you started.</p>"},{"location":"git/#setting-up-git","title":"Setting Up Git","text":"<ul> <li>Install Git on your computer. You can download it from here and follow the installation instructions.</li> </ul>"},{"location":"git/#configuring-git","title":"Configuring Git","text":"<ul> <li>After installation, set your name and email using the following commands:</li> </ul> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"git/#creating-a-new-repository","title":"Creating a New Repository","text":"<ul> <li>Local Repository: To start a new project with Git, create a new directory and navigate into it. Then run:</li> </ul> <pre><code>git init\n</code></pre> <p>This step is better to be done on GitHub.</p> <ul> <li>Clone Repository: To work on an existing project, clone it from a remote repository using:</li> </ul> <pre><code>git clone &lt;remote_url&gt;\n</code></pre> <ul> <li>Switching Branches: Create and switch to a new branch with:</li> </ul> <pre><code>git checkout -b &lt;branch_name&gt;\n</code></pre>"},{"location":"git/#basic-commands","title":"Basic Commands","text":"<ul> <li>Adding Files: Stage changes for commit using:</li> </ul> <pre><code>git add &lt;filename&gt;\n</code></pre> <ul> <li>Committing Changes: Save staged changes to your local repository with:</li> </ul> <pre><code>git commit -m \"Your commit message\"\n</code></pre> <p>Use <code>-a</code> to tell the command to automatically stage files that have been modified and deleted. So, you don't need to use <code>git add</code> anymore. For new files, <code>git add</code> should be used.</p> <pre><code>git commit -am \"Your commit message\"\n</code></pre>"},{"location":"git/#collaboration","title":"Collaboration","text":"<ul> <li>Pushing Changes: Upload your local commits to a remote repository using:</li> </ul> <pre><code>git push origin &lt;branch_name&gt;\n</code></pre> <ul> <li>Pulling Changes: Retrieve and merge changes from the remote repository with:</li> </ul> <pre><code>git pull origin &lt;branch_name&gt;\n</code></pre>"},{"location":"git/#view-commands","title":"View Commands","text":"<ul> <li>Checking Status: To see which files are staged or modified, use:</li> </ul> <pre><code>git status\n</code></pre> <ul> <li>Viewing Commit History: See a list of past commits using:</li> </ul> <pre><code>git log\n</code></pre>"},{"location":"git/#git-advanced","title":"Git Advanced","text":""},{"location":"git/#git-lfs-large-file-storage-guide","title":"Git LFS (Large File Storage) Guide","text":"<p>Git LFS is a tool that helps manage large files in Git repositories efficiently. It stores large files (like images, videos, etc.) outside the repository while keeping references to them in your version control system. This reduces the size of your Git repository and optimizes performance.</p>"},{"location":"git/#step-1-install-git-lfs","title":"Step 1: Install Git LFS","text":"<p>To begin using Git LFS, you need to install it on your system:</p> <pre><code>sudo apt install git-lfs\n</code></pre> <p>If you don't have sudo rights, follow these steps instead:</p> <pre><code>mkdir -p ~/bin\ncd ~/bin\nwget https://github.com/git-lfs/git-lfs/releases/latest/download/git-lfs-linux-amd64-v3.6.1.tar.gz\ntar -xvzf git-lfs-linux-amd64-v3.6.1.tar.gz\nrm git-lfs-linux-amd64-v3.6.1.tar.gz\necho 'export PATH=$HOME/bin/git-lfs-3.6.1:$PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Note: Check the latest version at https://github.com/git-lfs/git-lfs/releases and adjust the version number in the commands accordingly. The version shown here (3.6.1) might be outdated.</p>"},{"location":"git/#step-2-initialize-git-lfs-in-your-repository","title":"Step 2: Initialize Git LFS in Your Repository","text":"<p>After installation, you need to enable Git LFS in your project:</p> <pre><code>git lfs install\n</code></pre> <p>Now, your Git repository is ready to handle large files.</p>"},{"location":"git/#step-3-track-large-files","title":"Step 3: Track Large Files","text":"<p>Specify which file types or individual large files you want Git LFS to track.</p> <ul> <li>Track by Extension: To track all files with a certain extension (e.g., <code>.psd</code>), use:</li> </ul> <pre><code>git lfs track \"*.psd\"\n</code></pre> <p>This will automatically add a line in the <code>.gitattributes</code> file, ensuring all <code>.psd</code> files are managed by Git LFS:</p> <pre><code>*.psd filter=lfs diff=lfs merge=lfs -text\n</code></pre> <ul> <li>Track Individual Files: For large files without a specific extension, you can track them individually by specifying the exact file path:</li> </ul> <pre><code>git lfs track \"my_large_file\"\n</code></pre> <p>This will add an entry to <code>.gitattributes</code> like:</p> <pre><code>my_large_file filter=lfs diff=lfs merge=lfs -text\n</code></pre> <p>Note: Adding text manually in the <code>.gitattributes</code> is the same as running <code>git lfs track</code>. </p> <p>By tracking files this way, Git LFS ensures large files are stored separately while keeping references in your Git repository.</p> <p>Once a file is tracked by Git LFS and pushed to the remote repository, Git stores a reference (pointer) to the large file in your regular Git repository. The actual large file is stored separately on the Git LFS server (which could be GitHub or another remote LFS storage). </p> <p>In the future, when you push or pull, Git will just handle the reference (which is small in size) in the main repository, while Git LFS takes care of storing or fetching the actual large file.</p>"},{"location":"git/#step-4-add-commit-and-push-files-to-the-remote-repository","title":"Step 4: Add, Commit and Push Files to the Remote Repository","text":"<p>Now, add the large files you want to commit to your repository as you normally would:</p> <pre><code>git add .gitattributes\ngit add large_file.psd\ngit commit -m \"Add large file using Git LFS\"\ngit push\n</code></pre>"},{"location":"git/#step-5-pull-files-from-a-remote-repository","title":"Step 5: Pull Files from a Remote Repository","text":"<p>When you or someone else pulls the project, Git LFS will automatically download the large files referenced in the repository:</p> <pre><code>git pull\n</code></pre> <p>If you only pull the references to the large files, you can use the following command to explicitly download the large files:</p> <pre><code>git lfs pull\n</code></pre> <p>When using <code>git clone</code>, all files, including those tracked by LFS, are downloaded. If LFS-tracked files have been updated, running <code>git lfs pull</code> will download only the updated large files. <code>git lfs push</code> can be used to push only the large files to the LFS server without pushing other changes to the repository. However, in most cases, just running <code>git push</code> is sufficient, as Git LFS automatically handles the large files.</p>"},{"location":"git/#step-6-check-git-lfs-status","title":"Step 6: Check Git LFS Status","text":"<p>To check the current status of tracked files in Git LFS, use:</p> <pre><code>git lfs status\n</code></pre> <p>This command shows you which files are being tracked and which are pending to be pushed or committed.</p>"},{"location":"git/#links","title":"Links","text":"<ul> <li>More information on Git from coderefinery</li> </ul>"},{"location":"git/#courses","title":"Courses","text":"<p>From Software Carpentry, follow \"Version control with Git\" course.</p>"},{"location":"gpu/","title":"Access Snellius GPUs","text":""},{"location":"gpu/#access-snellius-gpus","title":"Access Snellius GPUs","text":""},{"location":"gpu/#small-compute-via-fnwi-faculty-and-nwo","title":"Small Compute via FNWI Faculty and NWO","text":"<p>The FNWI institute offers small compute resources about three times a year, with each allocation providing approximately 50K-100K SBUs. NWO also provides access to small compute resources.</p> <p>Create a ticket at https://servicedesk.surf.nl under \"Apply for access / Direct institute contract\" or \"Apply for access / Small Compute applications (NWO).\"</p> <p>Follow the instructions in the Setup section to create an account.</p>"},{"location":"gpu/#nwo-large-compute","title":"NWO Large Compute","text":"<p>For larger amounts of compute, please refer to possible options in NWO grants or Access to compute services. </p>"},{"location":"gpu/#direct-purchase-contract","title":"Direct Purchase Contract","text":"<p>For more information, visit Direct Purchase Contract. To find the latest rates for services, search for \"SURF Services and Rates\" on Google.</p>"},{"location":"gpu/#access-hipster-gpus-fnwi-research-cluster","title":"Access Hipster GPUs (FNWI research cluster)","text":"<p>Hipster is the FNWI (Faculty of Science) research cluster. To access Hipster GPUs, see the \"Who can use it?\" section on the HIPSTER page.</p>"},{"location":"gpu/#access-crunchomics-cpu-only","title":"Access Crunchomics (CPU Only)","text":"<p>Crunchomics is the Genomics Compute Environment for SILS and IBED. It provides only CPU resources\u2014no GPUs are available. To request an account, contact the current administrator as listed on the Crunchomics documentation page. The contact person may change, so always check the documentation for the latest information. As of now, you can contact Wim de Leeuw (<code>w.c.deleeuw@uva.nl</code>).</p> <p>Once your account is created, you will receive a username for access (e.g., <code>username@omics-h0.science.uva.nl</code>), which you can use to connect via SSH <code>ssh username@omics-h0.science.uva.nl</code>. </p> <p>For more information, please see this tutorial.</p>"},{"location":"gpu/#setup","title":"Setup","text":""},{"location":"gpu/#setup-ssh-keys","title":"Setup SSH keys","text":"<p>Create an SSH key or use the one you already have.</p> <p>To generate a new SSH key, open a terminal and run:</p> <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> <p>This command creates a modern, secure Ed25519 key. If your system or remote service does not support Ed25519, you can generate an RSA key instead:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> <p>When prompted, press Enter to accept the default file location, and optionally set a passphrase for added security. Once created, your public key will be stored in <code>~/.ssh/id_ed25519.pub</code>. You can display it with:</p> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre> <p>Copy the contents of this file and add it to your remote service (e.g., GitHub, GitLab, or a server) under SSH keys. You can then test the connection with:</p> <pre><code>ssh -T git@github.com\n</code></pre>"},{"location":"gpu/#snellius","title":"Snellius","text":"<p>Create an account</p> <p>https://portal.cua.surf.nl : first copied public key content in here (only done once)</p> <p>In the case of an issue, create in https://servicedesk.surf.nl a ticket under \"Servicedesk / create a ticket\" or email servicedesk@surf.nl. </p> <p>Usage</p> <p>Use the Snellius (similar for e.g. sshfs/scp):</p> <pre><code>ssh -X username@snellius.surf.nl\n</code></pre>"},{"location":"gpu/#hipster","title":"Hipster","text":"<p>Access the machine using:</p> <pre><code>ssh -X username@hipster.science.uva.nl\n</code></pre> <p>You will be prompted for your password. To enable passwordless access, add your public SSH key to the <code>~/.ssh/authorized_keys</code> file on the Hipster machine.</p>"},{"location":"gpu/#setup-environment","title":"Setup environment","text":"<p>The first time to setup your environment, run below script:</p> <pre><code>module purge # unload all modules\nmodule load 2025 # Check available modules first with `module avail` to ensure you can use this version.\nmodule load Miniconda3/25.5.1-1 # Check available modules first with `module avail`\nconda init # Sets up conda in your shell by adding initialization code to ~/.bashrc so conda activates automatically\n</code></pre> <p>After that, the basic virtualenv from conda can be created. See below e.g.:</p> <pre><code>conda create -n test python=3.10\nconda activate test\n</code></pre> <p>Or instead of <code>conda</code> use python virtual env which is faster and lighter. But then you only limited to the python version provided by the cluster. In conda, python versions can be selected.</p> <pre><code>module load Python/3.13.1-GCCcore-14.2.0\npython -m venv venv_name  # Or use a full/relative path, e.g. /home/youruser/ven_dir/venv_name\nsource /path/to/venv_name/bin/activate  # Or `. /path/to/venv_name/bin/activate` to activate the virtual environment. \n</code></pre> <p>On Hipster, available modules may differ and can change over time. Always check the HIPSTER page for the latest information.</p> <pre><code>module use /cvmfs/software.eessi.io/init/modules\nmodule load EESSI/2023.06 # After `module use`, if you do `module avail` this module `EESSI/2023.06` will be listed.\nmodule load Python/3.11.5-GCCcore-13.2.0 # Hipster seems not have conda. So use python instead. \nmodule load CUDA/12.4.0 # This module should be loaded, which is not needed in Snellius.\n</code></pre> <p>The rest of creating the virtual environment is the same as above: </p> <pre><code>python -m venv venv_name  # Or use a full/relative path, e.g. /home/youruser/ven_dir/venv_name\nsource /path/to/venv_name/bin/activate  # Or `. /path/to/venv_name/bin/activate` to activate the virtual environment. \n</code></pre> <p>Check GPU Usage</p> <p>Run <code>nvidia-smi</code> or install for example <code>pytorch</code>. </p> <pre><code>pip install torch numpy # torch version 2.8.0+cu128\n</code></pre> <p>When you install PyTorch using <code>pip install torch</code>, the GPU-enabled build is installed by default\u2014even on machines without a GPU. You can verify this by running:</p> <pre><code>python\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.tensor([1,2], device=\"cuda\")\n</code></pre> <p>In Snellius, first connect to a GPU node such as <code>gcn1</code> before running your jobs or activating your Python environment.</p> <pre><code>ssh gcn1 # Don't forget to activate your Python environment after connecting, as this is a new machine.\n</code></pre> <p>Note: If you close your terminal where you typed <code>ssh</code> or lose your SSH connection, the allocation will be terminated. To avoid this, start a <code>tmux</code> session before running <code>salloc</code> so your session persists even if the connection drops. First type <code>tmux</code>, then in <code>tmux</code> run the <code>salloc</code> command.</p> <p>You can allocate a GPU in interactive mode with the following command:</p> <p>Note: The <code>salloc</code> command differs slightly between Snellius and Hipster. The command for Hipster will not work on Snellius, and vice versa, due to differences in SLURM configuration and possibly SLURM versions.</p> <pre><code># Snellius\nsalloc --gpus=1 --partition=gpu_a100 --time=01:00:00\n\n# Hipster\nsalloc --gpus=l4:1 --time=00:05:00\nsalloc --gres=gpu:l4:1 --cpus-per-task=16 --time=00:5:00 # --cpus-per-task must be given\n</code></pre> <p>Obtain the partition name using <code>sinfo -o \"%G\"</code>. On Snellius, <code>accinfo</code> also displays the available partitions.</p> <p>Once a GPU is allocated, you can connect to it via SSH (e.g., <code>ssh gcn46</code>). Remember to activate your virtual environment after connecting, as each GPU node is a separate machine.</p>"},{"location":"gpu/#schedule-tasks","title":"Schedule Tasks","text":"<p>SLURM is a job scheduler used by many computer clusters and supercomputer, such as Snellius. It allocates resources to users and monitors work. It is a configurable workload manager. <code>squeue</code>, <code>sbatch</code>, <code>srun</code>, <code>sinfo</code>, and <code>scancel</code> are examples of the most commonly used commands in SLURM.</p>"},{"location":"gpu/#use-gpus","title":"Use GPUs","text":""},{"location":"gpu/#remote-vscode","title":"Remote VSCode","text":"<ul> <li>Install the <code>Remote-SSH</code> extension from the Extensions view.</li> <li>Press F1 (or Ctrl+p + &gt;) and type <code>Remote-SSH: Connect to Host</code> to connect to a remote host.  </li> <li>If you don\u2019t have a <code>~/.ssh/config</code> file set up, you\u2019ll need to run <code>Remote-SSH: Add New SSH Host</code> or set it up manually.</li> </ul> <p>If you want to access the GPU machine (e.g., <code>gcn700</code>), you can directly connect via Snellius using the command <code>ssh gcn700</code>. However, if you want to debug using Visual Studio Code (VSCode) on your computer, you need to add the following lines to your <code>~/.ssh/config</code> file. The rest of the process is the same as using remote SSH. Note that you can use <code>ssh me</code>, <code>ssh gcn700</code>, or remote SSH in VSCode.</p> <pre><code>Host me\n    User myuser\n    HostName snellius.surf.nl\n    IdentityFile ~/.ssh/id_rsa\nHost gcn700\n    User myuser\n    HostName gcn700\n    ProxyJump me\n    IdentityFile ~/.ssh/id_rsa\n</code></pre>"},{"location":"gpu/#quick-test","title":"Quick Test","text":"<p>The <code>int3</code> GPU machine is available for quick GPU tests without requiring a formal GPU allocation. It is based on MIG (Multi-Instance GPU) technology. You can view its configuration using <code>nvidia-smi</code>, <code>nvidia-smi -L</code>, or by inspecting <code>/etc/slurm/gres.conf</code>. However, this node has limited resources. For reference examples, check out Interactive Development GPU Node, Using IDEs and Remote Visualization.</p> <p>To connect to the <code>int3</code> GPU machine, run:</p> <pre><code>ssh int3 # also the same `ssh gcn1`\n</code></pre> <p>In the case of using <code>ssh gcn1</code>, if get the error, <code>Certificate invalid</code>, remove the old known-hosts entry manually from <code>.ssh/known_hosts</code> or by command:</p> <p>If you see the error <code>Certificate invalid</code>, remove the old host key entry from your known hosts by either manually editing <code>~/.ssh/known_hosts</code> and deleting the gcn1 line, or running:</p> <pre><code>ssh-keygen -R gcn1\n</code></pre>"},{"location":"gpu/#run-a-job","title":"Run a job:","text":"<p>There is a web-based interface to request resources for interactive session via ondemand. However, I recommend using a SLURM job instead; see below for details.</p> <p>NB. The run file should be executable. Make it executable with <code>chmod a+x runfile.sh</code>.</p> <pre><code>sbatch runfile.sh\n</code></pre> <p>e.g. runfile.sh:</p> <pre><code>#!/bin/bash\n#SBATCH --gpus=1\n#SBATCH --partition=gpu\n#SBATCH --time=14:00:00\n#SBATCH -o yolo8_train4_%j.out\n\necho \"gpus $SLURM_GPUS on node: $SLURM_GPUS_ON_NODE\"\necho \"nodes nnodes: $SLURM_NNODES, nodeid: $SLURM_NODEID, nodelist $SLURM_NODELIST\"\necho \"cpus on node: $SLURM_CPUS_ON_NODE per gpu $SLURM_CPUS_PER_GPU per task $SLURM_CPUS_PER_TASK omp num thread $OMP_NUM_THREADS\"\necho \"tasks per node $SLURM_TASKS_PER_NODE pid $SLURM_TASK_PID\"\n\n# activate your environment\nsource $HOME/.bashrc\nconda activate test # your conda venv\n\necho \"start training\"\nyolo detect train data=/home/username/data/data8_v1/data.yaml model=/home/username/exp/runs/detect/bgr23/weights/best.pt imgsz=1920 batch=8 epochs=100 name=bgr cache=true close_mosaic=0 augment=True rect=False mosaic=1.0 mixup=0.0\necho \"end training\"\n</code></pre> <p>More SBATCH options and the \"output environmental variables\" can be found from the sbatch help. </p> <p></p> <p>Check job is running</p> <p>User squeue with job id or username. </p> <pre><code>squeue -j jobid\nsqueue -u username\n# squeue with more options\nsqueue -o \"%.10i %.9P %.25j %.8u %.8T %.10M %.9l %.6D %.10Q %.20S %R\"\n</code></pre> <p>If the job is running, it will save the result in the output file with the name specified by <code>SBATCH -o</code> option. NB. <code>%j</code> in the name replaced by job id. In the example <code>yolo8_train4_%j.out</code>, the output file will be olo8_train4_2137977.out. The job id is the id you get after running sbatch.</p> <p>IMPORTANT Each person has a limited budget in the unit of SBU (system billing unit). It is also listed as accounting weight factor in Snellius partitions and accounting. e.g. If you request for 1 A100 GPU, it is 1/4 node, which has 18 cores for 10 hours, the SBU is <code>128 * 10 = 1280</code>. So in a <code>runfile.sh</code>, the basic slurm settings are as:</p> <pre><code>#SBATCH --gpus=1\n#SBATCH --partition=gpu\n#SBATCH --time=14:00:00\n</code></pre> <p>NB. <code>--cpus-per-gpu</code> or <code>--cpus-per-task</code> is automatically set for 1/4 of node, which in <code>gpu</code> partition is 18. For more info, check SBU calculating.</p> <p></p> <p>There are more options for variables such as below. You can get the full list from the sbatch help. </p> <pre><code>#SBATCH --gpus-per-node=1\n#SBATCH --cpus-per-gpu=18\n#SBATCH --cpus-per-task=1\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n</code></pre> <p>NB: Jobs that require more resources and or running for a long time (walltime: <code>SBATCH --time</code>) are not easily scheduled. Try to first test if everything is OK by running one or two epochs, then request for resources. Moreover, estimate the time your experiment runs by roughly calculating how long each epoch takes and multiply by epochs and then increase this time for a bit counting for caching, data transfer.</p> <p></p> <p>Check finished jobs</p> <pre><code>sacct -j jobid -o \"JobID,JobName,MaxRSS,Elapsed,NNodes,NodeList\"\n</code></pre> <p>More options are in the sacct help page.</p> <p></p> <p>Run tensorboard from remote computer Connect to Snellius and map your local port to remote port:</p> <pre><code>ssh -X username@snellius.surf.nl -L your_local_port:127.0.0.1:remote_port\n</code></pre> <p>In Snellius machine, run tensorboard with the remote port:</p> <pre><code>tensorboard --logdir_spec=18:/home/username/exp18,12:/home/username/exp12 --port remote_port # remote_port = 60011\n</code></pre> <p>Now, in the local machine, run <code>http://localhost:local_port</code>, e.g. <code>http://localhost:36999</code>. </p> <p></p> <p>Interactive mode</p> <p>IMPORTANT: This part is not recommended. Use it if you want to have a short check and want to use a bash.</p> <p>This is similar to using <code>sbatch</code> with a <code>runfile.sh</code>, but here the parameters are set directly in the <code>srun</code> or <code>salloc</code> command. This way, you get an interactive shell on the allocated machine. You can use either <code>srun</code> or <code>salloc</code> for this purpose.</p> <pre><code># srun\nsrun --gpus=1 --partition=gpu --time=00:10:00 --pty bash -il\n# salloc: allocates resources but does not connect via SSH; you need to SSH manually.\nsalloc --gpus=1 --partition=gpu_a100 --time=01:00:00\nssh machine_name # e.g. ssh gcn1003\n</code></pre> <p>Multigpu in single or multi node For every GPU there is 18 CPU no matter if specified in slurm or not. Slurm, batch scheduler, will ignore if another value for CPU is specified. Each task runs on one CPU. So <code>ntasks-per-node</code> or <code>ntasks</code> are the same here. Apparently, <code>with OMP_NUM_THREADS=4</code>, or other value, we can tell torchrun to use 4 threads per CPU.</p> <p>Basically, only specifiying number of gpus and the partition is enough. Below example uses 2 GPUs on a single Node, with 1 threads per CPU. </p> <pre><code>#!/bin/bash\n#SBATCH --gpus=2\n#SBATCH --partition=gpu\n#changing the  OMP_NUM_THREADS env variable is the same as --cpus-per-task\nexport OMP_NUM_THREADS=1\ntorchrun --node_rank=0 --nnodes=1 --nproc_per_node=2 ~/test/multigpu_torchrun.py 50 10\n</code></pre> <p>For more information on ddp (distributed data parallel) in pytorch, look at the tutorial.</p> <p>Wandb in Snellius</p> <p>First run <code>wandb init</code> before sending the job via sbatch. Then run the code which has <code>wandb.init(project=project_name)</code>. <code>Project_name</code> is wandb project.</p> <p>Useful commands</p> <ul> <li><code>htop</code>, <code>nvtop</code>: monitor CPU and GPU respectively</li> <li><code>sbatch</code>: run a job</li> <li><code>srun</code>: run a job. e.g. run job in the interactive mode: <code>srun --gpus=1 --partition=gpu --time=00:10:00 --pty bash -il</code> </li> <li><code>salloc</code>: allocate resources interactively and then run a command. e.g <code>salloc --gpus=1 --partition=gpu --time=00:10:00</code></li> <li><code>squeue</code>: show the status of the job</li> <li><code>scancel</code>: cancel the job. </li> <li><code>scontrol</code>: show detailed job information. e.g. show job detail: <code>scontrol show j 7605565</code></li> <li><code>sinfo</code>: get information about GPUs. e.g. <code>\"%9P %70N %5t %32G %6D\"</code>, <code>sinfo -e -o  \"%9P %.6D %10X %4Y %24N %24f %32G\"</code>, <code>sinfo -p gpu</code></li> <li><code>sacct</code>: get statistics on completed jobs</li> <li><code>accinfo</code>, <code>budget-overview -p gpu</code>, <code>accuse</code>: show how much credite is left (Snellius commands)</li> <li><code>myquota</code>: show the limit of files. They are also listed in Snellius hardware and file systems.</li> <li><code>gpustat -acp</code>: show the gpu usage. It should be installed with pip, <code>pip install gpustat</code>. It has the information from <code>nvidia-smi</code>, but one-liner. </li> <li><code>module load/unload/purge/list/display/avail</code>: <ul> <li><code>load/unload/purge</code>: e.g. <code>module load CUDA/11.8.0</code>: load this module and use <code>unload</code> to unload this module. <code>purge</code> unload all modules.</li> <li><code>list</code>: e.g. <code>module list</code>: list of loaded modules. </li> <li><code>display</code>: e.g. <code>module display CUDA/11.8.0</code>: show information on where this module is. </li> <li><code>avail</code>: e.g. <code>module avail</code>: show list of all available modules, but first load 2022/2023/or higher version if available. </li> </ul> </li> </ul> <p>Some examples are given in Convenient Slurm commands. </p> <p></p>"},{"location":"gpu/#useful-links","title":"Useful links:","text":"<ul> <li>SURF service desk portal</li> <li>SURF wiki</li> <li>Snellius hardware </li> <li>file systems</li> <li>SBU calculating</li> <li>Example job scripts</li> <li>Convenient Slurm commands</li> <li>Squeue help: just use <code>squeue --help</code></li> <li>uvadlc: Working with the Snellius cluster</li> <li>microhh</li> </ul>"},{"location":"gpu/#details","title":"Details","text":""},{"location":"gpu/#slrum-interactive-mode","title":"SLRUM interactive mode","text":"<p>In the interactive mode, you can see slurm variables. E.g. <code>echo $$SLURM_MEM_PER_CPU</code>, <code>echo $SLURM_CPUS_PER_TASK</code>.</p> <pre><code># interactive mode: request 2 CPUs (-c,--cpus-per-task), time can be unlimited (time=UNLIMITED)\nsrun -c2 --mem-per-cpu=200G --pty bash -il\n# interactive mode: request 1 GPU\nsrun --gpus=1 --partition=gpu --time=00:10:00 --pty bash -il\n</code></pre>"},{"location":"gpu/#slurm-list-of-common-commands","title":"SLURM list of common commands","text":"<pre><code>#SBATCH --job-name=result # appears in squeue\n#SBATCH -o exps/test_%j.out # -o,--output, %j=job id\n#SBATCH --error=test_%j.err\n#SBATCH --time=00:10:00 # time can be UNLIMITED\n#SBATCH --mem-per-cpu=1G\n#SBATCH --cpus-per-task=2 # -c,--cpus-per-task\n#SBATCH --gpus=1 # number of gpu\n#SBATCH --partition=gpu # type of gpu\n</code></pre> <p>Only these values are shown on snellius:</p> <pre><code>$SLURM_CPUS_ON_NODE:  the number of CPUs allocated to your job using the following environment variable:\n$SLURM_GPUS: This gives you the total number of GPUs allocated to your job.\n</code></pre> <p>These values are empty on snellius:</p> <pre><code>$SLURM_MEM_PER_NODE: This variable gives you the total amount of memory allocated per node in megabytes (MB)\n$SLURM_MEM_PER_CPU * $SLURM_CPUS_ON_NODE: in the case of requesting memory using --mem-per-cpu\n\n$SLURM_GPUS_PER_NODE: This gives the number of GPUs per node allocated to your job.\n$SLURM_JOB_GPUS: This gives you a list of the GPUs allocated to your job.\n\necho \"cpu pertask: $SLURM_CPUS_PER_TASK\"\necho \"cpu per gpu: $SLURM_CPUS_PER_GPU\"\n</code></pre>"},{"location":"gpu/#slurm-task-array","title":"SLURM task array","text":"<p>Example using task array:</p> <pre><code>#SBATCH --output=test_%A_%a.out # -o,--output, %A=job id, %a=array number\n#SBATCH --array=0-1\n\npython your_script.py $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"gpu/#get-cpu-and-gpu-information","title":"Get CPU and GPU information","text":"<p>When the job is running, you can get number of GPU, CPU, RAM.</p> <pre><code># job_id = 7605565\n`scontrol show job 7605565`\n</code></pre>"},{"location":"gpu/#cpu","title":"CPU","text":"<pre><code># RAM\n# ---\n# python\nf\"{psutil.virtual_memory().total:,}\" # total of the node\n# command line\nfree -h # total of the node\nhtop -u -d 0 # total of the node\n\n# disk space\n# ----------\nf\"{psutil.disk_usage('/').total:,}\" # total of the node\n# command line\ndf -h --total # total of the node\n\n# Number of CPUs\n# --------------\n# python\nlen(os.sched_getaffinity(0))\n# N.B: total cpu (72 for A100 not divided by 4)\npsutil.cpu_count(logical=True) # total of the node\nos.cpu_count() # total of the node\n# command line\nnproc\nhtop -u -d 0 # total of the node\n</code></pre>"},{"location":"gpu/#gpu","title":"GPU","text":"<pre><code># GPU VRAM\n# --------\n# python\nf\"{torch.cuda.get_device_properties(0).total_memory:,}\"\n# command line\nnvidia-smi --query-gpu=memory.total --format=csv\n\n# GPU type\n# --------\ntorch.cuda.get_device_name(0) \n# command line\nnvidia-smi -L\n\n# Number of GPUs\n# --------------\n# python\ntorch.cuda.device_count()\n# command line\nnvidia-smi -L\n</code></pre>"},{"location":"gpu/#sinfo","title":"Sinfo","text":"<p>Status of the nodes:</p> <pre><code>sinfo -o \"%9P %70N %5t %32G %6D\"\n</code></pre> <pre><code>    %P is the partition name.\n    %N is the node name.\n    %t is the state of the node (e.g., idle, allocated, down).\n    %G is the generic resource information, including GPUs.\n    %D is the number of nodes.\n</code></pre> <p>You can get detailed node information via: <code>scontrol show node &lt;node-name&gt;</code></p> <pre><code>scontrol show nodes | awk '/NodeName/ {node=$1} /State=/ {state=$1} /Gres=gpu/ {print node, state}'\n</code></pre> <p>Get other information:</p> <pre><code>&gt; sinfo -e -o \"%9P %.6D %10X %4Y %24N %24f %32G\" | grep gpu_a\ngpu_a100      36 4          18   gcn[37-72]               hwperf,scratch-node      gpu:a100:4(S:0-1),cpu:72\n</code></pre> <p>The <code>-e</code> option in sinfo stands for \"extend\" and is used to show information about all nodes, even those that are in states like down, drain, fail, or unknown. Without -e, sinfo typically only shows nodes that are available or idle.</p> <p>Breaking down each field:</p> <pre><code>    %9P: Partition name, gpu_a100.\n    %6D: Number of nodes, 36.\n    %10X: Number of GPUs (sockets) per node, 4.\n    %4Y:  Number of cores per GPU (socket), 18.\n    %24N: List of nodes in this partition (in this case, gcn[37-72]).\n    %24f: Features of the nodes (e.g., hwperf,scratch-node).\n    %32G: Shows detailed information about GPU and CPU availability (e.g., gpu:a100:4(S:0-1),cpu:72).\n</code></pre> <pre><code>&gt; sinfo | grep gpu_a\n\ngpu_a100     up 5-00:00:00      5   resv gcn[47-49,69,71]\n</code></pre> <p>Explanation:</p> <pre><code>    gpu_a100 is the partition name.\n    up 5-00:00:00 means the partition is up and available for 5 days.\n    5 represents the number of nodes available.\n    resv indicates that the nodes are reserved.\n    gcn[47-49,69,71] specifies the nodes within the gpu_a100 partition.\n</code></pre>"},{"location":"gpu/#free-gpus","title":"Free GPUs","text":"<p>Here is a list of free GPUs:  </p> <ul> <li>Google Colab</li> <li>Kaggle</li> <li>Gradient by Paperspace</li> <li>Amazon SageMaker Studio Lab</li> <li>Microsoft Azure (for student accounts)</li> </ul>"},{"location":"gpu/#external-gpus","title":"External GPUs","text":"<ul> <li>University: LUMI, Snellius</li> <li>Cloud GPU comparison</li> <li>GPU Cloud Providers Pricing info</li> <li>Vast.ai</li> <li>Lambda Labs: On demand, One, two &amp; three year contracts.</li> <li>RunPod.io</li> <li>Together.ai</li> <li>Fireworks AI</li> <li>deepinfra</li> <li>Lepton AI</li> <li>VULTR</li> <li>Novita</li> <li>lightning.ai</li> <li>Hyperbolic.xyz</li> <li>Latitude AI</li> <li>Paperspace</li> <li>Jarvislabs</li> <li>HPC-AI</li> <li>GCP</li> <li>AWS</li> </ul>"},{"location":"gradio_hf/","title":"Hugging Face Guide","text":""},{"location":"gradio_hf/#new-model-setup","title":"New Model Setup","text":""},{"location":"gradio_hf/#uploading-a-model","title":"Uploading a Model","text":""},{"location":"gradio_hf/#create-a-model-repository","title":"Create a Model Repository","text":"<p>Go to Hugging Face and create a <code>New Model</code> under your profile, which essentially creates a repository for your models. Refer to Hugging Face Repositories Getting Started for detailed instructions. </p> <p>Once created, you can clone the repository and proceed with the next steps on your local machine.</p>"},{"location":"gradio_hf/#set-up-git-lfs-large-file-storage","title":"Set Up Git LFS (Large File Storage)","text":"<p>Hugging Face uses Git LFS to manage large files, such as model weights. Install and set up Git LFS as follows:</p> <pre><code>sudo apt install git-lfs\ngit lfs install\n</code></pre>"},{"location":"gradio_hf/#configure-hugging-face-interface","title":"Configure Hugging Face Interface","text":"<p>Install the Hugging Face Hub library and set up authentication:</p> <pre><code>pip install huggingface_hub\n\n# Configure Git to store credentials\ngit config --global credential.helper store\n\n# Log in to Hugging Face CLI\nhuggingface-cli login\n\n# Your token will be saved in the following location:\ncat $HOME/.cache/huggingface/token\n\n# Enable LFS support for large files\nhuggingface-cli lfs-enable-largefiles\n</code></pre>"},{"location":"gradio_hf/#push-a-large-file","title":"Push a Large File","text":"<p>Once Git LFS is configured, you can push large files just like a normal Git commit:</p> <pre><code>git add your_model\ngit commit -m \"Add a large file using Git LFS\"\ngit push\n</code></pre> <p>For more details on uploading models, refer to the Hugging Face Model Uploading Guide.</p>"},{"location":"gradio_hf/#downloading-a-model","title":"Downloading a Model","text":"<p>To download a model from Hugging Face, you can use the following Python code:</p> <pre><code>from huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(repo_id=\"fkariminejadasl/bird\", filename=\"45_best.pth\")\n</code></pre> <p>For more downloading options, refer to the Hugging Face Models Downloading Guide.</p>"},{"location":"gradio_hf/#creating-a-new-space-on-hugging-face","title":"Creating a New Space on Hugging Face","text":"<p>To start, navigate to Hugging Face and create a <code>New Space</code> under your profile. This will generate a repository for your application. For detailed guidance, refer to the Gradio Spaces documentation.</p> <p>Once your space is created, you can clone the repository to your local machine, add your <code>app.py</code> and <code>requirements.txt</code> files, and push the changes back to the repository. The application will launch automatically.</p>"},{"location":"gradio_hf/#application-code-and-dependencies","title":"Application Code and Dependencies","text":"<ul> <li> <p><code>app.py</code>: Your Gradio application code must be saved in a file named <code>app.py</code>.</p> </li> <li> <p><code>requirements.txt</code>: This file should list all the Python dependencies your app requires, including any custom libraries. An example <code>requirements.txt</code> might look like this:</p> </li> </ul> <pre><code>torch\ngradio\nhuggingface_hub\ngit+https://github.com/username/repository_name.git@branch_name\n</code></pre> <ul> <li>Note: The <code>branch_name</code> can also be replaced by <code>tag_name</code> or <code>commit_hash</code> depending on your need.</li> </ul> <p>If the <code>pyproject.toml</code> file for a custom library is located in a subdirectory within the repository, you can specify the subdirectory like this:</p> <pre><code>git+https://github.com/username/repository_name.git@branch_name#subdirectory=subdirectory_name\n</code></pre> <p>For further details on how to use <code>pip install</code> with various options, refer to the Pip Install Guide.</p>"},{"location":"gradio_src/","title":"Gradio in SRC (SURF Research Cloud)","text":""},{"location":"gradio_src/#create-a-workspace","title":"Create a Workspace","text":"<p>In the SRC Dashboard, create an <code>Ubuntu 2204 - SUDO enabled</code> machine. </p>"},{"location":"gradio_src/#install-miniconda","title":"Install Miniconda","text":"<p>Follow the instructions here or below:</p> <pre><code>mkdir /scratch/venv\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /scratch/venv/miniconda.sh\nbash /scratch/venv/miniconda.sh -b -u -p /scratch/venv/\nrm /scratch/venv/miniconda.sh\n/scratch/venv/bin/conda init bash\n</code></pre> <p>Open a new terminal.</p>"},{"location":"gradio_src/#setup-virtual-environment","title":"Setup Virtual Environment","text":"<pre><code>conda create -n p310 python=3.10\nconda activate p310\n</code></pre> <p>Add the <code>conda activate p310</code> command to <code>~/.bashrc</code>.</p>"},{"location":"gradio_src/#install-bird-classification-app","title":"Install Bird Classification App","text":"<p>In <code>/scratch</code>:</p> <pre><code>git clone https://github.com/fkariminejadasl/bird-behavior.git\npip install .[app]\n</code></pre>"},{"location":"gradio_src/#setup-nginx","title":"Setup Nginx","text":"<pre><code>sudo apt install nginx\n</code></pre> <p>Create a <code>testgradio</code> file with the content below by using <code>sudo vi /etc/nginx/sites-available/testgradio</code>. For more detailed information, refer to Running Gradio on Your Web Server with Nginx.</p> <p>Find the domain name in <code>/etc/hosts</code>.  </p> <pre><code>server {\n    listen 80;\n    server_name your_domain_name;  # This is in /etc/hosts\n\n    location / {  # Change this if you'd like to serve your Gradio app on a different path\n        proxy_pass http://127.0.0.1:7860/; # Change this if your Gradio app will be running on a different port\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Since <code>include /etc/nginx/sites-enabled/*;</code> is added in the <code>/etc/nginx/nginx.conf</code> file, we need to create a symbolic link:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/testgradio /etc/nginx/sites-enabled\n</code></pre>"},{"location":"gradio_src/#setup-gradio-app-as-a-service","title":"Setup Gradio App as a Service","text":"<p>Create the service file using <code>sudo vi /etc/systemd/system/gradio.service</code>:</p> <pre><code>[Unit]\nDescription=Service to launch Gradio app\nAfter=network.target\n\n[Service]\nUser=fkariminej\nGroup=www-data\nEnvironment=\"CONDA_BASE=/scratch/venv\"\nEnvironment=\"CONDA_ENV_NAME=p310\"\nWorkingDirectory=/scratch/bird-behavior/app\nExecStart=/bin/bash -c \"source $CONDA_BASE/bin/activate $CONDA_ENV_NAME &amp;&amp; python testgradio.py\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"gradio_src/#enable-services","title":"Enable Services","text":"<p>Handy commands in <code>systemctl</code>: status, start, enable, disable, stop.</p> <pre><code>sudo systemctl status nginx\nsudo systemctl status gradio\n</code></pre>"},{"location":"gradio_src/#acknowledgement","title":"Acknowledgement","text":"<p>This document is modified from the original document by Berend Wijers and loosely follows the Serve Python App on Nginx.</p>"},{"location":"improve_training_results/","title":"Improving Deep Learning Training Results","text":"<p>Deep learning training outcomes can significantly improve by focusing on three main components: data, model, and training. This framework can be expanded to \"data, model, loss, optimizer,\" as detailed in Andrej Karpathy's insightful blog post. He suggests dividing the model into model and loss, with training encapsulated by the optimizer. Here are strategies to enhance each component:</p>"},{"location":"improve_training_results/#data","title":"Data","text":"<ul> <li>Data Augmentation: Apply data augmentation techniques to combat overfitting.</li> <li>Expand Dataset: Increasing the dataset size can lead to more robust training outcomes.</li> </ul>"},{"location":"improve_training_results/#model","title":"Model","text":"<ul> <li>Dropout: Integrate dropout layers to prevent overfitting.</li> <li>Activation Functions: Utilize ReLU/GLUE for non-linear transformations.</li> <li>Loss Functions: Proper selection of loss functions is crucial.</li> </ul>"},{"location":"improve_training_results/#training-optimizerscheduler","title":"Training (Optimizer/Scheduler)","text":"<ul> <li>Epochs: Increasing the number of iterations or epochs, especially when using dropout, is often necessary for optimal performance.</li> <li>Learning Rate: Adjusting the learning rate is a primary method for enhancing performance.</li> <li>Optimizer Choice: Switching optimizers, e.g., from Adam to AdamW, can improve results due to AdamW's weight decay feature, which acts as a regularizer.</li> <li>Scheduling: Implementing schedulers, with or without options for warm-up phases, can fine-tune the learning rate adjustment process over time. Example schedulers include <code>StepLR</code>, <code>CosineAnnealingLR</code>, and <code>ExponentialLR</code>, with warm-up variations like <code>CosLR</code> and <code>LinearLR</code>. </li> </ul> <p>Note: Weight decay, Dropout and Label Smoothing are the basic regularizers. </p> <p>Note: The strategies outlined above are often detailed in academic papers.</p>"},{"location":"jupyter_src/","title":"Shared Jupyter Notebook in SRC (SURF Research Cloud)","text":"<p>In order to make a notebook available to all users, <code>conda</code>, a <code>virtual environment</code>, and a <code>Jupyter kernel</code> should be installed in a shared location, such as <code>/opt</code>, where all users have access.</p>"},{"location":"jupyter_src/#create-a-workspace","title":"Create a Workspace","text":"<p>Create a \"Jupyter Notebook\" workspace from \"Create new workspace\" and attach an external storage. The storage is mounted under \"/data/storage_name\". You can transfer data for example by scp:</p> <pre><code>scp -r your_data username@ip_address:/data/storage_name\n</code></pre>"},{"location":"jupyter_src/#setup-conda-for-all-users","title":"Setup Conda for All Users","text":"<pre><code># Create a Directory for Conda in a System-Wide Location:\nsudo mkdir -p /opt/miniconda3\n\n# Download the Miniconda Installer:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n\n# Install Miniconda to the System-Wide Location:\nsudo bash /tmp/miniconda.sh -b -u -p /opt/miniconda3\n\n# Remove the Installer Script:\nrm /tmp/miniconda.sh\n\n# Set Permissions for All Users:\nsudo chmod -R 755 /opt/miniconda3\nsudo chown -R root:root /opt/miniconda3\n\n# Add Conda to the PATH for All Users:\necho 'export PATH=\"/opt/miniconda3/bin:$PATH\"' | sudo tee -a /etc/profile.d/conda.sh\n# /opt/miniconda3/etc/profile.d/conda.sh\n\n# Initialize Conda for All Users' Shells:\n/opt/miniconda3/bin/conda init --all\n\n# Verify the Installation:\nconda --version\n</code></pre>"},{"location":"jupyter_src/#make-kernel-for-all-users","title":"Make Kernel for All Users","text":"<pre><code># Activate Conda\nsource /opt/miniconda3/bin/activate\n\n# Run Conda as Root Using Full Path\nsudo /opt/miniconda3/bin/conda create -y -p /opt/miniconda3/envs/p310 python=3.10\n\n# Set Permissions\nsudo chmod -R 755 /opt/miniconda3/envs/p310\nsudo chown -R root:root /opt/miniconda3/envs/p310\n\n# Install ipykernel in the New Environment\nconda activate /opt/miniconda3/envs/p310\nsudo /opt/miniconda3/envs/p310/bin/pip install ipykernel\n\n# Register a System-Wide Jupyter Kernel: Create a new kernel specification that all users can access\n# This places the kernel spec in a system directory (e.g., /usr/local/share/jupyter/kernels/p310).\nsudo /opt/miniconda3/envs/p310/bin/python -m ipykernel install --name p310 --display-name \"bird\"\n\n# Permissions for Kernel Directory\nsudo chmod -R 755 /usr/local/share/jupyter/kernels/p310\n\n# Confirm the Installation: You should see p310 listed.\njupyter kernelspec list\n</code></pre>"},{"location":"jupyter_src/#install-your-package-for-all-users","title":"Install Your Package for All Users","text":"<pre><code># Navigate to your desired directory\ncd /scratch\n\n# Clone the GitHub repo\ngit clone https://github.com/fkariminejadasl/bird-behavior.git\ncd bird-behavior\n\n# Checkout the specific branch\ngit checkout cleanup1\n\n# Install the package into the p310 environment\nsudo /opt/miniconda3/envs/p310/bin/pip install -e .\n</code></pre>"},{"location":"linux/","title":"Linux","text":""},{"location":"linux/#shared-directories","title":"Shared Directories","text":"<p>Linux distributions typically follow the Filesystem Hierarchy Standard (FHS), which defines where different types of files are placed. Some common system-wide (shared) directories are:</p> <ul> <li>/etc: System-wide configuration files.  </li> <li>/usr: Contains the majority of userland programs and data.  </li> <li>/usr/bin: Executables for most user programs.  </li> <li>/usr/sbin: Executables for system administration.  </li> <li>/usr/lib: Shared libraries and internal binaries used by programs in <code>/usr/bin</code> and <code>/usr/sbin</code>.  </li> <li>/usr/local: For programs installed locally (outside of the distribution\u2019s package manager), typically in subdirectories like <code>/usr/local/bin</code> and <code>/usr/local/lib</code>.  </li> <li>/usr/share: Architecture-independent data such as icons, documentation, and locale files.</li> <li>/var: Variable data that changes during system operation.  </li> <li>/var/lib: State information and variable data for system programs.</li> <li>/var/log: Log files.</li> <li>/opt: Optional or third-party software, often self-contained packages.</li> <li>/lib, /lib64: Core system libraries (for <code>/bin</code> and <code>/sbin</code>).</li> <li>$HOME/.config/autostart: Applications that run at startup for a specific user. Similar to <code>systemctl</code>, but <code>autostart</code> is for user-specific applications, whereas systemctl manages system-wide services.</li> </ul> <p>When you install packages using a package manager like <code>apt</code>, the files typically go into these shared directories depending on their type:</p> <ul> <li>Executables usually end up in <code>/usr/bin</code> or <code>/usr/sbin</code>.</li> <li>Libraries go into <code>/usr/lib</code>.</li> <li>Configuration files go into <code>/etc</code>.</li> <li>Documentation and data files often end up in <code>/usr/share</code>.</li> </ul> <p>These directories are accessible system-wide, so all users on the system can use the installed programs without additional per-user setup.</p>"},{"location":"linux/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"linux/#file-and-directory-management","title":"File and Directory Management","text":"Command Description <code>ls</code> List files and directories in the current directory. <code>ls -l</code> List files with detailed information. <code>ls -a</code> List all files, including hidden ones. <code>cd [directory]</code> Change the current directory to the specified one. <code>pwd</code> Display the current working directory path. <code>mkdir [directory]</code> Create a new directory. <code>rmdir [directory]</code> Remove an empty directory. <code>rm [file]</code> Delete a file. <code>rm -r [directory]</code> Recursively delete a directory and its contents. <code>cp [source] [destination]</code> Copy files or directories. <code>mv [source] [destination]</code> Move or rename files or directories. <code>touch [file]</code> Create an empty file or update the timestamp of an existing file. <code>cat [file]</code> Display the contents of a file. <code>less [file]</code> View the contents of a file one screen at a time. <code>head [file]</code> Display the first 10 lines of a file. <code>tail [file]</code> Display the last 10 lines of a file. <code>diff [file1] [file2]</code> Compare two files line by line and display the differences. <code>readlink [link]</code> Display the target of a symbolic link. <code>readlink -f [path]</code> Display the absolute path, resolving all symbolic links. <code>ln [target] [link_name]</code> Create a hard link to a file. <code>ln -s [target] [link_name]</code> Create a symbolic (soft) link to a file or directory. <code>lsb_release -a</code> Show Linux version info. If not installed, run <code>cat /etc/os-release</code>."},{"location":"linux/#file-permissions-and-ownership","title":"File Permissions and Ownership","text":"Command Description <code>chmod [permissions] [file]</code> Change the permissions of a file or directory. <code>chown [owner]:[group] [file]</code> Change the owner and group of a file or directory. <code>chgrp [group] [file]</code> Change the group of a file or directory."},{"location":"linux/#process-management","title":"Process Management","text":"Command Description <code>ps</code> Display information about active processes. <code>pgrep: ps + grep</code> Search for processes by name or other attributes and display their PIDs. <code>top</code> Display real-time system information, including active processes. <code>htop</code> Interactive process viewer (requires installation). <code>nvtop</code> Interactive monitor for NVIDIA GPUs (requires NVIDIA GPUs). <code>kill [PID]</code> Terminate a process by its Process ID (PID). <code>killall [process_name]</code> Terminate all processes with the specified name. <code>bg</code> Resume a suspended job in the background. <code>fg</code> Bring a background job to the foreground."},{"location":"linux/#disk-usage-and-storage","title":"Disk Usage and Storage","text":"Command Description <code>df -h</code> Display disk space usage in human-readable format. <code>du -sh [directory]</code> Display the size of a directory and its contents."},{"location":"linux/#networking","title":"Networking","text":"Command Description <code>ifconfig</code> Display or configure network interfaces. <code>ip a</code> Display all network interfaces and their IP addresses. <code>ping [host]</code> Send ICMP ECHO_REQUEST packets to network hosts. <code>wget [url]</code> Download files from the internet. <code>curl [url]</code> Transfer data from or to a server. <code>ssh [user]@[host]</code> Connect to a remote host via SSH."},{"location":"linux/#file-transfer","title":"File Transfer","text":"Command Description <code>scp [source] [user@host:destination]</code> Securely copy files between hosts over a network. <code>rsync [source] [user@host:destination]</code> Synchronize files and directories between two locations efficiently. <code>mount [device] [mount_point]</code> Mount a device to the filesystem. <code>umount [device]</code> Unmount a device from the filesystem. <code>sshfs user@remote:/remote/path /local/mountpoint</code> Mount remote files and open them in your file browser as if they were local. <code>fusermount -uz /path/to/mountpoint</code>  or <code>umount -l /path/to/mountpoint</code> Unmount remote files, which was mounted by sshfs. <ul> <li><code>fusermount -z</code> or <code>umount -l</code> is for lazy unmounting. This allows you to move on without waiting for the system to resolve the issue.</li> </ul>"},{"location":"linux/#user-management","title":"User Management","text":"Command Description <code>adduser [username]</code> Create a new user. <code>passwd [username]</code> Change the password for a user. <code>deluser [username]</code> Delete a user. <code>usermod -aG [group] [username]</code> Add a user to a group."},{"location":"linux/#system-information","title":"System Information","text":"Command Description <code>uname -a</code> Display all system information. <code>uname -r</code> Display the kernel version. <code>uptime</code> Show how long the system has been running. <code>date</code> Display or set the system date and time. <code>who</code> Show who is logged into the system. <code>whoami</code> Display the current logged-in user's username."},{"location":"linux/#package-management-debian-based-systems","title":"Package Management (Debian-based systems)","text":"Command Description <code>apt update</code> Update the package index. <code>apt upgrade</code> Upgrade all installed packages to their latest versions. <code>apt install [package]</code> Install a new package. <code>apt remove [package]</code> Remove an installed package. <code>apt purge [package]</code> Remove an installed package completely. <code>apt search [package]</code> Search for a package in the repositories. <code>snap install [package]</code> Install a new package. <code>snap remove [package]</code> <code>rm -rf ~/snap/[package]</code> Remove an installed package completely, including its leftover data. <p>Note: <code>apt</code> and <code>snap</code> should be run with <code>sudo</code>, e.g., <code>sudo apt purge cloudcompare</code>.</p>"},{"location":"linux/#text-processing","title":"Text Processing","text":"Command Description <code>grep [pattern] [file]</code> Search for a pattern in a file. <code>rg [pattern] [path]</code> Search for a pattern in files under a path (or in a file). Install with <code>sudo apt install ripgrep</code> <code>sed 's/[old]/[new]/' [file]</code> Replace text in a file using stream editor. <code>awk '{print $1}' [file]</code> Pattern scanning and processing language."},{"location":"linux/#compression-and-archiving","title":"Compression and Archiving","text":"Command Description <code>tar -cvf [archive.tar] [files]</code> Create a tarball archive of files. <code>tar -xvf [archive.tar]</code> Extract files from a tarball archive. <code>gzip [file]</code> Compress a file using gzip. <code>gunzip [file.gz]</code> Decompress a gzip compressed file. <p>This cheat sheet provides a quick reference to common Linux commands. For more detailed information, refer to the manual pages by typing <code>command --help</code> or <code>man command</code> in the terminal. </p>"},{"location":"linux/#examples","title":"Examples","text":"<pre><code># Copy a local file(s) to a remote host:\n# -r: copy directories recursively.\nscp -r /path/to/local/file.txt user@remote_host:/path/to/remote/directory/\n\n# Copy a file from a remote host to the local machine:\nscp user@remote_host:/path/to/remote/file.txt /path/to/local/directory/\n\n# Synchronize a local directory to a remote host:\n# `-a`: Archive mode (preserves permissions, times, symbolic links, etc.).\n# `-v`: Verbose output.\n# `-z`: Compress data during transfer. Slow option.\n# `-P`: shows progress and allows resume with the same command if interrupted\nrsync -avP /path/to/local/directory/ user@remote_host:/path/to/remote/directory/\n\n# Synchronize a remote directory to the local machine:\nrsync -avP user@remote_host:/path/to/remote/directory/ /path/to/local/directory/\n\n# Mounts a USB drive at /mnt/usb so you can access its files.\nmount /dev/sdb1 /mnt/usb\n# To unmount the filesystem mounted with:\numount /dev/sdb1\n# or\numount /mnt/usb\n# If the device is busy and won't unmount, try:\numount -l /mnt/usb  # Lazy unmount\n\n# Find the process ID(s) of a running program:\n# pgrep -ifa porcesss_name: -i: ingore case, -f only process id, -a: full command, similar to ps aux | grep process name\npgrep process_name\n\n# Find processes by user:\npgrep -u username\n\n# Find patterns with line numbers\nrg -n \"detach|__main__\" # n: line number (default), e: regular expression (works without n, e in this example)\ngrep -RInE 'detach|__main__' . # n: line number, E: regular expression, R: recursive, I: ingore binaries\n\n# Find differences\ndiff --color -U 0 file1 file2\n\n# Display the absolute path\nreadlink -f $HOME\n\n# To create a symbolic link named `my_link` that points to a file `myfile.txt`\nln -s myfile.txt my_link\n\n# To unzip a file to a specific location\nunzip filename.zip -d /path/to/destination\n\n# Save both standard output and error to a file. The output is also displayed on the screen.\n# tee: Copy standard input to each FILE, and also to standard output.\n# 2&gt;&amp;1: redirects stderr (2) to stdout (1).\nyour_command &gt; output.txt 2&gt;&amp;1\nyour_command 2&gt;&amp;1 | tee output.txt\n\n# Convert png to pdf by ImageMagick\nsudo apt install imagemagick\n# ImageMagick has a security policy that restricts writing PDFs. You can fix this by modifying the ImageMagick policy file and then convert your file.\n# Open the policy file for editing:\nsudo nano /etc/ImageMagick-6/policy.xml\n# Find and modify the PDF policy:\n# change `&lt;policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" /&gt;` to `&lt;policy domain=\"coder\" rights=\"read | write\" pattern=\"PDF\" /&gt;` \n# Convert the png to pdf \nconvert your_image.png -density 300 your_image.pdf\n</code></pre>"},{"location":"linux/#courses","title":"Courses","text":"<p>From Software Carpentry, follow \"The Unix Shell\" course.</p>"},{"location":"object_detection/","title":"Object Detection: A Quick Overview","text":""},{"location":"object_detection/#introduction","title":"Introduction","text":"<p>Object detection is a critical task in computer vision, involving the classification and localization of objects within an image or video. This manual provides a quick overview of various methods to enhance the efficiency and accuracy of object detection. We'll explore different categories of object detection, including Faster R-CNN, YOLO (You Only Look Once), CenterNet, and DETR (DEtection Transformer). Finally, we'll delve into open-set object detection and object detection with limited data.</p>"},{"location":"object_detection/#categories-of-object-detection-methods","title":"Categories of Object Detection Methods","text":"<p>Object detection methods can be categorized based on key attributes influencing their design and performance. Here are prominent categories:</p> <ul> <li>Two-Stage vs. One-Stage Methods:</li> <li>Two-stage methods, like Faster R-CNN, involve region proposal and subsequent classification, offering high accuracy but may be slower.</li> <li> <p>One-stage methods, such as YOLO, perform object detection in a single step, providing faster inference for real-time applications.</p> </li> <li> <p>Anchor-Based vs. Anchor-Free Methods:</p> </li> <li> <p>Anchor-based methods, like Faster R-CNN, use predefined anchor boxes, while anchor-free methods, such as CenterNet, eliminate the need for predefined anchors.</p> </li> <li> <p>Region-Based vs. Query-Based Methods:</p> </li> <li> <p>Region-based methods divide an image into regions (e.g., Faster R-CNN), while query-based methods like DETR use transformer architectures for set prediction.</p> </li> <li> <p>Plain vs. Hierarchical Methods:</p> </li> <li>Plain methods maintain a single-scale feature map, e.g., ViTDet, while hierarchical methods contain multi-scale features.</li> </ul>"},{"location":"object_detection/#two-stage-methods-faster-r-cnn","title":"Two-Stage Methods: Faster R-CNN","text":"<p>Faster R-CNN (Region-based Convolutional Neural Network):</p> <ul> <li> <p>Overview: A two-stage framework combining region proposal and object classification using a region proposal network (RPN).</p> </li> <li> <p>Links: R-CNN, Fast R-CNN, Faster R-CNN</p> </li> </ul>"},{"location":"object_detection/#one-stage-methods-yolo","title":"One-Stage Methods: YOLO","text":"<p>YOLO (You Only Look Once):</p> <ul> <li> <p>Overview: A one-stage algorithm dividing the image into a grid and predicting bounding boxes and class probabilities directly for real-time object detection.</p> </li> <li> <p>Links: YOLO, YOLO brief history</p> </li> </ul>"},{"location":"object_detection/#anchorless-methods-centernet","title":"Anchorless Methods: CenterNet","text":"<p>CenterNet:</p> <ul> <li> <p>Overview: An anchorless approach focusing on predicting object centers and regressing bounding box coordinates directly, eliminating the need for predefined anchors.</p> </li> <li> <p>Links: CenterNet</p> </li> </ul>"},{"location":"object_detection/#transformer-based-methods-detr","title":"Transformer-Based Methods: DETR","text":"<p>DETR (DEtection Transformer):</p> <ul> <li> <p>Overview: A transformer-based object detection model formulating object detection as a set prediction problem, simultaneously predicting object classes and bounding box coordinates.</p> </li> <li> <p>Links: LW-DETR, RT-DETR, D-FINE, DETR, deformableDETR</p> </li> </ul>"},{"location":"object_detection/#open-set-object-detection-open-vocabulary-object-detection-ovd","title":"Open-Set Object Detection | Open Vocabulary Object Detection (OVD)","text":"<ul> <li> <p>Overview: Open-set object detection, or open vocabulary object detection, aims to detect objects of novel categories beyond the training vocabulary. Traditional models are limited to a fixed set, but open-set detection scales up the vocabulary size.</p> </li> <li> <p>Links: Grounding DINO, OWL-VIT, Detic, paperwithcode list.</p> </li> </ul>"},{"location":"object_detection/#object-detection-with-limited-data","title":"Object Detection with Limited Data","text":"<p>To address the challenge of limited labeled data, leveraging pre-training in self-supervised learning is an effective strategy. Two prominent methods are contrastive learning and reconstruction-based methods. In contrastive learning, data augmentation is applied, and the model learns by bringing the representation of augmented parts together while pushing non-augmented parts further apart. Another method involves removing part of the data, and the model attempts to reconstruct the missing portion, as seen in Masked Autoencoders (MAE).</p> <p>Alternatively, foundation models\u2014large models trained on extensive datasets\u2014can serve as a pre-training step. These pre-trained models can be fine-tuned on specific tasks using a smaller dataset or used to distill knowledge into a smaller model, minimizing size while preserving performance.</p> <p>Another common approach involves training with different modalities, particularly text and image data, in a self-supervised manner. Following the success of models like CLIP, various methods, such as Grounding DINO and OWL-VIT, have adopted this approach for training.</p>"},{"location":"postgres_plpython3u/","title":"Set Up a Custom Python Function within PostgreSQL","text":"<p>Before proceeding with the following steps, ensure you have set up a new environment on your system with a custom Python virtual environment located at <code>/scratch/venv/envs/p310</code>. For additional setup instructions, refer to here.</p> <p>Follow these steps to get everything working from scratch:</p>"},{"location":"postgres_plpython3u/#step-1-install-postgresql-and-plpython3u","title":"Step 1: Install PostgreSQL and <code>plpython3u</code>","text":""},{"location":"postgres_plpython3u/#install-postgresql-and-the-plpython-extension-plpython3u","title":"Install PostgreSQL and the PL/Python Extension (<code>plpython3u</code>):","text":"<p>Run the following commands to install PostgreSQL and the <code>plpython3u</code> extension:</p> <pre><code>sudo apt update\nsudo apt install postgresql-plpython3-14\n</code></pre>"},{"location":"postgres_plpython3u/#enable-plpython3u-in-your-database","title":"Enable <code>plpython3u</code> in Your Database:","text":"<p>To enable <code>plpython3u</code> in a specific database (e.g., <code>test</code>), first log into PostgreSQL:</p> <pre><code>sudo -u postgres psql\n</code></pre> <p>Then, create your test database (if it doesn\u2019t already exist) and enable the extension:</p> <pre><code>CREATE DATABASE test; \n\\connect test\nCREATE EXTENSION plpython3u;\n</code></pre> <p>Exit the <code>psql</code> interface:</p> <pre><code>\\q\n</code></pre>"},{"location":"postgres_plpython3u/#step-2-create-a-postgresql-user-for-fkariminej","title":"Step 2: Create a PostgreSQL User for <code>fkariminej</code>","text":"<p>To facilitate authentication, create a PostgreSQL user matching your Linux system user.</p>"},{"location":"postgres_plpython3u/#create-a-postgresql-user","title":"Create a PostgreSQL User:","text":"<p>Open the PostgreSQL shell as the <code>postgres</code> superuser:</p> <pre><code>sudo -u postgres psql\n</code></pre> <p>Create the PostgreSQL user <code>fkariminej</code> with superuser privileges:</p> <pre><code>CREATE USER fkariminej WITH SUPERUSER CREATEDB CREATEROLE;\n</code></pre> <p>Exit the shell:</p> <pre><code>\\q\n</code></pre>"},{"location":"postgres_plpython3u/#access-the-database","title":"Access the Database:","text":"<p>From now on, you can directly access the database with the following command:</p> <pre><code>psql -U fkariminej -d test\n</code></pre>"},{"location":"postgres_plpython3u/#ensure-access-permissions","title":"Ensure Access Permissions:","text":"<p>Make sure the <code>postgres</code> user has access to your home directory and the relevant paths:</p> <pre><code>sudo chmod o+rx /home/fkariminej\nsudo chmod o+rx /scratch\nsudo chmod o+rx /scratch/venv\nsudo chmod o+rx /scratch/venv/envs\nsudo chmod o+rx /scratch/venv/envs/p310\n</code></pre>"},{"location":"postgres_plpython3u/#step-3-create-a-shell-script-to-start-postgresql-with-python-environment-variables","title":"Step 3: Create a Shell Script to Start PostgreSQL with Python Environment Variables","text":""},{"location":"postgres_plpython3u/#create-a-shell-script-to-start-postgresql","title":"Create a Shell Script to Start PostgreSQL:","text":"<p>Open a new shell script file for PostgreSQL:</p> <pre><code>vi /scratch/pg_service.sh\n</code></pre>"},{"location":"postgres_plpython3u/#add-environment-variables-for-python-paths-and-executable","title":"Add Environment Variables for Python Paths and Executable:","text":"<p>Add the following lines to set the <code>PYTHONPATH</code> and <code>PYTHONUSERBASE</code> for your virtual environment:</p> <pre><code>#!/bin/bash\n\nexport PGDATABASE=test\nexport PGUSER=fkariminej\nexport PGPORT=5432\nexport PATH=/usr/lib/postgresql/14/bin:$PATH\nexport PYTHONUSERBASE=/scratch/venv/envs/p310\nexport PYTHONPATH=/scratch/venv/envs/p310/lib/python310.zip:/scratch/venv/envs/p310/lib/python3.10:/scratch/venv/envs/p310/lib/python3.10/lib-dynload:/scratch/venv/envs/p310/lib/python3.10/site-packages\n\n# Start PostgreSQL using the correct configuration file\npg_ctl -D /etc/postgresql/14/main -l /var/lib/postgresql/14/main/postgres_logfile start\n</code></pre>"},{"location":"postgres_plpython3u/#make-the-script-executable","title":"Make the Script Executable:","text":"<pre><code>chmod +x /scratch/pg_service.sh\n</code></pre>"},{"location":"postgres_plpython3u/#create-and-set-permissions-for-the-logfile","title":"Create and Set Permissions for the Logfile:","text":"<pre><code>sudo touch /var/lib/postgresql/14/main/postgres_logfile\nsudo chown postgres:postgres /var/lib/postgresql/14/main/postgres_logfile\n</code></pre>"},{"location":"postgres_plpython3u/#stop-the-currently-running-postgresql-instance","title":"Stop the Currently Running PostgreSQL Instance:","text":"<pre><code>sudo systemctl stop postgresql@14-main.service\n</code></pre> <p>You can find the services:</p> <pre><code>systemctl list-units --type=service | grep postgresql\n</code></pre>"},{"location":"postgres_plpython3u/#run-the-script-as-the-postgres-user","title":"Run the Script as the <code>postgres</code> User:","text":"<pre><code>sudo -u postgres /scratch/pg_service.sh\n</code></pre>"},{"location":"postgres_plpython3u/#verify-the-configuration","title":"Verify the Configuration:","text":"<p>After running the script, verify the setup:</p> <pre><code>psql -U fkariminej -d test\n</code></pre>"},{"location":"postgres_plpython3u/#create-and-test-a-plpython-function","title":"Create and Test a PL/Python Function:","text":"<p>Create a PL/Python function and run it to ensure everything works:</p> <pre><code>CREATE OR REPLACE FUNCTION pymtorch()\nRETURNS float8 AS $$\nimport sys\nimport torch\nx = torch.Tensor([1, 3]) * torch.Tensor([4, 3]).to(torch.float32)\nreturn float(x.sum())\n$$ LANGUAGE plpython3u;\n\nSELECT pymtorch();\n</code></pre>"},{"location":"postgres_plpython3u/#step-4-create-a-new-systemd-service-for-automation","title":"Step 4: Create a New Systemd Service for Automation","text":"<p>To avoid manually stopping and restarting PostgreSQL every time the machine starts, create a new service:</p>"},{"location":"postgres_plpython3u/#create-a-new-systemd-service-file","title":"Create a New Systemd Service File:","text":"<pre><code>sudo vi /etc/systemd/system/newpostgres.service\n</code></pre>"},{"location":"postgres_plpython3u/#add-the-following-configuration","title":"Add the Following Configuration:","text":"<pre><code>[Unit]\nDescription=Custom PostgreSQL Service using /scratch/pg_service.sh\nAfter=network.target\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/bin/bash -c \"sudo systemctl stop postgresql@14-main.service &amp;&amp; sudo -u postgres /scratch/pg_service.sh\"\nExecStop=/bin/bash -c \"sudo -u postgres /usr/lib/postgresql/14/bin/pg_ctl -D /etc/postgresql/14/main stop\"\nUser=fkariminej\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"postgres_plpython3u/#reload-systemd-and-enable-the-new-service","title":"Reload Systemd and Enable the New Service:","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl enable newpostgres.service\nsudo systemctl start newpostgres.service\nsudo systemctl status newpostgres.service\n</code></pre>"},{"location":"postgres_plpython3u/#clean-up-residual-files-if-needed","title":"Clean Up Residual Files if Needed:","text":"<p>If issues arise, clean up any leftover files:</p> <pre><code>sudo rm -f /var/lib/postgresql/14/main/postmaster.pid\nsudo rm -f /var/run/postgresql/.s.PGSQL.5432\n</code></pre>"},{"location":"postgres_plpython3u/#check-for-active-connections","title":"Check for Active Connections:","text":"<pre><code>sudo lsof -i :5432\n</code></pre>"},{"location":"postgres_plpython3u/#verify-postgresql-processes-are-not-running","title":"Verify PostgreSQL Processes Are Not Running:","text":"<pre><code>ps aux | grep postgres\nsudo kill -9 &lt;PID&gt;\n</code></pre>"},{"location":"postgres_plpython3u/#re-enable-the-new-service","title":"Re-enable the New Service:","text":"<pre><code>sudo systemctl enable newpostgres.service\n</code></pre>"},{"location":"postgres_plpython3u/#notes","title":"Notes","text":"<p>The <code>postgresql@14-main.service</code> is located in <code>/lib/systemd/system/{postgresql@.service, postgresql.service}</code>. Their symbolic links are found here:</p> <pre><code>'/etc/systemd/system/multi-user.target.wants/postgresql@14-main.service' -&gt; '/lib/systemd/system/postgresql@.service'\n/etc/systemd/system/multi-user.target.wants/postgresql.service -&gt; /lib/systemd/system/postgresql.service\n</code></pre>"},{"location":"postgres_plpython3u/#other-important-paths","title":"Other important paths:","text":"<pre><code>/usr/lib/postgresql/14/bin\n/var/lib/postgresql/14/main  # owner:group -&gt; postgres:postgres\n/etc/postgresql/14/main      # pg_ctl.conf -&gt; owner:group postgres:postgres\n</code></pre>"},{"location":"postgres_plpython3u/#sql-commands-cheat-sheet","title":"SQL Commands Cheat Sheet","text":"<pre><code>/*\nmultiple comments\n*/\nCREATE DATABASE test; \nDROP DATABASE test; \n\\connect test \nCREATE EXTENSION plpython3u; \nCREATE USER fkariminej WITH SUPERUSER CREATEDB CREATEROLE;\nSHOW data_directory; -- PGDATA SHOW data_directory;\nSELECT current_user; -- PGUSER here fkariminej\nSELECT current_database(); -- PGDATABASE here test\n\\df  -- Show list of functions\n\n-- A specific function (e.g. my_func), a specific schema (e.g., my_schema)\nSELECT my_func(); -- Run the function\nDROP FUNCTION my_func; -- Remove a function\nDROP FUNCTION my_func(integer, text); -- Remove a function with parameter\nDROP FUNCTION my_schema.my_func; -- Remove a function exists in a specific schema\nDROP FUNCTION IF EXISTS my_func;\n\n-- Print the definition of a function\nSELECT pg_get_functiondef(oid) \nFROM pg_proc \nWHERE proname = 'my_func';\n\n-- Print the definition of a function with Schema Context\nSELECT pg_get_functiondef(p.oid) \nFROM pg_proc p\nJOIN pg_namespace n ON p.pronamespace = n.oid\nWHERE p.proname = 'my_func' AND n.nspname = 'my_schema';\n\n-- Output the function definition in a file\n\\o output_file.sql\nSELECT pg_get_functiondef(oid) \nFROM pg_proc \nWHERE proname = 'my_func';\n\\o\n\n\\q -- Exit\n</code></pre> <pre><code>sudo -u postgres psql\nsudo -u postgres /scratch/pg_service.sh\npsql -U fkariminej -d test\n</code></pre>"},{"location":"practical_info_data/","title":"Some Practical Information on Data and Training","text":""},{"location":"practical_info_data/#data-transfer","title":"Data Transfer","text":""},{"location":"practical_info_data/#copy-data-from-surfdrive-research-drive-to-remote-machine-google-colabsnellius","title":"Copy Data from Surfdrive / Research Drive to Remote Machine (Google Colab/Snellius)","text":"<p>You can copy your data to Surfdrive, UvA Research Drive, or SURF Research Drive by zipping your data and using drag and drop to transfer the data. Then, use the share icon to share your data with a password and set it to have no expiration time. Use the \"Copy to Clipboard\" option to obtain the link. You will receive a link similar to <code>https://surfdrive.surf.nl/files/index.php/s/IS00bWerWu3MDJS</code>. The last element, e.g., <code>IS00bWerWu3MDJS</code>, represents your username. Utilize this username along with the password you specified when sharing this folder, as shown in the signature below, to download your data.</p> <pre><code>curl -u username:password surf_public_link -o your_output_file\n</code></pre> <p>Public link</p> <ul> <li>The Surfdrive: https://surfdrive.surf.nl/files/public.php/webdav</li> <li>The UvA Research Drive: https://uva.data.surf.nl/public.php/dav/files</li> <li>The SURF Research Drive: https://researchdrive.surfsara.nl/public.php/webdav</li> </ul> <p>For UvA Research Drive, you need to be logged in first. Use this link: https://uva.data.surf.nl/apps/files.</p> <p>For the example provided above, here is the code to download the data using curl and then unzip the data. The entire process of obtaining the data and unzipping it took less than 2 minutes for 2.2GB of data. When using with the Google Colab, remember to prefix each command with the ! sign.</p> <pre><code>curl -u \"IS00bWerWu3MDJS:password\" \"https://surfdrive.surf.nl/files/public.php/webdav\" -o mot\nunzip mot -d mot_data &gt; /dev/null 2&gt;&amp;1\n</code></pre> <p>There is some information on SURF wiki for Research Drive, SURF wiki for SURFdrive or older one on surfnet for SURFdrive, but I found it unclear.</p> <p>This is another example for the UvA Research Drive:</p> <pre><code>curl -u \"jKF3B43Ti6Z6PWB:password\" https://uva.data.surf.nl/public.php/dav/files/jKF3B43Ti6Z6PWB/?accept=zip -o file.zip\n</code></pre>"},{"location":"practical_info_data/#copy-data-from-the-google-drive-to-the-google-colab","title":"Copy Data from the Google Drive to the Google Colab","text":"<p>The data can be dragged and dropped. Alternatively, you can copy your data using other methods:</p> <p>The below option is for sharing single file. It can be a zip file.</p> <p>Share the file with \"Share/Share/Anyone with the link\". Then \"Share/Copy Link\". You get the url like this: <code>https://drive.google.com/file/d/id_to_copy/view?usp=drive_link</code>. Use <code>id_to_copy</code> in <code>gdown</code>:</p> <pre><code>! pip install -U gdown requests\n! gdown id_to_copy --output /content/\n</code></pre> <p>The other option is to mount the whole Google drive (not recommanded):</p> <pre><code>from google.colab import drive\ndrive.mount(\"/content/drive\")\n!cp /content/test.yaml \"/content/drive/MyDrive\"\n</code></pre>"},{"location":"practical_info_data/#copy-data-from-crunchomics-to-snellius-and-reverse","title":"Copy Data from Crunchomics to Snellius and Reverse","text":"<p>N.B. You can only copy data via Crunchomics machine. Port 22 is closed on Snellius.</p> <pre><code># copy from Crunchomics to Snellius\nscp -r test.txt username@snellius.surf.nl:/home/username/test.txt\nrsync -avz --progress test.txt username@snellius.surf.nl:/home/username/test.txt\n\n# copy from Snellius to Crunchomics\nscp -r username@snellius.surf.nl:/home/username/test.txt .\nrsync -avz --progress username@snellius.surf.nl:/home/username/test.txt .\n</code></pre>"},{"location":"practical_info_data/#copy-data-from-snellius-to-src-surf-research-cloud","title":"Copy Data from Snellius to SRC (Surf Research Cloud)","text":"<p>In Snellius, generate ssh key and add it to SRC. Then create the environment in SRC. In Snellius, you can for example ssh, rsync and scp to this machine. If you have created a Jupyter notebook, the ip address can be found in the \"Workspace\" under \"Details\" of this Jupyter notebook. </p> <p>This option doesn't work for Crunchomics. Example:</p> <pre><code>ssh fkariminej@145.38.194.242\nscp fkariminej@145.38.194.242:/home/fkariminej/test.txt .\n</code></pre>"},{"location":"practical_info_data/#copy-data-from-local-machine-to-surf-filesender","title":"Copy Data from Local Machine to SURF Filesender","text":"<p>You can upload data from your local machine and send it to a recipient using filesender.py. To view the required arguments, run:</p> <pre><code>python filesender.py --help\n</code></pre> <p>The required arguments are:</p> <pre><code>  -u USERNAME, --username USERNAME       # Your username\n  -a APIKEY, --apikey APIKEY             # Your API key\n  -r RECIPIENTS, --recipients RECIPIENTS # Recipient's email address\n  -b BASE_URL, --base_url BASE_URL       # Base URL of the SURF Filesender\n  -f FROM_ADDRESS, --from_address FROM_ADDRESS # Your email address\n</code></pre> <p>To obtain the API key and username, follow these steps: 1. Go to \"My Profile\" on SURF Filesender. 2. Copy the \"Secret\" value for the API key to use with <code>-a,--apikey</code>. 3. Copy the \"Identifiant\" value for the username to use with <code>-u,--username</code>.</p> <p>The base URL (<code>-b, --base_url</code>) is always <code>https://filesender.surf.nl/rest.php</code>.</p> <p>The from address (<code>-f, --from_address</code>) is your email address, and the recipients (<code>-r, --recipients</code>) is the recipient's email address. At the end of the command, specify the file you want to send. </p> <p>Here is an example command:</p> <pre><code>python filesender.py -b https://filesender.surf.nl/rest.php -u my_username_key -a my_api_key -r recipient_email@uva.nl -f my_email@gmail.com my_file_to_be_sent.zip\n</code></pre> <p>Prerequisites: Before using this script, make sure you have the following Python packages installed:</p> <pre><code>pip install requests urllib3 cryptography\n</code></pre>"},{"location":"practical_info_data/#references","title":"References","text":"<ul> <li>Surfdrive</li> <li>SRC(SURF Research Cloud)</li> <li>SURF Filesender</li> </ul>"},{"location":"python/","title":"Python Basics","text":""},{"location":"python/#setup-python","title":"Setup Python","text":"<p>Look at the guide in here or follow bellow steps.</p> <p>Install conda:</p> <pre><code>mkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm ~/miniconda3/miniconda.sh\n</code></pre> <p>After installing, initialize your newly-installed Miniconda.</p> <pre><code>~/miniconda3/bin/conda init bash\n</code></pre> <p>Setup your python:</p> <pre><code>conda create -n some_name python=3.10\nconda activate some_name\n</code></pre> <p>You can put <code>conda activate some_name</code> in your <code>~/.bashrc</code> to activate it when open a new terminal.</p> <p>Remove your conda environment:</p> <pre><code>conda remove -n some_name --all\n</code></pre>"},{"location":"python/#remove-conda","title":"Remove Conda","text":"<p>To completely remove Miniconda from your Ubuntu system, including all associated configurations and files, follow these steps:</p>"},{"location":"python/#jupyter-kernel","title":"Jupyter Kernel","text":"<pre><code># This process installs a new Jupyter kernel named test with the display name \"bird\".\npip install ipykernel\nsudo python -m ipykernel install --name test --display-name bird\n\n# Location of the Installed Kernel\njupyter kernelspec list\n# This command will display a list of all installed kernels along with their corresponding paths. \n#  test       /home/your_username/.local/share/jupyter/kernels/test\n\n# Removing the Installed Kernel\njupyter kernelspec uninstall test\n</code></pre>"},{"location":"python/#deactivate-any-active-conda-environments","title":"Deactivate Any Active Conda Environments","text":"<p>Ensure that no Conda environments are active by running:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"python/#remove-the-miniconda-installation-directory","title":"Remove the Miniconda Installation Directory","text":"<p>Delete the directory where Miniconda is installed. By default, this is <code>~/miniconda3</code>. Use the following command, adjusting the path if necessary:</p> <pre><code>rm -rf ~/miniconda3/\n</code></pre> <p>Note: Replace <code>~/miniconda3/</code> with the correct path if you installed Miniconda elsewhere.</p>"},{"location":"python/#remove-conda-related-hidden-files-and-directories","title":"Remove Conda-Related Hidden Files and Directories","text":"<p>Remove hidden files and directories in your home directory that Conda uses:</p> <pre><code>rm -rf ~/.conda ~/.condarc ~/.continuum\n</code></pre> <p>These directories store Conda environments and settings.</p>"},{"location":"python/#remove-conda-initialization-from-shell-configuration","title":"Remove Conda Initialization from Shell Configuration","text":"<p>Conda adds initialization code to your shell's configuration file (e.g., <code>.bashrc</code>). To remove these lines:</p> <ul> <li>Open the configuration file in a text editor:</li> </ul> <pre><code>nano ~/.bashrc\n</code></pre> <ul> <li>Scroll to the section managed by 'conda init', which looks like:</li> </ul> <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/home/username/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/home/username/miniconda3/etc/profile.d/conda.sh\" ]; then\n        . \"/home/username/miniconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/home/username/miniconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> <ul> <li> <p>Delete this entire block.</p> </li> <li> <p>Save the file and exit the editor (in nano, press <code>CTRL+O</code> to save and <code>CTRL+X</code> to exit).</p> </li> <li> <p>Apply the changes by sourcing the file:</p> </li> </ul> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"python/#remove-any-remaining-conda-related-cache","title":"Remove Any Remaining Conda-Related Cache","text":"<p>Check for and remove any remaining Conda-related cache files:</p> <pre><code>rm -rf ~/.cache/conda\n</code></pre>"},{"location":"python/#pytest","title":"Pytest","text":""},{"location":"python/#running-tests","title":"Running Tests","text":"<p>You can run your tests using:</p> <pre><code>pytest tests\n# Ignore tests with specific markers\npytest tests -m \"not ignore and not local\"\n</code></pre>"},{"location":"python/#debugging-in-vs-code","title":"Debugging in VS Code","text":"<p>To configure Pytest in VS Code, press <code>Ctrl+Shift+P</code> (or <code>Cmd+Shift+P</code> on macOS), then search for \"Python: Configure Tests\" and select it.</p> <p>Once configured correctly: - You should see \"Run Test | Debug Test\" options above each test function. - Click \"Debug Test\" to start debugging.</p>"},{"location":"python/#conda-commands-cheat-sheet","title":"Conda Commands Cheat Sheet","text":"<pre><code>conda create -n some_name python=3.10 # Create a new Conda virtual env\nconda activate some_name # Activate the Conda virtual env\nconda remove -n some_name --all # Remove the Conda virtual env\nconda install openblas-devel -c anaconda # Install a package from Conda\nconda list | grep blas # Search for a package in the installed list (similar to `pip freeze | grep blas`)\necho $CONDA_PREFIX # Path of the currently active env (e.g., ~/miniconda3/envs/some_name)\n</code></pre>"},{"location":"python/#useful-pip-commands","title":"Useful pip Commands","text":"<pre><code>pip -U install package1 packag2 # -U: for update\npip uinstall package\npip-autoremove -y package1 package2  # Remove package completely. First install `pip install -y pip-autoremove`\npip freeze\npip list\npip check\n</code></pre>"},{"location":"python/#python-courses","title":"Python Courses","text":"<p>From Software Carpentry, below courses are offered. </p> <ul> <li>Programming with Python</li> <li>Plotting and Programming in Python</li> </ul> <p>A CS50 course offered here.</p> <p>For more on software engineering side, you could also attend this course: - Intermediate Research Software Development with Python</p>"},{"location":"pytorch_lightning/","title":"Pytorch Lightning","text":"<p>Integrating Hydra with PyTorch Lightning can significantly enhance the flexibility and scalability of your machine learning projects. Hydra simplifies configuration management, allowing you to easily modify hyperparameters and settings without altering your codebase. PyTorch Lightning streamlines the training process by providing a structured framework for PyTorch code. Additionally, PyTorch Lightning facilitates advanced features like mixed precision training, Fully Sharded Data Parallel (FSDP), Distributed Data Parallel (DDP) across multiple nodes, and multi-GPU training, making it a powerful choice for scaling and optimizing your deep learning workflows.</p>"},{"location":"pytorch_lightning/#example-integrating-hydra-with-pytorch-lightning","title":"Example: Integrating Hydra with PyTorch Lightning","text":"<pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nimport lightning as L\nfrom omegaconf import DictConfig\nimport hydra\nfrom lightning.pytorch.loggers import WandbLogger\n\ntorch.set_float32_matmul_precision('high')\n\nclass LitModel(L.LightningModule):\n    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate):\n        super(LitModel, self).__init__()\n        self.save_hyperparameters()\n        self.layer_1 = nn.Linear(input_dim * input_dim, hidden_dim)\n        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('val_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n@hydra.main(version_base=None, config_path=\"configs\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Data\n    dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n    train_loader = DataLoader(mnist_train, batch_size=32, num_workers=15)\n    val_loader = DataLoader(mnist_val, batch_size=32, num_workers=15)\n\n    # Model\n    model = LitModel(\n        input_dim=cfg.model.input_dim,\n        hidden_dim=cfg.model.hidden_dim,\n        output_dim=cfg.model.output_dim,\n        learning_rate=cfg.model.learning_rate\n    )\n\n    # Initialize W&amp;B logger\n    wandb_logger = WandbLogger(project='my-awesome-project')\n\n    # Trainer\n    trainer = L.Trainer(\n        accelerator='gpu',\n        devices=cfg.trainer.gpus,\n        max_epochs=cfg.trainer.max_epochs,\n        precision='16-mixed',\n        logger=wandb_logger,\n    )\n\n    # Training\n    trainer.fit(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"pytorch_lightning/#create-a-configuration-file","title":"Create a Configuration File:","text":"<pre><code># config.yaml\nmodel:\n  input_dim: 28\n  hidden_dim: 64\n  output_dim: 10\n  learning_rate: 0.001\n\ntrainer:\n  max_epochs: 10\n  gpus: 1\n</code></pre>"},{"location":"pytorch_lightning/#install-the-required-libraries","title":"Install the Required Libraries:","text":"<pre><code>pip install lightning hydra-core wandb\n</code></pre>"},{"location":"pytorch_lightning/#run-the-training-script","title":"Run the Training Script:","text":"<pre><code>python train.py\n</code></pre> <p>To override specific parameters without modifying the config.yaml file, use command-line arguments:</p> <pre><code>python train.py model.learning_rate=0.01 trainer.max_epochs=20\n</code></pre>"},{"location":"pytorch_lightning/#benefits-of-using-hydra-with-pytorch-lightning","title":"Benefits of Using Hydra with PyTorch Lightning:","text":"<ul> <li>Flexible Configuration Management: Hydra allows you to maintain a clean separation between code and configuration, facilitating easy experimentation with different settings.</li> <li>Command-Line Overrides: Easily adjust parameters via command-line arguments, enabling rapid testing of various configurations.</li> <li>Scalability: PyTorch Lightning's structured approach, combined with Hydra's configuration management, supports scaling from simple experiments to complex training pipelines.</li> </ul>"},{"location":"resource_limitations/","title":"Training and Inference with Limited Resources","text":""},{"location":"resource_limitations/#training-with-resource-limitations","title":"Training with Resource Limitations","text":"<p>Training large deep learning models is notably resource-intensive, often presenting challenges in both memory and computational demands. In contrast, smaller models, while less demanding, may lack descriptive power. A practical approach to training larger models involves starting with a pre-trained model and then fine-tuning a small subset of parameters, typically the head of the model. If your training time is limited to just a few hours, it's advisable to checkpoint the model and resume training from the last saved checkpoint. Below, we present a variety of techniques designed to either reduce memory usage or/and enhance computational efficiency.</p> <ul> <li>Training: Cut Cross-Entropy (CCE), gradient accumulation, gradient checkpointing and CPU offloading. Gradient accumulation involves accumulating gradients over multiple mini-batches before updating model parameters. It's useful when the available memory is insufficient for a desired batch size. Gradient checkpointing (also known as activation checkpointing)) reduces memory usage by not saving intermediate tensors required for the backward pass. These tensors are recomputed during the backward pass, which increases computation time. CPU offloading stores weights in CPU RAM rather than on the GPU when they are not in use.</li> <li>Fine-Tuning Tricks: Fine-tune only a small number of parameters (PEFT), e.g., LoRA/controlNet.</li> <li>Specific to Attention Blocks in Transformers: FlashAttention, Flash-decoding.</li> <li>Tricks for GPU: Half-precision, quantization, paged optimizers (GPU to CPU transfer used in QLoRA for optimizer states). Examples are: fp32 -&gt; fp16/bf16 -&gt; int8 -&gt; nf4 (normal float 4-bit).</li> </ul>"},{"location":"resource_limitations/#inference-with-resource-limitations","title":"Inference with Resource Limitations","text":"<ul> <li>Model Parameters: <ul> <li>Model distillation: Distill a large model as a teacher model to a student model using distillation loss.</li> <li>Quantization techniques: Weight clustering (aka low-bit parallelization) is a compression technique.</li> </ul> </li> </ul>"},{"location":"resource_limitations/#nb","title":"N.B.","text":"<ul> <li>Memory consists of parameters (weights), gradients, optimizer states, and activations (batch size x largest layer).</li> <li>QLoRA freezes and quantizes the main model and adds a low-rank adapter (LoRA).</li> </ul>"},{"location":"resource_limitations/#references","title":"References","text":"<ul> <li>Fine-tuning of 7B model parameters on T4 from DeepLearning AI by Ludwig, presented by Travis Addair (watch from here to here.</li> <li>train a 70b language model on two 24GB GPUs: an open source system, based on FSDP and QLoRA, that can train a 70b model on two 24GB GPUs. They also used Gradient checkpointing, CPU offloading, and Flash Attention 2.</li> <li>LoRA, LoQT: Low-Rank Adapters for Quantized Pretraining</li> <li>SURF course on Profiling</li> <li>A Guide to Quantization in LLMs</li> </ul>"},{"location":"resource_limitations/#example-code","title":"Example code","text":"<p>Using fp16 (float16) in PyTorch:</p> <p>The detail explanation is in Automatic Mixed Precision and Example mixed precision training in pytroch.</p> <pre><code>import torch\n\n# Initialize model, optimizer, and other components\nmodel = MyModel().cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\nscaler = torch.GradScaler()\n\nfor inputs, labels in data_loader:\n    inputs, labels = inputs.cuda(), labels.cuda()\n\n    optimizer.zero_grad()\n\n    # Casts operations to mixed precision\n    with torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n\n    # Scales the loss and calls backward()\n    scaler.scale(loss).backward()\n\n    # Unscales gradients and calls optimizer step\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <p>Using bf16 (bfloat16) in PyTorch:</p> <p>It is the same as fp16 but without a scaler.</p>"},{"location":"resource_limitations/#pytorch-profiling","title":"Pytorch Profiling","text":"<p>The PyTorch Profiler is a tool that allows developers to understand and optimize their PyTorch code by analyzing its performance. Here's an example of setting up and using the PyTorch Profiler:</p>"},{"location":"resource_limitations/#code-example-with-pytorch-profiler","title":"Code Example with PyTorch Profiler","text":"<p>For more details take look at Tensorboard Profiler Tutorial.</p>"},{"location":"resource_limitations/#code-example","title":"Code Example","text":"<p>Here is a step-by-step example of setting up and using the PyTorch Profiler. </p> <pre><code>import torch\nimport torchvision.models as models\nfrom torchvision.models import ResNet18_Weights\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n# Set up a model and input data\nmodel = models.resnet18(weights=ResNet18_Weights.DEFAULT)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Generate dummy input data\ninput_data = torch.randn(8, 3, 224, 224).to(device)  # Batch of 8 images\n\n# Define the profiling configuration\nwith profile(\n    activities=[\n        ProfilerActivity.CPU,  # Monitor CPU activity\n        ProfilerActivity.CUDA  # Monitor CUDA activity (if applicable)\n    ],\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\"),  # Save data for TensorBoard\n    profile_memory=True,\n    record_shapes=True,  # Record tensor shapes\n    with_stack=True  # Capture stack traces\n) as prof:\n\n    # Use record_function for specific profiling scopes\n    with record_function(\"model_inference\"):\n        output = model(input_data)  # Run the model inference\n\n# Analyze the profiler output\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# Visualize the profiler output using TensorBoard:\n# Run `tensorboard --logdir=./log` in the terminal\n</code></pre> <ul> <li><code>profile</code> Context Manager: This manages the profiling session and specifies which activities (CPU, CUDA) to profile.</li> <li><code>record_function</code>: Labels a specific code block for profiling, so you can see its performance separately.</li> <li><code>tensorboard_trace_handler</code>: Saves the profiling results in a format compatible with TensorBoard.</li> <li><code>key_averages()</code>: Aggregates and summarizes profiling results for analysis in the console.</li> </ul> <p>You can customize the profiler to include:</p> <ul> <li>Custom intervals: Use <code>schedule</code> to specify profiling start and stop.</li> <li>Memory profiling: Set <code>profile_memory=True</code> to track memory usage.</li> <li>Exporting results: Save results to file using <code>prof.export_chrome_trace(\"trace.json\")</code>.</li> </ul> <p>Notes: Use smaller models or batches for testing, as profiling large models can generate a lot of data.</p>"},{"location":"resource_limitations/#visualize-the-profiler-output","title":"Visualize the profiler output","text":"<p>After generating a trace, simply drag the <code>trace.json</code> generated in <code>log</code> file (example above) into Perfetto UI or in chrome browser by typing <code>chrome://tracing</code> to visualize your profile.</p> <p>The TensorBoard integration with the PyTorch profiler is now deprecated. But if you still want to use TensorBoard you should install <code>pip install torch_tb_profiler</code> and then use <code>tensorboard --logdir=./log</code></p>"},{"location":"resource_limitations/#cuda-memory-usage","title":"CUDA Memory Usage","text":"<p>For more details take look at torch_cuda_memory or understanding-gpu-memory-1.</p> <p>Disclaimer: The example below is taken from AlphaSignal.</p> <pre><code>import torch\nfrom torch import nn\n\n# Start recording memory snapshot history\ntorch.cuda.memory._record_memory_history(max_entries=100000)\n\n# Example model and computation\nmodel = nn.Linear(10_000, 50_000, device=\"cuda\")\nfor _ in range(3):\n    inputs = torch.randn(5_000, 10_000, device=\"cuda\")\n    outputs = model(inputs)\n\n# Dump memory history to a file and stop recording\ntorch.cuda.memory._dump_snapshot(\"profile.pkl\")\ntorch.cuda.memory._record_memory_history(enabled=None)\n</code></pre> <p>The code generates <code>profile.pkl</code> file. Open it in pytorch.org/memory_viz.</p>"},{"location":"resource_limitations/#references_1","title":"References","text":"<ul> <li>Performance Tuning Guide</li> <li>What Every User Should Know About Mixed Precision Training in PyTorch</li> </ul>"},{"location":"resource_limitations/#glossary","title":"Glossary","text":"<p>Model has L layers applys on the data of B batch size, S sequence length and D dimension (B x S x D). Parallelism applies on each part. Here is the names:</p> <ul> <li>Data Parallelism splits on B dimension (DP). Examples are DDP, FSDP (Fully Sharded Data Parallel), HSDP (Hybrid Sharded Data Parallel)</li> <li>Context Parallelism splits on S dimension (CP)</li> <li>Tensor Parallelism splits on D (TP)</li> <li>Pipeline Parallelism splits on L (PP)</li> </ul> <p>For more information watch the CS231N lecture on large distributed training or check the Huggging Face ultrascal playbook.</p> <p>Data Parallelism \u2014 Splits data across multiple devices, each running the same model. Every GPU holds a full copy of the model, processes a different mini-batch, and then all-reduces gradients to keep weights in sync.</p> <p>FSDP (Fully Sharded Data Parallel) \u2014 Shards model parameters, gradients, and optimizer states across data-parallel workers, cutting memory per GPU while still syncing each training step.</p> <p>Pipeline Parallelism \u2014 Divides model layers into sequential stages on different GPUs; many micro-batches flow through the pipeline so stages run concurrently.</p> <p>Context Parallelism \u2014 Partitions long input sequences over GPUs.</p> <p>Training, Fine-Tuning &amp; Inference Tools</p> <ul> <li>PyTorch (torchrun, DDP, FSDP): <code>torchrun</code> launches multi-process jobs across GPUs.</li> <li>DeepSpeed: Adds ZeRO stages that shard optimizer states/gradients and offers optional pipeline or tensor parallelism.</li> <li>Hugging Face Accelerate: Thin wrapper that lets the same PyTorch script jump from a single GPU to multi-GPU/TPU or plug into DeepSpeed/FSDP with just a few lines of code.</li> <li>Hugging Face PEFT: Implements LoRA, QLoRA, prefix/prompt tuning and more so you fine-tune large models by training only small adapter weights, cutting compute and storage costs.</li> <li>Hugging Face Optimum: Toolkit of hardware-specific optimizations \u2014 Bridges Transformers to ONNX Runtime, TensorRT, OpenVINO and other back ends, providing graph optimizations and quantization for faster inference on diverse hardware.</li> <li>vLLM: High-throughput GPU inference engine that uses PagedAttention and continuous batching to boost LLM serving throughput while reducing memory fragmentation.</li> <li>Unsloth: Lightweight library focused on fast LoRA/QLoRA fine-tuning; fused kernels and chunked loading.</li> </ul>"},{"location":"revealjs/","title":"Slides from Markdown","text":"<p>This is a tiny workflow to turn a <code>slides.md</code> file into a Reveal.js presentation (<code>slides.html</code>) using Python.</p>"},{"location":"revealjs/#files-you-need","title":"Files you need","text":"<p>Create two files in the same folder:</p> <ul> <li><code>make_slides.py</code> (build script)</li> <li><code>slides.md</code> (your slide content)</li> </ul>"},{"location":"revealjs/#1-create-make_slidespy","title":"1) Create <code>make_slides.py</code>","text":"<p>Save this as <code>make_slides.py</code>:</p> <pre><code>import pypandoc\n\n# Download pandoc if it isn't available (downloads a local copy pypandoc can use)\npypandoc.download_pandoc()\n\npypandoc.convert_file(\n    \"slides.md\",\n    \"revealjs\",\n    outputfile=\"slides.html\",\n    extra_args=[\n        \"--standalone\",\n        \"-t\", \"revealjs\",\n        \"-V\", \"revealjs-url=https://unpkg.com/reveal.js@5\",\n        \"-V\", \"theme=black\",\n        \"--mathjax\",\n        \"--syntax-highlighting=pygments\",\n    ],\n)\n\nprint(\"Slides written to slides.html\")\n</code></pre> <p>What this does:</p> <ul> <li>Reads <code>slides.md</code></li> <li>Produces a standalone <code>slides.html</code></li> <li>Loads Reveal.js from the CDN</li> <li>Enables MathJax for formulas</li> <li>Enables syntax highlighting for code blocks</li> </ul>"},{"location":"revealjs/#2-create-slidesmd","title":"2) Create <code>slides.md</code>","text":"<p>Save this as <code>slides.md</code>:</p> <pre><code># Title slide\n\nYour name  \nDate\n\n---\n\n## Second slide\n\n- Bullet 1\n- Bullet 2\n\n---\n\n## Third slide\n\nYou can write **normal markdown** here, formula $\\omega$ or code.\n\n$$x^2 + y^2 = 1$$\n\n```python\ndef f(x):\n    return x * x\n```\n</code></pre>"},{"location":"revealjs/#3-install-dependencies","title":"3) Install dependencies","text":"<p>Install pypandoc:</p> <pre><code>pip install pypandoc\n</code></pre> <p>You do not need to install Pandoc separately, because the script downloads it automatically.</p>"},{"location":"revealjs/#4-build-the-slides","title":"4) Build the slides","text":"<p>Run:</p> <pre><code>python make_slides.py\n</code></pre> <p>This writes <code>slides.html</code>.</p>"},{"location":"revealjs/#5-view-the-presentation","title":"5) View the presentation","text":"<p>Open <code>slides.html</code> in your browser (double-click it or use your file manager).</p> <p>Navigation tips:</p> <ul> <li>Arrow keys: next/previous slide</li> <li><code>Esc</code>: overview</li> <li><code>F</code>: fullscreen</li> <li>URL hash updates are enabled, so slide position is reflected in the URL</li> </ul>"},{"location":"scheuler/","title":"Scheduler","text":"<p>Here is a sample example to demonstrate how the scheduler changes the learning rate. You may choose to use a different scheduler.</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\n# Define a dummy model\nmodel = nn.Linear(10, 1)\n\n# Define optimizer and scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.1)\n\n# TensorBoard writer\nwriter = SummaryWriter(\"result/tensorboard/run1\")\n\n# Dummy training loop\nnum_epochs = 2500\nsteps_per_epoch = 10\n\nfor epoch in tqdm(range(num_epochs)):\n    for step in range(steps_per_epoch):\n        # Dummy input and loss\n        inputs = torch.randn(32, 10)\n        targets = torch.randn(32, 1)\n        outputs = model(inputs)\n        loss = nn.MSELoss()(outputs, targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Log learning rate to TensorBoard\n        current_lr = round(optimizer.param_groups[-1][\"lr\"], 6)\n        global_step = epoch * steps_per_epoch + step\n        writer.add_scalar(\"Learning Rate\", current_lr, global_step)\n\n    # Update the scheduler\n    scheduler.step()\n\n    lr_optim = round(optimizer.param_groups[-1][\"lr\"], 6)\n    lr_sched = scheduler.get_last_lr()[0]\n    writer.add_scalar(\"lr/optim\", lr_optim, epoch)\n    writer.add_scalar(\"lr/sched\", lr_sched, epoch)\n\n\n# Close the writer\nwriter.close()\n</code></pre>"},{"location":"segmenttree_install/","title":"Installing Deep Learning Tools for Point Cloud Segmentation","text":""},{"location":"segmenttree_install/#minkowskiengine-installation","title":"MinkowskiEngine Installation","text":""},{"location":"segmenttree_install/#on-snellius","title":"On Snellius","text":"<p><code>conda</code> should be installed first. This part doesn't require GPU machine and only done once.</p> <pre><code>module load 2023\nmodule load Anaconda3/2023.07-2\nconda init\n</code></pre> <p>After <code>conda init</code>, the terminal should be closed. So be careful, if you allocated the GPU, you will loss your GPUs, unless you do <code>ssh</code> to the allocated machine. E.g. <code>ssh gcn14</code>. </p> <p><code>MinkowskiEngine</code> requires GPU. You can ask for GPU by <code>salloc</code>.</p>"},{"location":"segmenttree_install/#cuda-11","title":"CUDA 11","text":"<pre><code>salloc --gpus=1 --partition=gpu --time=02:00:00\n\n# MinkowskiEngine==0.5.4\nconda create -n mink2 python=3.8\npython activate mink2\nmodule load 2022\nmodule load CUDA/11.7.0 # CUDA/11.6.0 was giving error\nconda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch\nconda install openblas-devel -c anaconda\n\ngit clone https://github.com/NVIDIA/MinkowskiEngine.git\ncd MinkowskiEngine\npython setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas\n</code></pre> <p>Note that CUDA is backwards compatible, so <code>CUDA 11.1</code> can be also used for all packages.</p> <pre><code>pip install numpy==1.19.5\nconda install pytorch=1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=11.1 -c pytorch -c nvidia\n</code></pre>"},{"location":"segmenttree_install/#cuda-12","title":"CUDA 12","text":"<p>Only runs on H100. <code>MinkowskiEngine</code> doesn't work before some changes in the <code>MinkowskiEngine</code> code. This fix is described here.</p> <p>Fix: There is an issue described in here. This fix is based on this link.</p> <ul> <li>Get the code</li> </ul> <pre><code>git clone https://github.com/NVIDIA/MinkowskiEngine.git \ncd MinkowskiCu12\n</code></pre> <ul> <li>Fix the code</li> </ul> <pre><code>1 - .../MinkowskiEngine/src/convolution_kernel.cuh\n\nAdd header:\n#include &lt;thrust/execution_policy.h&gt;\n\n2 - .../MinkowskiEngine/src/coordinate_map_gpu.cu\n\nAdd headers:\n\n#include &lt;thrust/unique.h&gt;\n#include &lt;thrust/remove.h&gt;\n\n3 - .../MinkowskiEngine/src/spmm.cu\n\nAdd headers:\n\n#include &lt;thrust/execution_policy.h&gt;\n#include &lt;thrust/reduce.h&gt; \n#include &lt;thrust/sort.h&gt;\n\n4 - .../MinkowskiEngine/src/3rdparty/concurrent_unordered_map.cuh\n\nAdd header:\n#include &lt;thrust/execution_policy.h&gt;\n</code></pre> <ul> <li>Installation</li> </ul> <pre><code>salloc --gpus=1 --partition=gpu_h100 --time=02:00:00\n\nmodule load 2023\nmodule load CUDA/12.1.1\nconda create -n mink-cu12 python=3.8\nconda activate mink-cu12\n# https://pytorch.org/get-started/previous-versions/\nconda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia\nconda install openblas-devel -c anaconda\n\ncd ~/dev/MinkowskiCu12\n# Before this command apply the Fix. \npython setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas\n</code></pre>"},{"location":"segmenttree_install/#docker-cuda-12-with-the-fix","title":"Docker CUDA 12 with the Fix","text":"<p>This information is from here.</p> <p>The base image is <code>nvidia/cuda:12.1.1-devel-ubuntu20.04</code> with <code>PyTorch 2.3.1</code> and <code>CUDA 12.1</code>.</p> <pre><code>ENV CUDA_HOME=/usr/local/cuda\nENV TORCH_CUDA_ARCH_LIST=\"6.0 6.1 6.2 7.0 7.2 7.5 8.0 8.6 8.9\"\nENV TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\"\nRUN git clone https://github.com/NVIDIA/MinkowskiEngine.git /tmp/MinkowskiEngine \\\n    &amp;&amp; cd /tmp/MinkowskiEngine \\\n    &amp;&amp; sed -i '31i #include &lt;thrust/execution_policy.h&gt;' ./src/convolution_kernel.cuh \\\n    &amp;&amp; sed -i '39i #include &lt;thrust/unique.h&gt;\\n#include &lt;thrust/remove.h&gt;' ./src/coordinate_map_gpu.cu \\\n    &amp;&amp; sed -i '38i #include &lt;thrust/execution_policy.h&gt;\\n#include &lt;thrust/reduce.h&gt;\\n#include &lt;thrust/sort.h&gt;' ./src/spmm.cu \\\n    &amp;&amp; sed -i '38i #include &lt;thrust/execution_policy.h&gt;' ./src/3rdparty/concurrent_unordered_map.cuh \\\n    &amp;&amp; python setup.py install --force_cuda --blas=openblas \\\n    &amp;&amp; cd - \\\n    &amp;&amp; rm -rf /tmp/MinkowskiEngine\n</code></pre>"},{"location":"segmenttree_install/#test-minkowskiengine","title":"Test MinkowskiEngine","text":"<pre><code>import torch\nimport torch.nn as nn\nimport MinkowskiEngine as ME\nimport sys\nsys.path.insert(0,\"~/dev/MinkowskiEngine/tests/python\")\nfrom common import data_loader\n\nclass ExampleNetwork(ME.MinkowskiNetwork):\n    def __init__(self, in_feat, out_feat, D):\n        super(ExampleNetwork, self).__init__(D)\n        self.conv1 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channels=in_feat,\n                out_channels=64,\n                kernel_size=3,\n                stride=2,\n                dilation=1,\n                bias=False,\n                dimension=D),\n            ME.MinkowskiBatchNorm(64),\n            ME.MinkowskiReLU())\n        self.conv2 = nn.Sequential(\n            ME.MinkowskiConvolution(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=2,\n                dimension=D),\n            ME.MinkowskiBatchNorm(128),\n            ME.MinkowskiReLU())\n        self.pooling = ME.MinkowskiGlobalPooling()\n        self.linear = ME.MinkowskiLinear(128, out_feat)\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.pooling(out)\n        return self.linear(out)\n\ncriterion = nn.CrossEntropyLoss()\nnet = ExampleNetwork(in_feat=3, out_feat=5, D=2)\nprint(net)\n\n# a data loader must return a tuple of coords, features, and labels.\ncoords, feat, label = data_loader()\ninput = ME.SparseTensor(feat, coordinates=coords)\n# Forward\noutput = net(input)\n\n# Loss\nloss = criterion(output.F, label)\n</code></pre>"},{"location":"segmenttree_install/#install-other-packages-for-segmentanytree","title":"Install other packages For SegmentAnyTree","text":"<pre><code># open3d, torch-sparse, torch-spline-conv: very slow\n# torch-points-kernels: different version of numpy, sklearn\n# Issue with pip install torch-scatter, torch-sparse, torch-cluster, torch-geometric. \n# I use it from: https://github.com/prs-eth/PanopticSegForLargeScalePointCloud\npip install torch-scatter==2.0.8 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\npip install torch-sparse==0.6.12 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\npip install torch-cluster==1.5.9 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\npip install torch-cluster==1.2.1 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html \npip install torch-geometric==1.7.2\n\npip install -U pip\npip install hydra-core wandb tqdm gdown types-six types-requests h5py\npip install numpy scikit-image numba \npip install matplotlib\npip install plyfile\npip install torchnet tensorboard\npip install open3d\npip install torch-scatter torch-sparse torch-cluster torch-geometric pytorch_metric_learning torch-points-kernels\nTORCH_CUDA_ARCH_LIST=\"6.0;7.0;7.5;8.0;8.6;9.0\" FORCE_CUDA=1\npip install cython\npip install torch-spline-conv\npip install git+https://github.com/mit-han-lab/torchsparse.git # failed to install\n</code></pre>"},{"location":"segmenttree_install/#run-segmentanytree-from-docker","title":"Run SegmentAnyTree From Docker","text":""},{"location":"segmenttree_install/#on-a-local-computer","title":"On a local computer","text":"<p>You need to install the NVIDIA Container Toolkit. Follow the instructions here.</p> <pre><code>docker pull donaldmaen/segment-any-tree\n\ndocker run -it --gpus all --name SAT     --mount type=bind,source=/home/fatemeh/Downloads/tree/SAT/,target=/home/nibio/mutable-outside-world/bucket_in_folder     --mount type=bind,source=/home/fatemeh/Downloads/tree/SAT/out,target=/home/nibio/mutable-outside-world/bucket_out_folder   --mount type=bind,source=/home/fatemeh/Downloads/tree/ds,target=/home/datascience  donaldmaen/segment-any-tree\n</code></pre>"},{"location":"segmenttree_install/#on-snellius_1","title":"On Snellius","text":"<p>Since Snellius uses <code>apptainer</code> instead of Docker, follow these steps:</p> <pre><code># On Snellius, the GPU container option is enabled. Run this command inside Apptainer to check if it detects the GPU:\n# First, request a GPU with the following command:\nsalloc --gpus=1 --partition=gpu --time=01:00:00\n\n# Test GPU visibility inside Apptainer:\napptainer run --nv docker://nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi\n\n# Pull the container image:\napptainer pull docker://donaldmaen/segment-any-tree\n\n# Start an interactive shell session with Apptainer:\napptainer shell --nv  --bind /home/user/data/tree/sat:/home/nibio/mutable-outside-world/bucket_in_folder     --bind /home/user/data/tree/sat/output:/home/nibio/mutable-outside-world/bucket_out_folder  --bind /home/user/data/tree/ds:/home/datascience    /home/user/segment-any-tree_latest.sif\n\n# Once inside Apptainer, run the following commands:\nApptainer&gt; cd dev/SegmentAnyTree/\nApptainer&gt; bash run_oracle_pipeline.sh\n</code></pre> <p>Note that if you use apptainer run instead of apptainer shell, it runs everything automatically.</p>"},{"location":"segmenttree_install/#install-treelearn","title":"Install TreeLearn","text":"<p>This content is adapted from the TreeLearn Pipeline Notebook.</p> <pre><code># Note that newer version laspy[lazrs] gives error \n# spconv-cu120 comes with warnings and then Floating point exception (core dumped).\nconda create -n treelearn python=3.9\nconda activate treelearn\npip install torch torch-scatter timm laspy[lazrs]==2.5.1 munch pandas plyfile pyyaml scikit-learn six tqdm open3d-cpu jakteristics shapely geopandas alphashape spconv-cu114 tensorboard tensorboardX\ncd ~/dev\ngit clone https://github.com/ecker-lab/TreeLearn.git\ncd TreeLearn\npip install -e .\n\nmkdir -p ~/Downloads/tree/treelearn/checkpoints\ncd ~/Downloads/tree/treelearn\nmkdir -p pipeline/forests\n\nwget https://data.goettingen-research-online.de/api/access/datafile/:persistentId?persistentId=doi:10.25625/VPMPID/1JMEQV -O checkpoints/model_weights.pth\n# https://data.goettingen-research-online.de/api/access/datafile/:persistentId?persistentId=doi:10.25625/VPMPID/C1AHQW for model_weights_diverse_training_data.pth\n\nwget https://data.goettingen-research-online.de/api/access/datafile/:persistentId?persistentId=doi:10.25625/VPMPID/0WDXL6 -O pipeline/forests/plot_7_cut.laz\n\ncd ~/dev/TreeLearn\npython\n</code></pre> <p>For Snellius with CUDA11: </p> <pre><code>module load 2022\nmodule load Anaconda3/2022.05\nconda init\n# exit and ssh again then conda will be in .bashrc\n\nmodule load 2022\nmodule load CUDA/11.7.0\n\nconda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch\n# pytorch 1.11 has issue with new numpy\npip install numpy==1.26\n# I install torch-scatter first to see if there is an issue. \npip install torch-scatter\n# Install the rest as above\n</code></pre> <pre><code># for python path\nsys.path.append(\"/home/user/dev/TreeLearn/tools/pipeline\")\nconfig_path = \"/home/user/dev/TreeLearn/configs/pipeline/pipeline.yaml\"\nconfig.forest_path = \"/home/user/data/tree/treelearn/pipeline/forests/plot_7_cut.laz\"\nconfig.dataset_test.data_root = \"/home/user/data/tree/treelearn/pipeline/forests/tiles\"\nconfig.pretrain = \"/home/user/data/tree/treelearn/checkpoints/model_weights.pth\"\n</code></pre> <pre><code>import sys\nsys.path.append(\"/home/fatemeh/dev/TreeLearn/tools/pipeline\")\nfrom pipeline import run_treelearn_pipeline\nimport argparse, pprint\nfrom tree_learn.util import get_config\n\nconfig_path = \"/home/fatemeh/dev/TreeLearn/configs/pipeline/pipeline.yaml\"\nconfig = get_config(config_path)\n\n# adjust config\nconfig.forest_path = \"/home/fatemeh/Downloads/tree/treelearn/pipeline/forests/plot_7_cut.laz\"\nconfig.dataset_test.data_root = \"/home/fatemeh/Downloads/tree/treelearn/pipeline/forests/tiles\"\nconfig.pretrain = \"/home/fatemeh/Downloads/tree/treelearn/checkpoints/model_weights.pth\"\nconfig.tile_generation = True\nconfig.sample_generation.stride = 0.9 # small overlap of tiles\nconfig.shape_cfg.outer_remove = False # default value = 13.5\nconfig.save_cfg.save_treewise = False\nconfig.save_cfg.return_type = \"voxelized_and_filtered\"\nprint(pprint.pformat(config.toDict(), indent=2))\n\nimport logging\nlogger = logging.getLogger(\"TreeLearn\")\nfor handler in logger.handlers[:]:\n    logger.removeHandler(handler)\nlogging.basicConfig()\nch = logging.StreamHandler(sys.stdout)\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\nlogger.setLevel(logging.INFO)\n\nrun_treelearn_pipeline(config)\n</code></pre>"},{"location":"segmenttree_install/#install-forainet","title":"Install ForAINet","text":"<p>Follow the rest of the installation guide. However, a few changes need to be made. Follow the steps below:</p> <ul> <li>The guide mentions that <code>requirements.txt</code> should be installed, but the file is not provided. Additionally, <code>torch-cluster</code> should be installed from a separate file.</li> <li><code>MinkowskiEngine</code> should be installed locally.  </li> </ul> <pre><code># make requirements.txt from https://github.com/nicolas-chaulet/torch-points3d/blob/master/requirements.txt\n# remove: omegaconf, hydra-core and replace by hydra-core&gt;=1.0.0,&lt;1.1.0\n# remove: torch-scatter, torch-sparse, torch-cluster, torch-geometric, torch, torchvision, numpy\npip install torch-scatter==2.0.8 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\npip install torch-sparse==0.6.12 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\npip install torch-cluster==1.5.9 -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\npip install torch-geometric==1.7.2\n\n# Don't do: export CUDA_HOME=/usr/local/cuda-11\n# Don't do: pip install -U git+https://github.com/NVIDIA/MinkowskiEngine -v --no-deps --install-option=\"--blas_include_dirs=${CONDA_PREFIX}/include\" \u2013install-option=\"--blas=openblas\"\nmodule load 2022\nmodule load CUDA/11.7.0 # CUDA/11.6.0 was giving error\ngit clone https://github.com/NVIDIA/MinkowskiEngine.git\ncd MinkowskiEngine\npython setup.py install --blas_include_dirs=${CONDA_PREFIX}/include --blas=openblas\n</code></pre>"},{"location":"segmenttree_install/#tools","title":"Tools","text":"<p>labelCloud is installed both on python 3.9, 3.10 but run with errors. </p> <p>point cloud annotation tools: https://chatgpt.com/share/67adeacf-bef8-8011-ba02-c59ae671126c</p> <ul> <li>cloudcompare for visualization: <code>sudo snap install cloudcompare</code>. Note that <code>sudo apt install cloudcompare</code> can't open <code>.laz</code> file.</li> </ul>"},{"location":"segmenttree_install/#miscellaneous","title":"Miscellaneous","text":""},{"location":"segmenttree_install/#gpu-compute-capability","title":"GPU Compute Capability","text":"<p>You can define from the below code or from GPU Compute Capability.</p> <pre><code>import torch\ntorch.cuda.get_device_capability()\n</code></pre> <p>H100: 9.0, A100: 8.6</p>"},{"location":"segmenttree_install/#cuda-modules-available-on-snellius","title":"CUDA modules available on Snellius","text":"<pre><code># module load 2024\nCUDA/12.6.0\ncuDNN/9.5.0.50-CUDA-12.6.0\n\n# module load 2023\nCUDA/12.1.1 \nCUDA/12.4.0\ncuDNN/8.9.2.26-CUDA-12.1.1\n\n# module load 2022\nCUDA/11.6.0    \nCUDA/11.7.0\nCUDA/11.8.0\ncuDNN/8.4.1.50-CUDA-11.7.0\ncuDNN/8.6.0.163-CUDA-11.8.0\n</code></pre>"},{"location":"segmenttree_install/#cuda-modules-paths-on-snellius","title":"CUDA modules paths on Snellius","text":"<p>To get the path information such as <code>CUDA_HOME</code>, <code>CUDA_PATH</code>, <code>PAHT</code>, <code>LIBRARY_PATH</code>, <code>LD_LIBRARY_PATH</code> use: </p> <pre><code>module load CUDA/12.6.0\nmodule display CUDA/12.6.0\n</code></pre>"},{"location":"segmenttree_install/#tried-but-failed","title":"Tried but failed","text":"<p>These are my previos attemps to solve issues encountered. But resolvinig issues didn't install MinkowskiEngine. But it is worth mentioning them here.</p>"},{"location":"segmenttree_install/#pip-deprecation-build-option-and-global-option","title":"pip DEPRECATION: --build-option and --global-option","text":"<p>This option doesn't work, even with the <code>--no-build-isolation</code> fix.</p> <pre><code>export CXX=g++\npip install -U git+https://github.com/NVIDIA/MinkowskiEngine --no-build-isolation -v --config-settings=build_option=--force_cuda --config-settings=blas_include_dirs=${CONDA_PREFIX}/include --config-settings=blas=openblas\n</code></pre> <p>MinkowskiEngine requires torch at build time, pip\u2019s default build isolation can cause \u201cPytorch not found\u201d errors. <code>--no-build-isolation</code> tells pip not to create a temporary/isolated environment at build time and instead to use the packages in your current conda/pip environment (where you have PyTorch installed).</p>"},{"location":"segmenttree_install/#distutil-issue-with-numpy","title":"Distutil issue with Numpy","text":"<p><code>numpy.distutils</code> removed after numpy=1.24.</p> <pre><code>conda install \"numpy=1.23.*\" setuptools pip wheel\n</code></pre>"},{"location":"segmenttree_install/#find-openbals","title":"Find openbals","text":"<pre><code>OPENBLAS=$(find ~/.conda/envs/test -name \"libopenblas.so\" | head -n 1)\nexport LD_LIBRARY_PATH=$(dirname $OPENBLAS):$LD_LIBRARY_PATH\npip install --no-cache-dir --force-reinstall numpy\n</code></pre> <pre><code># &gt;&gt;&gt; np.__config__.show()\n  \"Build Dependencies\": {\n    \"blas\": {\n      \"name\": \"scipy-openblas\",\n      \"found\": true,\n      \"version\": \"0.3.28\",\n      \"detection method\": \"pkgconfig\",\n      \"include directory\": \"/opt/_internal/cpython-3.10.15/lib/python3.10/site-packages/scipy_openblas64/include\",\n      \"lib directory\": \"/opt/_internal/cpython-3.10.15/lib/python3.10/site-packages/scipy_openblas64/lib\",\n      \"openblas configuration\": \"OpenBLAS 0.3.28  USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell MAX_THREADS=64\",\n      \"pc file directory\": \"/project/.openblas\"\n    },\n    \"lapack\": {\n      \"name\": \"scipy-openblas\",\n      \"found\": true,\n      \"version\": \"0.3.28\",\n      \"detection method\": \"pkgconfig\",\n      \"include directory\": \"/opt/_internal/cpython-3.10.15/lib/python3.10/site-packages/scipy_openblas64/include\",\n      \"lib directory\": \"/opt/_internal/cpython-3.10.15/lib/python3.10/site-packages/scipy_openblas64/lib\",\n      \"openblas configuration\": \"OpenBLAS 0.3.28  USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell MAX_THREADS=64\",\n      \"pc file directory\": \"/project/.openblas\"\n    }\n  }\n</code></pre>"},{"location":"segmenttree_install/#cuda-installation-in-docker","title":"CUDA installation in Docker","text":"<pre><code># docker pull or From\n# nvidia/cuda:12.6.0-runtime-ubuntu22.04\n# nvidia/cuda:12.6.0-devel-ubuntu22.04\n# nvidia/cuda:11.1.1-cudnn8-devel-ubuntu20.04\n\n# https://developer.nvidia.com/cuda-downloads\nwget https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run\nsh cuda_11.1.1_455.32.00_linux.run --silent --toolkit\n\nexport PATH=/usr/local/cuda-11.1/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:$LD_LIBRARY_PATH\nexport CUDA_HOME=/usr/local/cuda-11.1\n</code></pre>"},{"location":"segmenttree_install/#references","title":"References","text":"<ul> <li>TreeLearn: ~30M parameters github, paper</li> <li>SegmentAnyTree: ~12M parameters, paper</li> <li>ForAINet: paper, github</li> </ul> <p>TreeLearn, SegmentAnyTree, and ForAINet are based on PointGroup model. NB SoftGroup is improved version of the PointGroup.</p> <p>Point2Tree utilizes PointNet++ for semantic segmentation and employs a \"Bayesian flow approach,\" a non-deep-learning method, for instance segmentation.</p> <p>FSCT(code) is a semantic segmentation model, seemingly based on PointNet++.</p> <p>TLS2Tree performs instance segmentation of individual trees, leveraging the FSCT semantic segmentation model as a preprocessing step. Notably, it does not rely on deep learning.</p>"},{"location":"services/","title":"Services","text":""},{"location":"services/#service-desk","title":"Service Desk","text":"Name URL FNWI Service Desk serviceportal.uva.nl SURF Service Desk servicedesk.surf.nl"},{"location":"services/#services-direct-links","title":"Services Direct Links","text":"Service Name URL SURF Research Cloud sram.surf.nl SURF Filesender filesender.surf.nl SURF Drive surfdrive.surf.nl SURF Research Drive researchdrive.surf.nl FNWI Faculty Research Drive uva.data.surf.nl HIPSTER cluster for FNWI Feiog HIPSTER Crunchomics (CPU-only) cluster for IBED Crunchomics European Open Science Cloud (EOSC) EOSC piolot"},{"location":"services/#general-services","title":"General Services","text":"<p>Some of the phone numbers and services are mentioned in the following locations:</p> Name URL IBED Team/IBED/General/Files/IBED general information*.pdf FNWI A-Z list Direct Link  or via staff.uva.nl/Faculty of Science/A-Z"},{"location":"simulation/","title":"Simulation: Generation / Editing","text":"<ul> <li> <p>Generative models for synthetic data: diffusion model, Gaussian Splatting / neural rendering</p> </li> <li> <p>Sim2Real and Real2Sim: Common in robotics for transferring policies or data between simulation and the real world</p> </li> <li> <p>Scene-level generation / editing: ScanEdit takes a scanned 3D scene with object instances and uses an LLM to interpret high level editing instructions such as move or arrange to produce a rearranged scene.</p> </li> <li> <p>Object-level synthetic generation / editing: MeshCoder takes a 3D point cloud of an object and uses a multimodal LLM to output a Blender Python script that reconstructs the object and allows editing through code. MeshPad provides an interactive tool for 3D mesh creation and editing driven by 2D sketches.</p> </li> <li> <p>Procedural 3D scene or dataset generation: for example Unity and Blender based pipelines</p> </li> </ul>"},{"location":"slam/","title":"Localization, Mapping, 3D Reconstruction","text":"<p>Dense and sparse local feature matching methods such as LoFTR and Dedode</p> <p>Sparse coordinate representation (SCR) such as R-SCoRe, ACE</p> <p>VideoMimic: SMPL for parameteric human mesh model using (VIMO Tram from Daniilidis), Grounded SAM2 for people detection, ViTPose for 2D keypoints, BSTRO for contact points</p>"},{"location":"tasks/","title":"Example Tasks for Large Language, Multimodal, and Vision Models","text":"<p>This text provides a detailed overview of various tasks that different large models, such as Large Language Models (LLMs), Large Multimodal Models (LMMs), and Large Vision Models (LVMs), are capable of handling, including text generation, translation, image analysis, multimodal reasoning, and more, illustrating the diverse applications of these advanced AI systems.</p>"},{"location":"tasks/#llm-large-language-model","title":"LLM (Large Language Model):","text":"<ul> <li> <p>Examples: </p> <ul> <li>Text generation and writing: content creation, summarization, paraphrasing, scriptwriting, poetry, social media posts, email drafting, product descriptions, resume/cover letter writing, letter writing.</li> <li>Text comprehension and analysis: question answering, text analysis, critical review, legal document analysis, code understanding.</li> <li>Translation and language tasks: language translation, language learning assistance, text correction, dialect and style adaptation.</li> <li>Research and information retrieval: fact-checking, information gathering, historical context, current events analysis.</li> <li>Creative and artistic assistance: creative brainstorming, character creation, plot development, meme creation.</li> <li>Programming and technical tasks: code generation, debugging assistance, algorithm explanation, data analysis, simulation, and modeling.</li> <li>Decision-making support: pros and cons analysis, risk assessment, scenario planning, strategic planning.</li> <li>Education and tutoring: subject tutoring, test preparation, essay assistance, learning resources.</li> <li>Communication and interaction: chatbot support, role-playing, interview simulation.</li> <li>Entertainment: game dialogue creation, trivia and quizzes, interactive storytelling.</li> <li>Personal assistance: task management, goal setting, personal advice.</li> <li>Customization and personalization: persona development, tone adjustment, content filtering.</li> <li>Simulation and modeling: scenario simulation, market analysis, behavioral modeling.</li> <li>Explanation: explaining complex concepts.</li> <li>Ethics and philosophy: ethical analysis, debate simulation.</li> </ul> </li> <li> <p>Examples from Llama 3: general knowledge and instruction following, code, math, reasoning, tool use (search engine, Python interpreter, mathematical computational engine), multilingual capabilities, long context (code reasoning, summarization, question answering).</p> </li> </ul>"},{"location":"tasks/#lmm-large-multimodal-model-vlm-vision-language-model","title":"LMM (Large Multimodal Model) / VLM (Vision Language Model):","text":"<ul> <li> <p>Examples from Unified-IO 2 (images, text, audio, action, points, bounding boxes, keypoints, and camera poses): image editing, image generation, free-form VQA, depth and normal generation, visual-based audio generation, robotic manipulation, reference image generation, multiview image completion, visual parsing and segmentation, keypoint estimation, visual audio localization, future frame prediction.</p> </li> <li> <p>Examples from 4M-21: multimodal retrieval (e.g., given image retrieve caption, segment), multimodal generation (e.g., given image generate depth), out-of-the-box (zero-shot) tasks (e.g., normal, depth, semantic segmentation, instance segmentation, 3D human pose estimation, clustering). </p> </li> <li> <p>Examples from ImageBind (images, text, audio, depth, thermal, and IMU): cross-modal retrieval, embedding-space arithmetic, audio to image generation.</p> </li> <li> <p>Examples from Llama 3 on image: visual grounding (grounding information such as points, bounding boxes, and masks), MMMU or multimodal reasoning (understand images and solve college-level problems in multiple-choice and open-ended questions), VQA / ChartQA / TextVQA / DocVQA (image, chart, diagram, text in the image).</p> </li> <li>Examples from Llama 3 on video: QA: PerceptionTest (answer temporal reasoning questions focusing on skills like memory, abstraction, physics, semantics, and different types of reasoning such as descriptive, explanatory, predictive, counterfactual), temporal and causal reasoning, with a focus on open-ended question answering, compositional reasoning requiring spatiotemporal localization of relevant moments, recognition of visual concepts, and joint reasoning with subtitle-based dialogue, reasoning over long video clips to understand actions, spatial relations, temporal relations, and counting.</li> <li> <p>Examples from Llama 3 on speech: speech recognition, speech translation, spoken question answering.</p> </li> <li> <p>Examples: something (audio, text) to image generation, open-set object detection, image captioning, image description, visual question answering, visual commonsense reasoning, text-based image retrieval, image-based text generation, text-guided image manipulation, data visualization.</p> </li> </ul>"},{"location":"tasks/#lvm-large-vision-model","title":"LVM (Large Vision Model)","text":"<ul> <li>Examples from SAM2: promptable visual segmentation (prompts: clicks, boxes, or masks), promptable segmentation.</li> <li>Examples from SAM: (prompts: point, box, segment, text): zero-shot single point valid mask evaluation (segmenting an object from a single foreground point), zero-shot edge detection, zero-shot object proposals, zero-shot instance segmentation, zero-shot text-to-mask. </li> <li>Fine-tuning on specific tasks. <ul> <li>Image task examples: classification, object detection, semantic segmentation, instance segmentation, image generation, style transfer, super-resolution, image inpainting, face recognition and analysis, optical character recognition, scene understanding, anomaly detection, gesture and pose recognition, image retrieval, emotion recognition, visual enhancement.</li> <li>Multiple images/video task examples: action recognition, video object detection, multiple object tracking, visual object tracking, track anypoint, optical flow, 3D object reconstruction, SLAM/SfM.</li> </ul> </li> </ul>"},{"location":"tasks/#references","title":"References","text":"<ul> <li>LLM, LMM, VLM: Llama 3, 4M-21, Unified-IO 2, Florence</li> <li>LVM: DINOv2, SAM, SAM 2. CLIP, SigLIP, ImageBind</li> </ul>"},{"location":"tasks/#sota-and-popular-off-the-shelf-models","title":"SOTA and Popular Off-the-Shelf Models:","text":"<p>The following list highlights some of the current state-of-the-art (SOTA) and previously leading methods used in various domains such as tracking, depth estimation, optical flow, 3D reconstruction, segmentation, and language models. These methods are selected based on papers and research studies I have read, where they were commonly employed as off-the-shelf solutions.</p> <p>Please note that the field of machine learning and computer vision is rapidly evolving, and new models are frequently introduced. This list reflects a snapshot of current practices, but advancements in the field may lead to newer and potentially better-performing techniques over time.</p> <ul> <li>Vision Encoders: Vision-Only Model Encoders: PE (Perception Encoder, Mets), DINO, MAE (Masked Autoencoders), ResNet. Vision-Text Model Encoders: AIMv2, SigLIP, CLIP, BLIP. Vision-Text Encoder for Generation: LlamGen. E.g. In Janus (DeepSeek), LlamaGen is used for Geneneation Encoder and SigLIP for Understanding Encoder.</li> <li>Depth Map: DepthAnything, Depth Pro, DepthCrafter (for video), MiDaS, Depth Anything v2, DPT, ZoeDepth</li> <li>Optical Flow: SEA RAFT, RAFT</li> <li>3D/4D Reconstruction: D4RT (EepMind), V-DPM (Vedaldi), Depth Anything 3, MapAnything, ViPE (Nvidia), VGGT, COLMAP (Non-Deep Learning), DuSt3R, MASt3R, 4D (ViPE, St4RTrack, Easi3D, CUT3R, DAS3R, MonST3R, Dynamic Point Maps), 4D Online (ODHSR, only human), VideoMimic, ACE Zero (ACE0), noposplat (potentially for sparse reconstruction), </li> <li>Point Matching and Point Tracking: TAP (CoTracker3, TAPIR, PIP), SAM3 (Segment and Tracking Anything: SAM combined with DeAOT);SuperPoint combined with lightGLUE or SuperGLUE, MASt3R, </li> <li>Multi-Object Tracking (MOT):: SAM3 (Segment Anything Model) for image and video, MOTR, ByteTrack, BoT-Sort, FairMOT</li> <li>Referred Multi-Object Tracking / Text-guided Spatial Video Grounding (SVG): TempRMOT</li> <li>Text-guided Video Temporal Grounding (VTG): VTG aims to get start and end frames by prompting the action. FlashVTG</li> <li>Object Detection: Florence-2, Grounding DINO / DINO-X, PaliGemma 2, moondream2, small: Yolo Family (latest Generalist YOLO paper, YOLO12)</li> <li>Segmentation: Grounding DINO combined with SAM, Florence-2</li> <li>Pose Estimation: OpenPose</li> <li>Unified Multimodal Image Understanding and Generation: BLIP3-o (Saleforce), Jaus Pro (DeepSeek), EMU3 (Beijing Baai), MetaQuery, Metamorph, Chameleon(Meta)</li> <li>Image Captioning: xGen-MM (BLIP-3), RAM (Recognize Anything), CogVLM2, PaliGemma 2</li> <li>Visual Question Answering: Any of the VLMs or LMM such as Phi-3.5, PaliGemma 2, moondream2. older ones for multi-image LMM: Mantis, OpenFlamingo, Emu, Idefics </li> <li>Generative Video Models, Text-to-Video Generation Models: LYRA, SANA-Video, GEN3C (nvidia), CogVideoX (THUDM Tsinghua University), Pika Labs (Pika Labs), Stable Video Diffusion (Stability AI), Movie Gen and Emu Video (Meta), Sora (OpenAI), Gen-3 Alpha (Runway AI), Veo (Google DeepMind), Flux (Black Forest Labs), Hunyuan Video, DynamiCrafter, VideoCrafter (Tencent AI Lab), PixelDance and Seaweed (ByteDance owns TikTok can access via Jimeng AI platform), MiniMax T2V-01, Video-01 (Minimax can access via Hailuo AI platform), Luma Dream Machine (Luma Labs), Kling (Kuaishou), Alibaba (Wan), Open-Sora (HPC AI Tech)</li> <li>Text-to-Image Generation Models: SANA (MIT, NVIDIA, Tsinghua), FLUX1 (Black Forest Labs), Ideogram v2 (Ideogram), Midjourney v6 (Midjourney), Stable Diffusion 3.5 (Stablity AI), DALLE 3 (OpenAI), Firefly 3 (Adobe), Imagen 3, Flamingo (Google DeepMind), Aurora of Grok (xAI), Pixtral (Mistral), PixArt-alpha (Huawei), Janus (DeepSeek), CogView4 (THUDM Tsinghua University)</li> <li>Speech Models: Speech-to-Text, Text-to-Speech, Speech-to-Speech: Moshi (Kyutai), ElevenLabs, Google, OpenAI, Speech-to-Text: Whisper (OpenAI), Wav2Vec (Meta), SuperWhisper/Wisper Flow</li> <li>Control Video by Action: Genie 3 (Google DeepMind)</li> <li>Vision Language Models (VLMs): image, multi-image, video. Open VLMs: PLM (Perception LM, Meta), Cosmos Nemotron (NVidia), DeepSeek-VL2 (DeepSeek), QWen2-VL (Alibaba), InternVL2 (OpenGVLab), LLAVA 1.5, LLama3.2 (mainly LLM), Cambrian-1, CogVLM2 (Tsinghua University), MolMo (Ai2), SmolVLM (Hugging Face). Proprietary: GPT-40 (OpenAI), Claude Sonnet 3.5 (Claude), Gemini 1.5 Pro (Google)</li> <li>Large Language Models (LLMs):  Open source: DeepSeek v3 (DeepSeek),Qwen (Alibaba), LLAMA-3 (Meta), Phi-3 (Microsoft), Gemma 3 (Google),  OLMo 2 (Ai2), Helium-1 (Kyutai), Sky-T1-32B (UC Berkeley), Cerebras-GPT (Cerebras). Proprietary: Claude3 (Anthropic), Gemini (Google DeepMind), Nova (Amazon), Flash 3, Nexus (Reka AI)   Note that: Some of the models are Multimodal.</li> <li>Point Cloud Encoders: Sonata, Point Transformer V3 (PTv3), MinkowskiNet</li> <li>Point Cloud with Text: OpenScene (Open-vocabulary 3D Scene Understanding/ OV search in 3)</li> <li>Multimodal: OmniVinci (NVIDIA)</li> <li>Multi-teacher (Agglomerative): Chorus (Oswald), AM-RADIOv2.5 (NVIDIA), COVT (Darrell, chain of visual thought) (not really multi teacher. Different models trains to generates tokens)</li> <li>Other Foundation Models: Motor Control: HOVER. Weather Forcast: FourCastNet (NVidia), GenCast (DeepMind). Multilingual: SeamlessM4T (Meta), Brain2Qwerty (Meta). Remote Sensing: LISAT (Darrell), EarthGPT, RS-GPT4V</li> </ul>"},{"location":"tasks/#companies","title":"Companies","text":"<p>OpenAI (ChatGPT), Anthropic (Claude), X (Grok), Meta (LLaMa) </p> <p>DeepSeek, Moonshot AI (Kimi), Alibaba (QWen, Wan), Zhipu AI (GLM), MiniMax (Hailuo), ByteDance / TikTok, Tencent Hunyuan</p>"},{"location":"tasks/#finding-models","title":"Finding Models","text":""},{"location":"tasks/#models-in-general","title":"Models in General","text":"<ul> <li>Hugging Face</li> <li>Roboflow contains the list of top models.</li> </ul>"},{"location":"tasks/#lmm-large-multimodal-model-vlm-vision-language-model_1","title":"LMM (Large Multimodal Model) / VLM (Vision Language Model):","text":"<ul> <li>Find open-source models: The Open VLM leaderboard shows the scores of top VLMs, and you can see which models are open-source or proprietary.</li> <li>Hugging Face Video Generation Leaderboards, artificialanalysislu</li> <li>Text-to-Video</li> <li>Blog post on VLM: Implement a Vision Language Model from Scratch; Vision Language Using and Finetuning; vision language explanation;</li> </ul>"},{"location":"tasks/#llm-large-language-models","title":"LLM (Large Language Models)","text":"<ul> <li>Find open-source models: LLM Arena LMArena formerly LMSYS shows the current top LLMs. You can see which models are open-source or proprietary. scale.com.</li> </ul>"},{"location":"tasks/#speech-models","title":"Speech Models","text":"<ul> <li>Speech Arena</li> </ul>"},{"location":"tasks/#api-providers","title":"API Providers","text":"<p>Artificial Analysis</p>"},{"location":"time_series/","title":"Time Series","text":""},{"location":"time_series/#general-time-series-models","title":"General Time-Series Models","text":"<p>In all these works, the code, model and weights are released: </p> <ul> <li>MOMENT (Auton Lab, 2024)</li> <li>Chronos-2, Chronos (AWS, 2024)</li> <li>Moirai 2.0, MOIRAI-MOE (Silvio Savarese, Salesforce Research, 2024): Time-series foundation model, which uses mixture of expertes to select for different data frequencies. It is build upon MOIRAI (Salesforce Research, 2024). </li> <li>Timer-XL (Tsinghua University, 2025) implemented other methods as well in here. The older work is Timer (2024) and the newer work Sundial (2025).</li> <li>TOTO (Datadog, 2025) comes with code and weights</li> <li>SimDiff Diffusion-based model in time series forecasting </li> <li>TOTEM (Georgia Gkioxari, Caltech, 2024)</li> <li>TimesFM (Google Research, 2024)</li> <li>Lag-Llama (ServiceNow, 2024)</li> <li>TimeFound (2025)</li> <li>TTMs (IBM, 2024)</li> </ul> <p>Older works: PatchTST (2022), TimeGPT-1 (Nixtla, 2023), TIME-LLM, LLMTime, AutoTimes, GPT-4TS or FPT (2023).</p> <p>Autoformer, Informer,  Reformer for the long-term forecasting. Some of these methods are provided in HuggingFace Time Series Models. In Transformers Effective for Time Series Forecasting?, argues the transformers are not needed.</p>"},{"location":"time_series/#time-series-representation-learning","title":"Time-Series Representation Learning","text":"<ul> <li>TF-C: Time-Frequency Consistency (TF-C) Model (Harvard MIMS Lab, NeurIPS 2022) - A cross-domain time-series representation model that leverages contrastive learning between time-domain and frequency-domain views of the same signal\u200b. By enforcing consistent embeddings in both domains, TF-C learns general features transferable to diverse sensors (EEG, accelerometer, etc.). Weights are available.</li> <li>TS-TCC: Time-Series Representation Learning via Temporal and Contextual Contrasting: contrastive. augmentation: jitter, scale the amplitude and permutation.</li> <li>TimesURL: Learning Universal Representations of Time Series via Contrastive Learning: contrastive and reconstruction based (MAE) based on TS2Ve model. augmentation: left-righ cropping with frequency swaping of the negative example. hard negatives are temporal and instance based. In temporal way, the augmentation mix in different time. In the instance-based augmentation, this mixing occurs in mixing the different instances.</li> <li>TS2Vec: Towards universal representation of time series. Augmentation: croping</li> <li>CPC: Representation Learning with Contrastive Predictive Coding: The contrastive learning works such as SimCLR, MoCo, SwAV, DINO are based on the self distillation introduced in this paper. The postive are the predictions.</li> <li>TNC (Temporal Neighborhood Coding): Triplet loss (neighbor positive and further negative)</li> </ul>"},{"location":"time_series/#generalized-category-discovery-gcd","title":"Generalized Category Discovery (GCD)","text":"<p>GCD (Generalized Category Discovery) uses supervised contrastive and self-supervised contrastive learning to obtain representations. These representations are then clustered with supervised K-means, enforcing that the labeled samples remain in their original cluster. A Hungarian matching assignment computes clustering accuracy on the labeled data, and this accuracy determines the number of clusters. SimGCD: is a parametric extension of GCD that employs both supervised and unsupervised contrastive losses for representation learning, alongside supervised and unsupervised clustering losses with mean-entropy maximization to shape the clusters. Further SPTNet Builds on SimGCD by introducing Spatial Prompt Tuning\u2014additional prompt parameters learned on image patches\u2014to further improve clustering performance. \u03bcGCD Similar to SimGCD, but maintains a teacher model whose weights are updated via exponential moving average (EMA). Note that in both SimGCD and SPTNet, the teacher network is never trained directly, it is simply a detached version of the student model.</p> <p>GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery outperforms SimGCD in visual similar categories using CLIP text encoder. </p> <p>SelEx is based on GCD, with improvements coming from the clustering component to ensure balanced clusters during training, as well as from the use of both supervised and unsupervised self-expertise. Self-Expertise assigns different weights to samples depending on whether they are positive or negative, and this weighting is influenced by their class hierarchies (coarse or fine-grained).</p> <p>Some GCD or NCD (Novel Category Discovery) e.g. UNO methods are based on pioneering works such as SeLa (Asano): self-labeling via simultaneous clustering and representation learning, and SwAV.</p>"},{"location":"time_series/#imbalanced-generalized-category-discovery-gcd","title":"Imbalanced Generalized Category Discovery (GCD)","text":"<ul> <li>SimGCD: Clustering is trained jointly with the representation learning network. This is not an imbalanced data setting. However, entropy is used as a regularization to prevent the model from over-predicting certain label classes. This issue is more about imbalanced prediction, which arises during joint learning of clustering and representation, but not in separate training as done in the original GCD method. MASA: Multi-Activity Sequence Alignment via Implicit Clustering is related in terms of parametric clustering, though it addresses a different task.</li> <li>LegoGCD</li> <li>AGCD</li> <li>BaCon: Towards Distribution-Agnostic Generalized Category Discovery</li> <li>Long-Tailed Learning for Generalized Category Discovery</li> <li>Generalized Category Discovery under the Long-Tailed Distribution</li> <li>DebiasGCD</li> <li>Long-tailed GCD</li> <li>ImbaGCD</li> </ul>"},{"location":"time_series/#multimodal-time-series","title":"Multimodal Time Series","text":"<p>Time-series \u2192 text (captioning). TSML introduces a multimodal encoder\u2013decoder that merges a 1-D CNN\u2013based time-series encoder with a positional text\u2010token stream, and learns this stack end-to-end on an in-context\u2013generated, cross-modally denoised synthetic caption corpus\u2014setting a new state-of-the-art in descriptive accuracy across multiple benchmarks. TADACap, in contrast, requires no gradient updates: it employs a novel diverse\u2010retrieval strategy to pull the most relevant series\u2013caption pairs from a domain\u2010specific memory bank and reuses those captions directly\u2014achieving comparable semantic quality with dramatically lower annotation effort and zero fine-tuning. Together, these approaches illustrate the full spectrum\u2014from fully trained specialist decoders to pure retrieval\u2013plus\u2013reuse pipelines\u2014for interpretable time-series narration.</p> <p>Chat-style time-series assistants. ChatTS treats multivariate time series as a first-class modality by generating attribute-rich synthetic data (via an attribute-based time-series generator and the Time Series Evol-Instruct algorithm) and fine-tuning both a lightweight context-aware time-series encoder and a 14 B LLM on six alignment and four reasoning tasks\u2014yielding a chat interface that can answer questions, detect anomalies, and explain forecasts directly from raw numbers. ChatTime, by contrast, reframes each normalized and discretized numeric value as a new \u201cforeign-language\u201d token, expands a 1 B-parameter LLM\u2019s vocabulary accordingly, and then applies continuous pre-training plus instruction fine-tuning\u2014updating only the added token embeddings and heads (\u2248 350 M trainable parameters)\u2014to deliver zero-shot forecasting and seamless bidirectional dialogue between time series and text without touching the core model weights. Together, they span the design space from full LLM fine-tuning for maximal conversational fidelity to parameter-efficient tuning for lightweight, on-device time-series assistants.</p> <p>Forecasting &amp; reasoning with LLMs in the loop. TimeXL integrates a prototype\u2010based multimodal encoder with a closed-loop trio of LLM stages\u2014prediction, reflection, and refinement\u2014to produce up to an 8.9 % AUC improvement alongside human-centric, case-based rationales, without requiring full LLM fine-tuning. CAPTime freezes both its pretrained time-series encoder and base LLM, then aligns temporal patterns with exogenous text via learnable interactions and a mixture-of-distribution experts, yielding calibrated, multimodal probabilistic forecasts. SMETimes systematically evaluates sub-3 B-parameter \u201cSmall Language Models\u201d using statistical prompting, an adaptive fusion embedding architecture, and a dynamic mixture-of-experts framework to rival 7 B baselines\u2014achieving 3.8\u00d7 faster training, 5.2\u00d7 lower memory, and state-of-the-art accuracy. TimeCMA employs dual-branch encoding\u2014weak, disentangled time-series embeddings alongside robust LLM-derived prompt embeddings\u2014and aligns them via cross-modality similarity, passing only the last token to downstream predictors to cut computation, outperforming prior methods on eight datasets.</p> <p>Only ChatTS, ChatTime, SMETimes weights are released.</p>"},{"location":"time_series/#discriminative-representation","title":"Discriminative Representation","text":"<p>The representation that can be used in <code>GCN (Generalized Category Discovery)</code> (GCN, SelEx). </p> <p>Contrastive learning, Sparse autoencoder or older method such as DEC (Deep Embedded Clustering), SOM (Self Organizing Maps).</p>"},{"location":"time_series/#characteristics-of-time-series","title":"Characteristics of Time Series","text":"<p>Implicit Reasoning in Deep Time Series Forecasting: It is observed that certain linear, MLP-based, and patch-based Transformer models generalize effectively in carefully structured out-of-distribution scenarios, suggesting underexplored reasoning capabilities beyond simple pattern memorization.</p>"},{"location":"time_series/#other-literature","title":"Other literature","text":""},{"location":"tmux/","title":"TMUX","text":""},{"location":"tmux/#session-management","title":"Session Management","text":"<p>Note that all <code>Ctrl-b</code> commands are executed within a tmux session. If you are on a cluster (HPC) and run tmux, your session remains active even if your connection to the cluster is lost or the terminal is closed. This is especially useful, for example, when you interactively allocate a GPU using <code>salloc --gpus=1 --partition=gpu_a100 --time=00:05:00</code>.</p> <ul> <li> <p>Create a New Session: <code>tmux</code> \u2014 Start a new session with a default name <code>tmux new -s session_name</code> \u2014 Start a new session and assign it a name   Or within an active session: <code>:new-session -s session_name</code></p> </li> <li> <p>Detach from Session: <code>Ctrl-b d</code> \u2014 This detaches you from the current session, leaving it running in the background.    With <code>tmux ls</code>, this session will appear in the list unless it has been explicitly killed.</p> </li> <li> <p>Kill a Session:   Simply type <code>exit</code>   Or within an active session:   <code>Ctrl-b :kill-session -t session_name</code></p> </li> <li> <p>Reattach to a Session: <code>tmux a -t session_name</code> \u2014 Attach to a specific session by its name.   You can get the session name from <code>tmux ls</code>. For example, if the output is <code>0: 1 windows</code>, the command would be <code>tmux a -t 0</code>.</p> </li> <li> <p>List Sessions: <code>tmux ls</code>  \u2014 This lists all sessions  <code>Ctrl-b s</code> \u2014 This lists all sessions, allowing you to switch between them. </p> </li> </ul>"},{"location":"tmux/#pane-management-dividing-a-window","title":"Pane Management (Dividing a Window)","text":"<ul> <li> <p>Enter copy/scroll mode:   <code>Ctrl-b [</code> \u2014 Then you can scroll by <code>\u2191, \u2193</code>. To exit, press <code>q</code> or <code>esc</code>.</p> </li> <li> <p>Split Window Vertically: <code>Ctrl-b %</code> \u2014 This splits the current window into two panes side by side.</p> </li> <li> <p>Split Window Horizontally: <code>Ctrl-b \"</code> \u2014 This splits the current window into two panes, one above the other.</p> </li> <li> <p>Switch Between Panes: <code>Ctrl-b \u2190, \u2192, \u2191, \u2193</code>  \u2014 This navigates between panes in the respective direction.   <code>Ctrl-b o</code>  \u2014 This cycles through open panes in the current window. <code>Ctrl-b ;</code>  \u2014 This toggles between the last two active panes.  </p> </li> <li> <p>Resize Panes: <code>Ctrl-b Ctrl \u2190, \u2192, \u2191, \u2193</code> \u2014 Resize pane. <code>Ctrl-b :resize-pane -D</code> \u2014 Resize pane downwards. <code>Ctrl-b :resize-pane -U</code> \u2014 Resize pane upwards. <code>Ctrl-b :resize-pane -L</code> \u2014 Resize pane to the left. <code>Ctrl-b :resize-pane -R</code> \u2014 Resize pane to the right.</p> </li> <li> <p>Close a Pane: <code>Ctrl-b x</code> \u2014 This closes the current pane.</p> </li> </ul>"},{"location":"tmux/#window-page-management","title":"Window (Page) Management","text":"<ul> <li> <p>Create a New Window (Page): <code>Ctrl-b c</code></p> </li> <li> <p>Switch Between Windows (Pages): <code>Ctrl-b n</code>  \u2014 This moves to the next window. <code>Ctrl-b p</code>  \u2014 This moves to the previous window. <code>Ctrl-b w</code>  \u2014 This lists all windows and allows you to select one to switch to. <code>Ctrl-b &lt;window number&gt;</code>  \u2014 This directly switches to the window by its number.</p> </li> <li> <p>Rename a Window: <code>Ctrl-b ,</code> \u2014 This allows you to rename the current window.</p> </li> <li> <p>Close a Window (Page): <code>Ctrl-b &amp;</code> \u2014 This closes the current window.</p> </li> </ul>"},{"location":"tracking/","title":"Object Tracking: A Quick Overview","text":""},{"location":"tracking/#multi-object-tracking","title":"Multi Object Tracking","text":""},{"location":"tracking/#end-to-end-tracking","title":"End-to-End Tracking","text":"<p>MOTR Family</p> <ul> <li>MeMOTR</li> <li>MOTRv3 and CO-MOT: Improvement on MOTR by increasing detection objects in the loss term as extra supervision.</li> <li>MO-YOLO: : Efficient (fast training on 1 2080 ti GPU, 8 hours) YOLO with transformer in the encoder (RT-YOLO) and MOTR in the decoder.</li> </ul>"},{"location":"tracking/#point-tracking-track-any-point","title":"Point Tracking (Track any point)","text":"<ul> <li>CoTracker3  (Andrea Vedaldi)</li> <li>MVTracker (Marc Pollefeys): for multi-view 3D point tracking. Similar as CoTracker but it works in 3D. </li> </ul>"},{"location":"tracking/#others","title":"Others","text":"<ul> <li>NOOUGAT: online/offline graph-based learned associations</li> </ul>"},{"location":"transformer_models_with_variable_sequence_lengths/","title":"Handling Variable Sequence Lengths in Transformer Models","text":"<p>This guide provides code examples for handling variable sequence lengths in Transformer models using positional encodings. It includes examples of both sinusoidal (fixed) and learned positional embeddings, along with considerations when training on sequences of varying lengths.</p>"},{"location":"transformer_models_with_variable_sequence_lengths/#example-1-transformer-model-with-sinusoidal-positional-encoding","title":"Example 1: Transformer Model with Sinusoidal Positional Encoding","text":"<p>The following code demonstrates a Transformer model using sinusoidal positional encoding, which can handle variable sequence lengths without modification. </p> <p>This example is adapted from this source.</p> <pre><code>import math\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)  # Shape: [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2) * \n                             (-math.log(10000.0) / d_model))  # Shape: [d_model/2]\n\n        pe = torch.zeros(max_len, d_model)  # Shape: [max_len, d_model]\n        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, d_model]\n        \"\"\"\n        x = x + self.pe[:x.size(0), :]  # Add positional encoding\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, \n                 d_hid: int, nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, \n                                                 d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -&gt; None:\n        initrange = 0.1\n        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n        nn.init.zeros_(self.decoder.bias)\n        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            src: Tensor of shape [seq_len, batch_size]\n            src_mask: Tensor of shape [seq_len, seq_len]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\ndef generate_square_subsequent_mask(sz: int) -&gt; Tensor:\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n# Initialize the model\nntokens = 1000  # Vocabulary size\nd_model = 512   # Embedding dimension\nnhead = 8       # Number of attention heads\nd_hid = 2048    # Feedforward network dimension\nnlayers = 6     # Number of Transformer layers\ndropout = 0.5   # Dropout rate\n\nmodel = TransformerModel(ntokens, d_model, nhead, d_hid, nlayers, dropout)\n\n# Prepare input data with variable sequence length\nseq_len = 10    # Sequence length can vary\nbatch_size = 2  # Batch size\nx = torch.randint(0, ntokens, (seq_len, batch_size))  # Random input\n\nsrc_mask = generate_square_subsequent_mask(seq_len)\n\n# Run the model\noutput = model(x, src_mask)\n</code></pre>"},{"location":"transformer_models_with_variable_sequence_lengths/#example-2-transformer-model-with-learned-positional-embeddings","title":"Example 2: Transformer Model with Learned Positional Embeddings","text":"<p>In this example, positional embeddings are learned parameters, allowing the model to potentially capture position-specific patterns.</p> <pre><code>import torch\nfrom torch import nn, Tensor\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass LearnedPositionalEncoding(nn.Module):\n\n    def __init__(self, max_len: int, d_model: int):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(max_len, d_model)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, d_model]\n        \"\"\"\n        seq_len = x.size(0)\n        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n        position_ids = position_ids.unsqueeze(1).expand(seq_len, x.size(1))\n        position_embeddings = self.pos_embedding(position_ids)\n        return x + position_embeddings\n</code></pre>"},{"location":"transformer_models_with_variable_sequence_lengths/#example-3-transformer-model-with-learned-positional-embeddings-initialized-with-sinusoidal-positional-encoding","title":"Example 3: Transformer Model with Learned Positional Embeddings Initialized with Sinusoidal Positional Encoding","text":"<p>In this example, positional embeddings are learned parameters initialized with Sinusoidal Positional Encoding.</p> <pre><code>class SinusoidalInitializedPositionalEncoding(nn.Module):\n    def __init__(self, max_len: int, d_model: int):\n        super().__init__()\n        # Create a learnable positional embedding parameter\n        self.pos_embedding = nn.Parameter(torch.zeros(max_len, d_model))\n\n        # Initialize with sinusoidal positional encoding\n        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n                             (-math.log(10000.0) / d_model))  # [d_model/2]\n\n        sinusoidal_embedding = torch.zeros(max_len, d_model)  # [max_len, d_model]\n        sinusoidal_embedding[:, 0::2] = torch.sin(position * div_term)  # Even indices\n        sinusoidal_embedding[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n\n        # Assign the sinusoidal values to the parameter without tracking gradients\n        with torch.no_grad():\n            self.pos_embedding.copy_(sinusoidal_embedding)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, d_model]\n        \"\"\"\n        seq_len = x.size(0)\n        x = x + self.pos_embedding[:seq_len, :].unsqueeze(1)\n        return x\n</code></pre>"},{"location":"transformer_models_with_variable_sequence_lengths/#considerations-when-training-with-learned-positional-embeddings","title":"Considerations When Training with Learned Positional Embeddings","text":"<p>Training a model with learned positional embeddings (PE) where the majority of sequence lengths are significantly shorter than the specified <code>max_len</code> can indeed present challenges:</p> <ul> <li> <p>Underrepresentation of Longer Positions: If most training sequences are short, embeddings corresponding to higher positions (i.e., positions near <code>max_len</code>) receive minimal updates during training. This lack of exposure can lead to poor generalization for longer sequences during inference, as the model hasn't adequately learned representations for these positions.</p> </li> <li> <p>Inefficient Resource Utilization: Allocating parameters for positions up to <code>max_len</code> consumes memory and computational resources. If these positions are seldom used during training, this allocation becomes inefficient.</p> </li> </ul>"},{"location":"transformer_models_with_variable_sequence_lengths/#mitigation-strategies","title":"Mitigation Strategies:","text":"<ul> <li> <p>Dynamic Positional Embeddings: Instead of a fixed <code>max_len</code>, employ dynamic positional embeddings that adjust based on the actual sequence lengths encountered during training. This approach ensures that the model learns appropriate embeddings for the positions it processes.</p> </li> <li> <p>Curriculum Learning: Start training with shorter sequences and progressively introduce longer ones. This method helps the model gradually adapt to various sequence lengths, ensuring that embeddings for higher positions are adequately trained.</p> </li> <li> <p>Data Augmentation: Artificially increase the length of training sequences by padding or concatenating sequences. This technique exposes the model to a broader range of positions, aiding in the learning of embeddings across the entire range up to <code>max_len</code>.</p> </li> <li> <p>Regularization Techniques: Apply regularization methods to prevent overfitting to shorter sequences, encouraging the model to generalize better to longer sequences.</p> </li> </ul>"},{"location":"transformer_models_with_variable_sequence_lengths/#summary","title":"Summary","text":"<ul> <li> <p>Sinusoidal Positional Encoding: Handles variable sequence lengths naturally without the need for learned parameters tied to specific positions.</p> </li> <li> <p>Learned Positional Embeddings: Require careful consideration of sequence length distribution in the training data to ensure embeddings for all positions are adequately trained.</p> </li> <li> <p>Training Strategies: Adjust your data and training process, not the model code, to handle variable sequence lengths effectively.</p> </li> </ul>"},{"location":"ts_augmentation/","title":"Time Series Augmentation","text":""},{"location":"ts_augmentation/#common-time-series-data-augmentation-techniques","title":"\ud83e\uddf0 Common Time Series Data Augmentation Techniques","text":"<p>Beyond guided warping, several standard augmentation methods are widely used in time series analysis</p> <ul> <li>Jittering: Adding random noise to the time series</li> <li>Scaling: Multiplying the time series by a random scalar to change its amplitude</li> <li>Permutation: Randomly shuffling segments of the time series</li> <li>Time Warping: Stretching or compressing the time intervals in the series</li> <li>Magnitude Warping: Applying smooth, nonlinear transformations to the amplitude</li> <li>Window Slicing: Extracting random sub-sequences from the time series</li> <li>Window Warping: Randomly stretching or compressing specific windows within the time series</li> <li>Flipping: Reversing the time series sequence</li> <li>Mixup: Combining two time series by weighted averaging</li> <li>Frequency Domain Augmentation:Altering the frequency components of the time series</li> </ul>"},{"location":"ts_augmentation/#python-implementations","title":"\ud83e\uddea Python Implementations","text":"<p>Below are Python implementations for some of these augmentation technique:</p>"},{"location":"ts_augmentation/#jittering","title":"Jittering","text":"<pre><code>import numpy as np\n\ndef jitter(x, sigma=0.03):\n    return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n</code></pre>"},{"location":"ts_augmentation/#scaling","title":"Scaling","text":"<pre><code>def scaling(x, sigma=0.1):\n    factor = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[1],))\n    return x * factor\n</code></pre>"},{"location":"ts_augmentation/#permutation","title":"Permutation","text":"<pre><code>def permutation(x, max_segments=5):\n    orig_steps = np.arange(x.shape[0])\n    num_segs = np.random.randint(1, max_segments)\n    split_points = np.array_split(orig_steps, num_segs)\n    np.random.shuffle(split_points)\n    permuted = np.concatenate(split_points)\n    return x[permuted]\n</code></pre>"},{"location":"ts_augmentation/#time-warping","title":"Time Warping","text":"<pre><code>from scipy.interpolate import CubicSpline\n\ndef time_warp(x, sigma=0.2):\n    orig_steps = np.arange(x.shape[0])\n    random_warp = np.random.normal(loc=1.0, scale=sigma, size=x.shape[0])\n    warp_steps = np.cumsum(random_warp)\n    warp_steps = (warp_steps - warp_steps.min()) / (warp_steps.max() - warp_steps.min()) * (x.shape[0] - 1)\n    cs = CubicSpline(warp_steps, x, axis=0)\n    return cs(orig_steps)\n</code></pre>"},{"location":"ts_augmentation/#magnitude-warping","title":"Magnitude Warping","text":"<pre><code>def magnitude_warp(x, sigma=0.2, knot=4):\n    orig_steps = np.arange(x.shape[0])\n    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2,))\n    warp_steps = np.linspace(0, x.shape[0]-1, num=knot+2)\n    cs = CubicSpline(warp_steps, random_warps)\n    return x * cs(orig_steps).reshape(-1, 1)\n</code></pre>"},{"location":"ts_augmentation/#window-slicing","title":"Window Slicing","text":"<pre><code>def window_slice(x, reduce_ratio=0.9):\n    target_len = int(np.ceil(reduce_ratio * x.shape[0]))\n    start = np.random.randint(0, x.shape[0] - target_len)\n    return x[start:start+target_len]\n</code></pre>"},{"location":"ts_augmentation/#window-warping","title":"Window Warping","text":"<pre><code>def window_warp(x, window_ratio=0.1, scales=[0.5, 2.0]):\n    warp_size = int(np.ceil(window_ratio * x.shape[0]))\n    start = np.random.randint(0, x.shape[0] - warp_size)\n    window = x[start:start+warp_size]\n    scale = np.random.choice(scales)\n    window = resample(window, int(warp_size * scale))\n    warped = np.concatenate((x[:start], window, x[start+warp_size:]))\n    return resample(warped, x.shape[0])\n</code></pre> <p>Note: The <code>resample</code> function can be imported from <code>scipy.signal</code>.</p>"},{"location":"ts_augmentation/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li> <p>Time Series Data Augmentation for Neural Networks by Time Warping with a Discriminative Teacher: paper, An empirical survey of data augmentation for time series classification with neural networks: paper, code</p> </li> <li> <p>Tsaug Library: An open-source Python package for time series augmentation, offering a variety of augmentation techniques: code</p> </li> </ul>"}]}