
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://fkariminejadasl.github.io/ml-notebooks/transformer_models_with_variable_sequence_lengths/">
      
      
        <link rel="prev" href="../companies/">
      
      
        <link rel="next" href="../segmenttree_install/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Handling Variable Sequence Lengths in Transformer Models - ML Notebooks</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#handling-variable-sequence-lengths-in-transformer-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ML Notebooks" class="md-header__button md-logo" aria-label="ML Notebooks" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ML Notebooks
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Handling Variable Sequence Lengths in Transformer Models
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ML Notebooks" class="md-nav__button md-logo" aria-label="ML Notebooks" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    ML Notebooks
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning Tutorials and Notebooks
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning General
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Deep Learning General
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Access Snellius GPUs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning_project_setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning Project Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_lightning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pytorch Lightning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../practical_info_data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Practical Information about Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../jupyter_src/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Shared Jupyter Notebook in SRC (SURF Research Cloud)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradio_src/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gradio App in SRC (SURF Research Cloud)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradio_hf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hugging Face Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../postgres_plpython3u/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Set Up a Custom Python Function within PostgreSQL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Training and Inference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Training and Inference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resource_limitations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Training and Inference with Limited Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../improve_training_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Improve Deep Learning Training Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ddp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DDP(Distributed Data Parallel in PyTorch
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Courses and Literature
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Courses and Literature
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../courses/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Courses in Deep Learning and Computer Vision
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../object_detection/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Object Detection: A Quick Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tracking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Object Tracking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../slam/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Localization, Mapping, 3D Reconstruction
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../time_series/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Time Series
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Blogpost
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Blogpost
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tasks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Example Tasks for Large Language, Multimodal, and Vision Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../companies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Companies
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Handling Variable Sequence Lengths in Transformer Models
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Handling Variable Sequence Lengths in Transformer Models
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#example-1-transformer-model-with-sinusoidal-positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example 1: Transformer Model with Sinusoidal Positional Encoding
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-2-transformer-model-with-learned-positional-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example 2: Transformer Model with Learned Positional Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-3-transformer-model-with-learned-positional-embeddings-initialized-with-sinusoidal-positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example 3: Transformer Model with Learned Positional Embeddings Initialized with Sinusoidal Positional Encoding
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#considerations-when-training-with-learned-positional-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Considerations When Training with Learned Positional Embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Considerations When Training with Learned Positional Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mitigation-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mitigation Strategies:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../segmenttree_install/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installing Deep Learning Tools for Point Cloud Segmentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AIEnhancement/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Enhancing AI Capabilities: Post-Training, Reasoning, and Agent, Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Generative AI
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ts_augmentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Time Series Augmentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../distribution_metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Metrics for Comparing Distributions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../simulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Simulation: Generation / Editing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    General
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    General
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../services/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Services
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Git Basics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Python Basics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linux Basics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Docker Basics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tmux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tmux Basics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../revealjs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Slides from Markdown
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Examples
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scheuler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scheduler
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Miscellaneous
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Miscellaneous
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../GEE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Google Earth Engine
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#example-1-transformer-model-with-sinusoidal-positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example 1: Transformer Model with Sinusoidal Positional Encoding
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-2-transformer-model-with-learned-positional-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example 2: Transformer Model with Learned Positional Embeddings
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-3-transformer-model-with-learned-positional-embeddings-initialized-with-sinusoidal-positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example 3: Transformer Model with Learned Positional Embeddings Initialized with Sinusoidal Positional Encoding
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#considerations-when-training-with-learned-positional-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Considerations When Training with Learned Positional Embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Considerations When Training with Learned Positional Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mitigation-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mitigation Strategies:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="handling-variable-sequence-lengths-in-transformer-models">Handling Variable Sequence Lengths in Transformer Models</h1>
<p>This guide provides code examples for handling variable sequence lengths in Transformer models using positional encodings. It includes examples of both sinusoidal (fixed) and learned positional embeddings, along with considerations when training on sequences of varying lengths.</p>
<hr />
<h2 id="example-1-transformer-model-with-sinusoidal-positional-encoding">Example 1: Transformer Model with Sinusoidal Positional Encoding</h2>
<p>The following code demonstrates a Transformer model using sinusoidal positional encoding, which can handle variable sequence lengths without modification. </p>
<p>This example is adapted from <a href="https://guyuena.github.io/PyTorch-study-Tutorials/beginner/transformer_tutorial.html">this source</a>.</p>
<pre><code class="language-python">import math
import torch
from torch import nn, Tensor
from torch.nn import TransformerEncoder, TransformerEncoderLayer

class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)  # Shape: [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2) * 
                             (-math.log(10000.0) / d_model))  # Shape: [d_model/2]

        pe = torch.zeros(max_len, d_model)  # Shape: [max_len, d_model]
        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices
        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -&gt; Tensor:
        &quot;&quot;&quot;
        Args:
            x: Tensor of shape [seq_len, batch_size, d_model]
        &quot;&quot;&quot;
        x = x + self.pe[:x.size(0), :]  # Add positional encoding
        return self.dropout(x)

class TransformerModel(nn.Module):

    def __init__(self, ntoken: int, d_model: int, nhead: int, 
                 d_hid: int, nlayers: int, dropout: float = 0.5):
        super().__init__()
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        encoder_layers = TransformerEncoderLayer(d_model, nhead, 
                                                 d_hid, dropout)
        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
        self.encoder = nn.Embedding(ntoken, d_model)
        self.d_model = d_model
        self.decoder = nn.Linear(d_model, ntoken)

        self.init_weights()

    def init_weights(self) -&gt; None:
        initrange = 0.1
        nn.init.uniform_(self.encoder.weight, -initrange, initrange)
        nn.init.zeros_(self.decoder.bias)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src: Tensor, src_mask: Tensor) -&gt; Tensor:
        &quot;&quot;&quot;
        Args:
            src: Tensor of shape [seq_len, batch_size]
            src_mask: Tensor of shape [seq_len, seq_len]
        &quot;&quot;&quot;
        src = self.encoder(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, src_mask)
        output = self.decoder(output)
        return output

def generate_square_subsequent_mask(sz: int) -&gt; Tensor:
    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)

# Initialize the model
ntokens = 1000  # Vocabulary size
d_model = 512   # Embedding dimension
nhead = 8       # Number of attention heads
d_hid = 2048    # Feedforward network dimension
nlayers = 6     # Number of Transformer layers
dropout = 0.5   # Dropout rate

model = TransformerModel(ntokens, d_model, nhead, d_hid, nlayers, dropout)

# Prepare input data with variable sequence length
seq_len = 10    # Sequence length can vary
batch_size = 2  # Batch size
x = torch.randint(0, ntokens, (seq_len, batch_size))  # Random input

src_mask = generate_square_subsequent_mask(seq_len)

# Run the model
output = model(x, src_mask)
</code></pre>
<hr />
<h2 id="example-2-transformer-model-with-learned-positional-embeddings">Example 2: Transformer Model with Learned Positional Embeddings</h2>
<p>In this example, positional embeddings are learned parameters, allowing the model to potentially capture position-specific patterns.</p>
<pre><code class="language-python">import torch
from torch import nn, Tensor
from torch.nn import TransformerEncoder, TransformerEncoderLayer

class LearnedPositionalEncoding(nn.Module):

    def __init__(self, max_len: int, d_model: int):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_len, d_model)

    def forward(self, x: Tensor) -&gt; Tensor:
        &quot;&quot;&quot;
        Args:
            x: Tensor of shape [seq_len, batch_size, d_model]
        &quot;&quot;&quot;
        seq_len = x.size(0)
        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)
        position_ids = position_ids.unsqueeze(1).expand(seq_len, x.size(1))
        position_embeddings = self.pos_embedding(position_ids)
        return x + position_embeddings
</code></pre>
<hr />
<h2 id="example-3-transformer-model-with-learned-positional-embeddings-initialized-with-sinusoidal-positional-encoding">Example 3: Transformer Model with Learned Positional Embeddings Initialized with Sinusoidal Positional Encoding</h2>
<p>In this example, positional embeddings are learned parameters initialized with Sinusoidal Positional Encoding.</p>
<pre><code class="language-python">class SinusoidalInitializedPositionalEncoding(nn.Module):
    def __init__(self, max_len: int, d_model: int):
        super().__init__()
        # Create a learnable positional embedding parameter
        self.pos_embedding = nn.Parameter(torch.zeros(max_len, d_model))

        # Initialize with sinusoidal positional encoding
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *
                             (-math.log(10000.0) / d_model))  # [d_model/2]

        sinusoidal_embedding = torch.zeros(max_len, d_model)  # [max_len, d_model]
        sinusoidal_embedding[:, 0::2] = torch.sin(position * div_term)  # Even indices
        sinusoidal_embedding[:, 1::2] = torch.cos(position * div_term)  # Odd indices

        # Assign the sinusoidal values to the parameter without tracking gradients
        with torch.no_grad():
            self.pos_embedding.copy_(sinusoidal_embedding)

    def forward(self, x: Tensor) -&gt; Tensor:
        &quot;&quot;&quot;
        Args:
            x: Tensor of shape [seq_len, batch_size, d_model]
        &quot;&quot;&quot;
        seq_len = x.size(0)
        x = x + self.pos_embedding[:seq_len, :].unsqueeze(1)
        return x
</code></pre>
<hr />
<h2 id="considerations-when-training-with-learned-positional-embeddings">Considerations When Training with Learned Positional Embeddings</h2>
<p>Training a model with learned positional embeddings (PE) where the majority of sequence lengths are significantly shorter than the specified <code>max_len</code> can indeed present challenges:</p>
<ul>
<li>
<p><strong>Underrepresentation of Longer Positions</strong>: If most training sequences are short, embeddings corresponding to higher positions (i.e., positions near <code>max_len</code>) receive minimal updates during training. This lack of exposure can lead to poor generalization for longer sequences during inference, as the model hasn't adequately learned representations for these positions.</p>
</li>
<li>
<p><strong>Inefficient Resource Utilization</strong>: Allocating parameters for positions up to <code>max_len</code> consumes memory and computational resources. If these positions are seldom used during training, this allocation becomes inefficient.</p>
</li>
</ul>
<h3 id="mitigation-strategies">Mitigation Strategies:</h3>
<ul>
<li>
<p><strong>Dynamic Positional Embeddings</strong>: Instead of a fixed <code>max_len</code>, employ dynamic positional embeddings that adjust based on the actual sequence lengths encountered during training. This approach ensures that the model learns appropriate embeddings for the positions it processes.</p>
</li>
<li>
<p><strong>Curriculum Learning</strong>: Start training with shorter sequences and progressively introduce longer ones. This method helps the model gradually adapt to various sequence lengths, ensuring that embeddings for higher positions are adequately trained.</p>
</li>
<li>
<p><strong>Data Augmentation</strong>: Artificially increase the length of training sequences by padding or concatenating sequences. This technique exposes the model to a broader range of positions, aiding in the learning of embeddings across the entire range up to <code>max_len</code>.</p>
</li>
<li>
<p><strong>Regularization Techniques</strong>: Apply regularization methods to prevent overfitting to shorter sequences, encouraging the model to generalize better to longer sequences.</p>
</li>
</ul>
<hr />
<h2 id="summary">Summary</h2>
<ul>
<li>
<p><strong>Sinusoidal Positional Encoding</strong>: Handles variable sequence lengths naturally without the need for learned parameters tied to specific positions.</p>
</li>
<li>
<p><strong>Learned Positional Embeddings</strong>: Require careful consideration of sequence length distribution in the training data to ensure embeddings for all positions are adequately trained.</p>
</li>
<li>
<p><strong>Training Strategies</strong>: Adjust your data and training process, not the model code, to handle variable sequence lengths effectively.</p>
</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
      
    
  </body>
</html>